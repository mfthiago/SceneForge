<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SceneForge - Video Recreation App</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- Google Identity Services -->
    <script src="https://accounts.google.com/gsi/client" async defer></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jszip/3.10.1/jszip.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mp4-muxer@5.1.3/build/mp4-muxer.min.js"></script>
    <style>
      @import url("https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap");
      * {
        font-family: "Inter", -apple-system, BlinkMacSystemFont, sans-serif;
      }
      ::-webkit-scrollbar {
        width: 8px;
        height: 8px;
      }
      ::-webkit-scrollbar-track {
        background: #18181b;
      }
      ::-webkit-scrollbar-thumb {
        background: #3f3f46;
        border-radius: 4px;
      }
      input[type="range"] {
        -webkit-appearance: none;
        height: 4px;
        border-radius: 2px;
        background: #3f3f46;
      }
      input[type="range"]::-webkit-slider-thumb {
        width: 14px;
        height: 14px;
        border-radius: 50%;
        background: #8b5cf6;
        cursor: pointer;
        -webkit-appearance: none;
      }

      /* Hide native scrollbar for scene container */
      .scene-scroll-hidden::-webkit-scrollbar {
        height: 0;
        display: none;
      }
      .scene-scroll-hidden {
        scrollbar-width: none;
        -ms-overflow-style: none;
      }

      /* Custom scrollbar styling */
      #scrollbar-track {
        background: linear-gradient(180deg, #27272a 0%, #3f3f46 100%);
        border: 1px solid #52525b;
      }
      #scrollbar-thumb {
        transition: box-shadow 0.2s, transform 0.1s;
      }
      #scrollbar-thumb:hover {
        box-shadow: 0 0 12px rgba(139, 92, 246, 0.7) !important;
        transform: scaleY(1.1);
      }

      /* Timeline trim handles - DaVinci Resolve style */
      .trim-handle {
        transition: opacity 0.15s ease, width 0.15s ease;
      }
      .trim-handle:hover {
        width: 6px !important;
      }
      .trim-handle::before {
        content: "";
        position: absolute;
        top: 50%;
        transform: translateY(-50%);
        width: 2px;
        height: 20px;
        background: white;
        border-radius: 1px;
      }
      .trim-handle-left::before {
        left: 2px;
      }
      .trim-handle-right::before {
        right: 2px;
      }
      .timeline-clip-container:hover .trim-handle {
        opacity: 1 !important;
      }

      /* Active dragging state */
      body.trim-dragging {
        cursor: ew-resize !important;
        user-select: none;
      }
      body.trim-dragging * {
        cursor: ew-resize !important;
      }

      /* Blade/Cut tool mode */
      body.blade-mode .timeline-block {
        cursor: crosshair !important;
      }
      body.blade-mode .timeline-block:hover {
        position: relative;
      }
      body.blade-mode .timeline-block:hover::after {
        content: "";
        position: absolute;
        top: 0;
        bottom: 0;
        width: 2px;
        background: #ef4444;
        pointer-events: none;
        box-shadow: 0 0 8px #ef4444;
      }

      /* Google Login Styles */
      .login-screen {
        min-height: 100vh;
        display: flex;
        align-items: center;
        justify-content: center;
        background: linear-gradient(
          135deg,
          #09090b 0%,
          #18181b 50%,
          #09090b 100%
        );
        position: relative;
        overflow: hidden;
      }
      .login-screen::before {
        content: "";
        position: absolute;
        inset: 0;
        background: radial-gradient(
            ellipse at 50% 0%,
            rgba(139, 92, 246, 0.15) 0%,
            transparent 50%
          ),
          radial-gradient(
            ellipse at 80% 80%,
            rgba(217, 70, 239, 0.1) 0%,
            transparent 40%
          );
        pointer-events: none;
      }
      .login-card {
        background: rgba(24, 24, 27, 0.8);
        backdrop-filter: blur(20px);
        border: 1px solid rgba(63, 63, 70, 0.5);
        border-radius: 24px;
        padding: 48px;
        max-width: 420px;
        width: 100%;
        text-align: center;
        position: relative;
        box-shadow: 0 25px 50px -12px rgba(0, 0, 0, 0.5);
      }
      .google-btn {
        display: flex;
        align-items: center;
        justify-content: center;
        gap: 12px;
        width: 100%;
        padding: 14px 24px;
        background: white;
        border: none;
        border-radius: 12px;
        font-size: 16px;
        font-weight: 500;
        color: #1f1f1f;
        cursor: pointer;
        transition: all 0.2s ease;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.15);
      }
      .google-btn:hover {
        background: #f8f8f8;
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
        transform: translateY(-1px);
      }
      .google-btn:active {
        transform: translateY(0);
      }
      .google-btn svg {
        width: 20px;
        height: 20px;
      }
      .user-menu {
        position: relative;
      }
      .user-avatar {
        width: 36px;
        height: 36px;
        border-radius: 50%;
        border: 2px solid #8b5cf6;
        cursor: pointer;
        transition: all 0.2s ease;
        object-fit: cover;
      }
      .user-avatar:hover {
        border-color: #a78bfa;
        transform: scale(1.05);
      }
      .user-dropdown {
        position: absolute;
        top: 100%;
        right: 0;
        margin-top: 8px;
        background: #18181b;
        border: 1px solid #3f3f46;
        border-radius: 12px;
        padding: 8px 0;
        min-width: 220px;
        box-shadow: 0 10px 40px rgba(0, 0, 0, 0.4);
        opacity: 0;
        visibility: hidden;
        transform: translateY(-10px);
        transition: all 0.2s ease;
        z-index: 100;
      }
      .user-dropdown.active {
        opacity: 1;
        visibility: visible;
        transform: translateY(0);
      }
      .user-info {
        padding: 12px 16px;
        border-bottom: 1px solid #3f3f46;
      }
      .user-name {
        font-weight: 600;
        color: #f4f4f5;
        font-size: 14px;
      }
      .user-email {
        font-size: 12px;
        color: #a1a1aa;
        margin-top: 2px;
      }
      .logout-btn {
        width: 100%;
        padding: 10px 16px;
        text-align: left;
        color: #ef4444;
        font-size: 13px;
        display: flex;
        align-items: center;
        gap: 8px;
        transition: background 0.15s ease;
        cursor: pointer;
        background: none;
        border: none;
      }
      .logout-btn:hover {
        background: rgba(239, 68, 68, 0.1);
      }
    </style>
  </head>
  <body class="bg-zinc-950 text-white">
    <div id="app"></div>

    <!-- Edit Prompt Modal -->
    <div
      id="edit-prompt-modal"
      class="fixed inset-0 bg-black/90 flex items-center justify-center z-50 hidden"
    >
      <div
        class="bg-zinc-900 rounded-2xl w-full max-w-2xl max-h-[90vh] overflow-hidden border border-zinc-700 shadow-2xl mx-4"
      >
        <!-- Header -->
        <div
          class="flex items-center justify-between px-6 py-4 border-b border-zinc-800 bg-zinc-800/50"
        >
          <div class="flex items-center gap-3">
            <div
              class="w-10 h-10 rounded-xl bg-gradient-to-br from-blue-500 to-violet-500 flex items-center justify-center"
            >
              <svg
                class="w-5 h-5 text-white"
                fill="none"
                stroke="currentColor"
                viewBox="0 0 24 24"
              >
                <path
                  stroke-linecap="round"
                  stroke-linejoin="round"
                  stroke-width="2"
                  d="M11 5H6a2 2 0 00-2 2v11a2 2 0 002 2h11a2 2 0 002-2v-5m-1.414-9.414a2 2 0 112.828 2.828L11.828 15H9v-2.828l8.586-8.586z"
                />
              </svg>
            </div>
            <div>
              <h3 class="text-lg font-semibold text-white">Edit Prompts</h3>
              <p class="text-xs text-zinc-400" id="edit-prompt-scene-label">
                Scene 1
              </p>
            </div>
          </div>
          <button
            onclick="closeEditPromptModal()"
            class="p-2 hover:bg-zinc-700 rounded-lg text-zinc-400 hover:text-white transition-colors"
          >
            <svg
              class="w-5 h-5"
              fill="none"
              stroke="currentColor"
              viewBox="0 0 24 24"
            >
              <path
                stroke-linecap="round"
                stroke-linejoin="round"
                stroke-width="2"
                d="M6 18L18 6M6 6l12 12"
              />
            </svg>
          </button>
        </div>

        <!-- Content -->
        <div class="p-6 space-y-6 overflow-y-auto max-h-[calc(90vh-140px)]">
          <!-- Image Prompt -->
          <div>
            <label class="block text-sm font-medium text-violet-400 mb-2">
              <span class="flex items-center gap-2">
                <svg
                  class="w-4 h-4"
                  fill="none"
                  stroke="currentColor"
                  viewBox="0 0 24 24"
                >
                  <path
                    stroke-linecap="round"
                    stroke-linejoin="round"
                    stroke-width="2"
                    d="M4 16l4.586-4.586a2 2 0 012.828 0L16 16m-2-2l1.586-1.586a2 2 0 012.828 0L20 14m-6-6h.01M6 20h12a2 2 0 002-2V6a2 2 0 00-2-2H6a2 2 0 00-2 2v12a2 2 0 002 2z"
                  />
                </svg>
                Image Prompt
              </span>
            </label>
            <textarea
              id="edit-image-prompt"
              class="w-full h-32 bg-zinc-800 border border-zinc-700 rounded-xl px-4 py-3 text-sm text-white placeholder-zinc-500 focus:outline-none focus:ring-2 focus:ring-violet-500 focus:border-transparent resize-none"
              placeholder="Describe the image to generate..."
            ></textarea>
          </div>

          <!-- Negative Prompt -->
          <div>
            <label class="block text-sm font-medium text-red-400 mb-2">
              <span class="flex items-center gap-2">
                <svg
                  class="w-4 h-4"
                  fill="none"
                  stroke="currentColor"
                  viewBox="0 0 24 24"
                >
                  <path
                    stroke-linecap="round"
                    stroke-linejoin="round"
                    stroke-width="2"
                    d="M18.364 18.364A9 9 0 005.636 5.636m12.728 12.728A9 9 0 015.636 5.636m12.728 12.728L5.636 5.636"
                  />
                </svg>
                Negative Prompt (things to avoid)
              </span>
            </label>
            <textarea
              id="edit-negative-prompt"
              class="w-full h-20 bg-zinc-800 border border-zinc-700 rounded-xl px-4 py-3 text-sm text-white placeholder-zinc-500 focus:outline-none focus:ring-2 focus:ring-red-500 focus:border-transparent resize-none"
              placeholder="What to avoid in the image..."
            ></textarea>
          </div>

          <!-- Motion Prompt -->
          <div>
            <label class="block text-sm font-medium text-fuchsia-400 mb-2">
              <span class="flex items-center gap-2">
                <svg
                  class="w-4 h-4"
                  fill="none"
                  stroke="currentColor"
                  viewBox="0 0 24 24"
                >
                  <path
                    stroke-linecap="round"
                    stroke-linejoin="round"
                    stroke-width="2"
                    d="M14.752 11.168l-3.197-2.132A1 1 0 0010 9.87v4.263a1 1 0 001.555.832l3.197-2.132a1 1 0 000-1.664z"
                  />
                  <path
                    stroke-linecap="round"
                    stroke-linejoin="round"
                    stroke-width="2"
                    d="M21 12a9 9 0 11-18 0 9 9 0 0118 0z"
                  />
                </svg>
                Motion Prompt (camera & movement)
              </span>
            </label>
            <textarea
              id="edit-motion-prompt"
              class="w-full h-24 bg-zinc-800 border border-zinc-700 rounded-xl px-4 py-3 text-sm text-white placeholder-zinc-500 focus:outline-none focus:ring-2 focus:ring-fuchsia-500 focus:border-transparent resize-none"
              placeholder="Describe camera movements, transitions..."
            ></textarea>
          </div>

          <!-- Speech/Dialogue -->
          <div>
            <label class="block text-sm font-medium text-amber-400 mb-2">
              <span class="flex items-center gap-2">
                <svg
                  class="w-4 h-4"
                  fill="none"
                  stroke="currentColor"
                  viewBox="0 0 24 24"
                >
                  <path
                    stroke-linecap="round"
                    stroke-linejoin="round"
                    stroke-width="2"
                    d="M8 12h.01M12 12h.01M16 12h.01M21 12c0 4.418-4.03 8-9 8a9.863 9.863 0 01-4.255-.949L3 20l1.395-3.72C3.512 15.042 3 13.574 3 12c0-4.418 4.03-8 9-8s9 3.582 9 8z"
                  />
                </svg>
                Speech / Dialogue
              </span>
            </label>
            <textarea
              id="edit-speech-prompt"
              class="w-full h-20 bg-zinc-800 border border-zinc-700 rounded-xl px-4 py-3 text-sm text-white placeholder-zinc-500 focus:outline-none focus:ring-2 focus:ring-amber-500 focus:border-transparent resize-none"
              placeholder="Character dialogue or voiceover text..."
            ></textarea>
          </div>
        </div>

        <!-- Footer -->
        <div
          class="flex items-center justify-between px-6 py-4 border-t border-zinc-800 bg-zinc-800/50"
        >
          <button
            onclick="closeEditPromptModal()"
            class="px-4 py-2 text-zinc-400 hover:text-white hover:bg-zinc-700 rounded-lg text-sm font-medium transition-colors"
          >
            Cancel
          </button>
          <button
            onclick="saveEditedPrompts()"
            class="px-6 py-2 bg-gradient-to-r from-violet-600 to-fuchsia-600 hover:from-violet-500 hover:to-fuchsia-500 rounded-lg text-sm font-medium text-white flex items-center gap-2 transition-colors"
          >
            <svg
              class="w-4 h-4"
              fill="none"
              stroke="currentColor"
              viewBox="0 0 24 24"
            >
              <path
                stroke-linecap="round"
                stroke-linejoin="round"
                stroke-width="2"
                d="M5 13l4 4L19 7"
              />
            </svg>
            Save Changes
          </button>
        </div>
      </div>
    </div>

    <script type="module">
      // ============================================
      // STATE MANAGEMENT
      // ============================================
      const state = {
        // Authentication
        isAuthenticated: false,
        user: null,

        step: 0, // 0: upload, 1: pipeline, 2: editor
        apiKeys: {
          gemini: localStorage.getItem("gemini_api_key") || "",
          kieai: localStorage.getItem("kieai_api_key") || "",
        },
        videoModel: localStorage.getItem("video_model") || "veo-gemini", // 'veo-gemini', 'kling-kieai', 'veo-kieai', 'veo-fast-kieai'
        showSettings: false,
        videoFile: null,
        videoUrl: null,
        videoDuration: 0,
        scenes: [],
        selectedScene: 0,
        isGenerating: false,
        processingStatus: "",
        creativeDirection: "lifestyle",
        repurposeLevel: 3,
        useReferenceImage: true, // Use scene frame as reference for character consistency
        consistentCharacter: false, // Use a selected scene's generated image as reference for all
        referenceSceneId: null, // The scene ID to use as character reference
        productImage: null, // User's product image for swapping
        productDescription: "", // Description of the product
        activeTool: "select", // 'select' or 'blade' - DaVinci Resolve style tools
        fullscreenImage: null, // Image URL for fullscreen preview modal
      };

      // ============================================
      // UNDO/REDO HISTORY SYSTEM
      // ============================================
      const historyStack = [];
      let historyIndex = -1;
      const MAX_HISTORY = 50; // Limit memory usage

      /**
       * Deep clone state for history (only clone undoable properties)
       */
      function cloneStateForHistory() {
        return {
          scenes: JSON.parse(JSON.stringify(state.scenes)),
          selectedScene: state.selectedScene,
        };
      }

      /**
       * Restore state from history snapshot
       */
      function restoreStateFromHistory(snapshot) {
        state.scenes = JSON.parse(JSON.stringify(snapshot.scenes));
        state.selectedScene = snapshot.selectedScene;
      }

      /**
       * Push current state to history (call before making changes)
       */
      function pushHistory() {
        // Remove any "future" states if we're not at the end
        if (historyIndex < historyStack.length - 1) {
          historyStack.splice(historyIndex + 1);
        }

        // Add current state to history
        historyStack.push(cloneStateForHistory());
        historyIndex = historyStack.length - 1;

        // Limit history size
        if (historyStack.length > MAX_HISTORY) {
          historyStack.shift();
          historyIndex--;
        }

        console.log(
          `[History] Pushed state, index: ${historyIndex}, stack size: ${historyStack.length}`
        );
      }

      /**
       * Undo - go back in history
       */
      window.undo = () => {
        if (historyIndex <= 0) {
          console.log("[History] Nothing to undo");
          return;
        }

        historyIndex--;
        restoreStateFromHistory(historyStack[historyIndex]);
        render();
        console.log(`[History] Undo, now at index: ${historyIndex}`);
      };

      /**
       * Redo - go forward in history
       */
      window.redo = () => {
        if (historyIndex >= historyStack.length - 1) {
          console.log("[History] Nothing to redo");
          return;
        }

        historyIndex++;
        restoreStateFromHistory(historyStack[historyIndex]);
        render();
        console.log(`[History] Redo, now at index: ${historyIndex}`);
      };

      /**
       * Check if undo is available
       */
      window.canUndo = () => historyIndex > 0;

      /**
       * Check if redo is available
       */
      window.canRedo = () => historyIndex < historyStack.length - 1;

      const listeners = new Set();
      function setState(updates) {
        Object.assign(state, updates);
        listeners.forEach((fn) => fn());
      }

      /**
       * setState with history - use for undoable actions
       * Saves current state before applying changes
       */
      function setStateWithHistory(updates) {
        pushHistory();
        Object.assign(state, updates);
        listeners.forEach((fn) => fn());
      }

      function subscribe(fn) {
        listeners.add(fn);
        return () => listeners.delete(fn);
      }

      // ============================================
      // NLE CORE ARCHITECTURE
      // Non-Linear Editor Data Model & Algorithms
      // Based on DaVinci Resolve / Premiere Pro patterns
      // ============================================

      /**
       * CORE CONCEPTS:
       *
       * 1. NON-DESTRUCTIVE EDITING
       *    - Source media files are never modified
       *    - ClipInstance references source with in/out points
       *    - Multiple instances can reference same source
       *
       * 2. DATA MODEL HIERARCHY
       *    Project
       *      └── MediaBin (source assets)
       *      └── Timeline
       *            └── Track(s)
       *                  └── ClipInstance(s)
       *
       * 3. TIME REPRESENTATION
       *    - All times in seconds (float)
       *    - Frame-accurate: time = frameNumber / fps
       *    - Typical FPS: 24, 25, 29.97, 30, 60
       *
       * 4. CLIP INSTANCE PROPERTIES
       *    - sourceId: reference to MediaAsset
       *    - sourceIn/sourceOut: in/out points in SOURCE time
       *    - timelineIn: where clip starts on TIMELINE
       *    - timelineDuration: computed from source in/out
       */

      const NLE = {
        // Configuration
        fps: 30,
        frameTime: 1 / 30,

        // ============================================
        // TIMELINE STATE (separate from generation state)
        // ============================================
        timeline: {
          playheadTime: 0, // Current playhead position in seconds
          duration: 0, // Total timeline duration
          tracks: [{ id: "V1", type: "video", clips: [] }],
          selectedClipId: null,
          isPlaying: false,
          inPoint: null, // Mark in for range selection
          outPoint: null, // Mark out for range selection
        },

        // ============================================
        // MEDIA BIN (source assets)
        // ============================================
        mediaBin: new Map(), // mediaId -> { url, duration, width, height, fps }

        /**
         * Register a source media asset
         */
        registerMedia(url, duration, metadata = {}) {
          const mediaId = `media_${Date.now()}_${Math.random()
            .toString(36)
            .substr(2, 9)}`;
          this.mediaBin.set(mediaId, {
            id: mediaId,
            url,
            duration,
            width: metadata.width || 720,
            height: metadata.height || 1280,
            fps: metadata.fps || 30,
          });
          return mediaId;
        },

        /**
         * Create a ClipInstance from a source media
         * @param {string} mediaId - Source media reference
         * @param {number} sourceIn - In point in source (0-1 normalized or seconds)
         * @param {number} sourceOut - Out point in source (0-1 normalized or seconds)
         * @param {number} timelineIn - Where to place on timeline (seconds)
         */
        createClipInstance(
          mediaId,
          sourceIn,
          sourceOut,
          timelineIn,
          metadata = {}
        ) {
          const media = this.mediaBin.get(mediaId);
          if (!media) {
            console.error("Media not found:", mediaId);
            return null;
          }

          // Convert normalized (0-1) to seconds if needed
          const inSec = sourceIn <= 1 ? sourceIn * media.duration : sourceIn;
          const outSec =
            sourceOut <= 1 ? sourceOut * media.duration : sourceOut;

          return {
            id: `clip_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
            mediaId,
            sourceIn: inSec, // In point in source (seconds)
            sourceOut: outSec, // Out point in source (seconds)
            sourceDuration: outSec - inSec, // Duration of source segment
            timelineIn, // Start position on timeline
            timelineOut: timelineIn + (outSec - inSec), // End position on timeline
            speed: 1.0, // Playback speed multiplier
            ...metadata,
          };
        },

        // ============================================
        // CORE NLE FUNCTIONS
        // ============================================

        /**
         * resolveClipsAtTime(t) - Find all clips active at time t
         * This is the core function for preview/render
         *
         * @param {number} t - Time in seconds (timeline time)
         * @param {string} trackId - Optional: specific track, or all tracks
         * @returns {Array} Active clips with computed source time
         */
        resolveClipsAtTime(t, trackId = null) {
          const results = [];
          const tracks = trackId
            ? this.timeline.tracks.filter((tr) => tr.id === trackId)
            : this.timeline.tracks;

          for (const track of tracks) {
            for (const clip of track.clips) {
              // Check if time t falls within this clip's timeline range
              if (t >= clip.timelineIn && t < clip.timelineOut) {
                // Calculate corresponding source time
                const timeInClip = t - clip.timelineIn;
                const sourceTime = clip.sourceIn + timeInClip * clip.speed;

                results.push({
                  clip,
                  track,
                  sourceTime, // Where to seek in source media
                  timeInClip, // Offset from clip start
                  opacity: 1.0, // For compositing (future)
                  zIndex: track.clips.indexOf(clip),
                });
              }
            }
          }

          return results;
        },

        /**
         * splitClip(clipId, t) - Split a clip at timeline time t
         * Creates two new clips from one, preserving source references
         *
         * @param {string} clipId - The clip to split
         * @param {number} t - Timeline time to split at
         * @returns {Object} { left: ClipInstance, right: ClipInstance }
         */
        splitClip(clipId, t) {
          // Find the clip and its track
          let targetClip = null;
          let targetTrack = null;
          let clipIndex = -1;

          for (const track of this.timeline.tracks) {
            const idx = track.clips.findIndex((c) => c.id === clipId);
            if (idx !== -1) {
              targetClip = track.clips[idx];
              targetTrack = track;
              clipIndex = idx;
              break;
            }
          }

          if (!targetClip) {
            console.error("Clip not found for split:", clipId);
            return null;
          }

          // Validate split point is within clip bounds
          if (t <= targetClip.timelineIn || t >= targetClip.timelineOut) {
            console.warn("Split point outside clip bounds");
            return null;
          }

          // Calculate source time at split point
          const timeInClip = t - targetClip.timelineIn;
          const sourceTimeSplit =
            targetClip.sourceIn + timeInClip * targetClip.speed;

          // Create LEFT clip (before split)
          const leftClip = {
            ...targetClip,
            id: `${targetClip.id}_L`,
            sourceOut: sourceTimeSplit,
            sourceDuration: sourceTimeSplit - targetClip.sourceIn,
            timelineOut: t,
          };

          // Create RIGHT clip (after split)
          const rightClip = {
            ...targetClip,
            id: `${targetClip.id}_R`,
            sourceIn: sourceTimeSplit,
            sourceDuration: targetClip.sourceOut - sourceTimeSplit,
            timelineIn: t,
            // timelineOut stays the same
          };

          // Replace original clip with two new ones
          targetTrack.clips.splice(clipIndex, 1, leftClip, rightClip);

          // Recalculate timeline duration
          this.recalculateTimelineDuration();

          return { left: leftClip, right: rightClip };
        },

        /**
         * trimClipEdge(clipId, edge, newTime, ripple)
         * Adjust clip in/out point with optional ripple
         *
         * @param {string} clipId - The clip to trim
         * @param {string} edge - 'in' or 'out'
         * @param {number} newTime - New timeline time for the edge
         * @param {boolean} ripple - If true, shift subsequent clips
         * @returns {Object} Updated clip
         */
        trimClipEdge(clipId, edge, newTime, ripple = false) {
          let targetClip = null;
          let targetTrack = null;
          let clipIndex = -1;

          for (const track of this.timeline.tracks) {
            const idx = track.clips.findIndex((c) => c.id === clipId);
            if (idx !== -1) {
              targetClip = track.clips[idx];
              targetTrack = track;
              clipIndex = idx;
              break;
            }
          }

          if (!targetClip) return null;

          const media = this.mediaBin.get(targetClip.mediaId);
          if (!media) return null;

          const oldTimelineIn = targetClip.timelineIn;
          const oldTimelineOut = targetClip.timelineOut;

          if (edge === "in") {
            // TRIM IN: Moving the start point
            // Can't trim past the out point (leave minimum 1 frame)
            const minTime = targetClip.timelineIn; // Where we started
            const maxTime = targetClip.timelineOut - this.frameTime;
            newTime = Math.max(0, Math.min(newTime, maxTime));

            // Calculate how much we're trimming
            const trimDelta = newTime - oldTimelineIn;

            // Check if we have enough source media (can't reveal more than exists)
            const newSourceIn =
              targetClip.sourceIn + trimDelta * targetClip.speed;
            if (newSourceIn < 0) {
              // Can't reveal before source start
              newTime = oldTimelineIn - targetClip.sourceIn / targetClip.speed;
              newTime = Math.max(0, newTime);
            }

            // Update clip
            const actualTrimDelta = newTime - oldTimelineIn;
            targetClip.sourceIn =
              targetClip.sourceIn + actualTrimDelta * targetClip.speed;
            targetClip.timelineIn = newTime;
            targetClip.sourceDuration =
              targetClip.sourceOut - targetClip.sourceIn;

            // Ripple: shift this and all subsequent clips
            if (ripple && actualTrimDelta !== 0) {
              // For trim in with ripple, we shift everything by the trim amount
              for (let i = clipIndex; i < targetTrack.clips.length; i++) {
                if (i > clipIndex) {
                  targetTrack.clips[i].timelineIn -= actualTrimDelta;
                  targetTrack.clips[i].timelineOut -= actualTrimDelta;
                }
              }
            }
          } else if (edge === "out") {
            // TRIM OUT: Moving the end point
            const minTime = targetClip.timelineIn + this.frameTime;
            newTime = Math.max(minTime, newTime);

            // Calculate how much we're trimming
            const trimDelta = newTime - oldTimelineOut;

            // Check if we have enough source media
            const newSourceOut =
              targetClip.sourceOut + trimDelta * targetClip.speed;
            if (newSourceOut > media.duration) {
              // Can't extend past source end
              newTime =
                oldTimelineOut +
                (media.duration - targetClip.sourceOut) / targetClip.speed;
            }

            // Update clip
            const actualTrimDelta = newTime - oldTimelineOut;
            targetClip.sourceOut =
              targetClip.sourceOut + actualTrimDelta * targetClip.speed;
            targetClip.timelineOut = newTime;
            targetClip.sourceDuration =
              targetClip.sourceOut - targetClip.sourceIn;

            // Ripple: shift all subsequent clips
            if (ripple && actualTrimDelta !== 0) {
              for (let i = clipIndex + 1; i < targetTrack.clips.length; i++) {
                targetTrack.clips[i].timelineIn += actualTrimDelta;
                targetTrack.clips[i].timelineOut += actualTrimDelta;
              }
            }
          }

          this.recalculateTimelineDuration();
          return targetClip;
        },

        /**
         * insertClip(clip, timelineIn) - Insert clip at position, rippling others
         */
        insertClip(clip, timelineIn, trackId = "V1") {
          const track = this.timeline.tracks.find((t) => t.id === trackId);
          if (!track) return null;

          // Find insertion point
          let insertIndex = track.clips.length;
          for (let i = 0; i < track.clips.length; i++) {
            if (track.clips[i].timelineIn >= timelineIn) {
              insertIndex = i;
              break;
            }
          }

          // Calculate clip duration
          const clipDuration =
            clip.sourceDuration || clip.sourceOut - clip.sourceIn;

          // Update clip timeline position
          clip.timelineIn = timelineIn;
          clip.timelineOut = timelineIn + clipDuration;

          // Ripple: shift all clips at and after insertion point
          const shiftAmount = clipDuration;
          for (let i = insertIndex; i < track.clips.length; i++) {
            track.clips[i].timelineIn += shiftAmount;
            track.clips[i].timelineOut += shiftAmount;
          }

          // Insert the clip
          track.clips.splice(insertIndex, 0, clip);

          this.recalculateTimelineDuration();
          return clip;
        },

        /**
         * deleteClip(clipId, ripple) - Remove clip with optional ripple
         */
        deleteClip(clipId, ripple = true) {
          for (const track of this.timeline.tracks) {
            const idx = track.clips.findIndex((c) => c.id === clipId);
            if (idx !== -1) {
              const deletedClip = track.clips[idx];
              const gapDuration =
                deletedClip.timelineOut - deletedClip.timelineIn;

              // Remove the clip
              track.clips.splice(idx, 1);

              // Ripple: shift subsequent clips to close the gap
              if (ripple) {
                for (let i = idx; i < track.clips.length; i++) {
                  track.clips[i].timelineIn -= gapDuration;
                  track.clips[i].timelineOut -= gapDuration;
                }
              }

              this.recalculateTimelineDuration();
              return deletedClip;
            }
          }
          return null;
        },

        /**
         * moveClip(clipId, newTimelineIn) - Move clip to new position
         */
        moveClip(clipId, newTimelineIn) {
          for (const track of this.timeline.tracks) {
            const clip = track.clips.find((c) => c.id === clipId);
            if (clip) {
              const duration = clip.timelineOut - clip.timelineIn;
              clip.timelineIn = Math.max(0, newTimelineIn);
              clip.timelineOut = clip.timelineIn + duration;

              // Re-sort clips by timeline position
              track.clips.sort((a, b) => a.timelineIn - b.timelineIn);

              this.recalculateTimelineDuration();
              return clip;
            }
          }
          return null;
        },

        /**
         * reorderClips() - Reorder clips to close gaps (ripple delete gaps)
         */
        reorderClips(trackId = "V1") {
          const track = this.timeline.tracks.find((t) => t.id === trackId);
          if (!track) return;

          // Sort by current position
          track.clips.sort((a, b) => a.timelineIn - b.timelineIn);

          // Close all gaps
          let currentTime = 0;
          for (const clip of track.clips) {
            const duration = clip.timelineOut - clip.timelineIn;
            clip.timelineIn = currentTime;
            clip.timelineOut = currentTime + duration;
            currentTime += duration;
          }

          this.recalculateTimelineDuration();
        },

        /**
         * Recalculate total timeline duration
         */
        recalculateTimelineDuration() {
          let maxTime = 0;
          for (const track of this.timeline.tracks) {
            for (const clip of track.clips) {
              maxTime = Math.max(maxTime, clip.timelineOut);
            }
          }
          this.timeline.duration = maxTime;
        },

        /**
         * Snap time to nearest frame boundary
         */
        snapToFrame(time) {
          return Math.round(time * this.fps) / this.fps;
        },

        /**
         * Convert legacy scene format to NLE ClipInstance format
         */
        sceneToClipInstance(scene) {
          if (!scene.generatedVideo) return null;

          // Register media if not already done
          let mediaId = scene._nleMediaId;
          if (!mediaId || !this.mediaBin.has(mediaId)) {
            mediaId = this.registerMedia(scene.generatedVideo, scene.duration, {
              isUploaded: scene.isUploaded,
              fileName: scene.fileName,
            });
            scene._nleMediaId = mediaId;
          }

          const media = this.mediaBin.get(mediaId);

          return {
            id: scene.id,
            mediaId,
            sourceIn: scene.trim.in * media.duration,
            sourceOut: scene.trim.out * media.duration,
            sourceDuration: (scene.trim.out - scene.trim.in) * media.duration,
            timelineIn: 0, // Will be computed based on order
            timelineOut: (scene.trim.out - scene.trim.in) * media.duration,
            speed: 1.0,
            // Legacy compatibility
            _legacyScene: scene,
          };
        },

        /**
         * Build NLE timeline from legacy scenes array
         */
        buildTimelineFromScenes(scenes) {
          const scenesWithVideo = scenes
            .filter((s) => s.generatedVideo)
            .sort((a, b) => a.order - b.order);

          // Clear existing clips
          this.timeline.tracks[0].clips = [];
          this.mediaBin.clear();

          let currentTime = 0;
          for (const scene of scenesWithVideo) {
            const clip = this.sceneToClipInstance(scene);
            if (clip) {
              clip.timelineIn = currentTime;
              clip.timelineOut = currentTime + clip.sourceDuration;
              this.timeline.tracks[0].clips.push(clip);
              currentTime = clip.timelineOut;
            }
          }

          this.recalculateTimelineDuration();
          return this.timeline.tracks[0].clips;
        },

        /**
         * Get clip at specific timeline index
         */
        getClipAtIndex(index, trackId = "V1") {
          const track = this.timeline.tracks.find((t) => t.id === trackId);
          return track?.clips[index] || null;
        },

        /**
         * Find clip and index at timeline time
         */
        findClipAtTime(t, trackId = "V1") {
          const track = this.timeline.tracks.find((t) => t.id === trackId);
          if (!track) return null;

          for (let i = 0; i < track.clips.length; i++) {
            const clip = track.clips[i];
            if (t >= clip.timelineIn && t < clip.timelineOut) {
              return { clip, index: i, track };
            }
          }
          return null;
        },
      };

      // ============================================
      // VIEWER / PREVIEW ENGINE
      // Responsible for real-time preview updates
      // ============================================
      const ViewerEngine = {
        currentFrame: null,
        frameCache: new Map(), // time -> ImageBitmap
        cacheSize: 30, // Frames to keep in cache
        prefetchRange: 1.0, // Seconds ahead/behind to prefetch

        /**
         * Render frame at time t
         * Core preview function: time -> frame
         */
        async renderFrameAtTime(t, videoElement) {
          // Find active clips at this time
          const activeClips = NLE.resolveClipsAtTime(t);

          if (activeClips.length === 0) {
            // No clips at this time - show black/placeholder
            return null;
          }

          // For single-track editor, just use first clip
          const { clip, sourceTime } = activeClips[0];
          const media = NLE.mediaBin.get(clip.mediaId);

          if (!media || !videoElement) return null;

          // Seek video to correct source time
          if (Math.abs(videoElement.currentTime - sourceTime) > 0.05) {
            videoElement.currentTime = sourceTime;
          }

          return { clip, sourceTime, media };
        },

        /**
         * Handle playhead move - update preview
         */
        async updatePreview(t) {
          const player = document.getElementById("preview-player");
          if (!player) return;

          const result = await this.renderFrameAtTime(t, player);
          if (result) {
            // Video element will show the frame
            NLE.timeline.playheadTime = t;
          }
        },

        /**
         * Clear frame cache
         */
        clearCache() {
          this.frameCache.clear();
        },
      };

      // ============================================
      // VIDEO PROCESSING - Extract real keyframes
      // ============================================
      async function extractKeyframes(videoFile, numFrames = 5) {
        return new Promise((resolve, reject) => {
          const video = document.createElement("video");
          video.preload = "metadata";
          video.muted = true;
          video.playsInline = true;

          const url = URL.createObjectURL(videoFile);
          video.src = url;

          video.onloadedmetadata = async () => {
            const duration = video.duration;
            const interval = duration / (numFrames + 1);
            const frames = [];

            video.currentTime = 0;

            for (let i = 1; i <= numFrames; i++) {
              const time = interval * i;
              await seekToTime(video, time);
              const canvas = document.createElement("canvas");
              canvas.width = video.videoWidth;
              canvas.height = video.videoHeight;
              const ctx = canvas.getContext("2d");
              ctx.drawImage(video, 0, 0);
              frames.push({
                time,
                dataUrl: canvas.toDataURL("image/jpeg", 0.8),
              });
            }

            URL.revokeObjectURL(url);
            resolve({ frames, duration });
          };

          video.onerror = () => {
            URL.revokeObjectURL(url);
            reject(new Error("Failed to load video"));
          };
        });
      }

      function seekToTime(video, time) {
        return new Promise((resolve) => {
          video.currentTime = time;
          video.onseeked = () => resolve();
        });
      }

      // ============================================
      // VIDEO UPLOAD TO GEMINI FILES API (for audio analysis)
      // ============================================
      async function uploadVideoToGemini(videoFile) {
        const apiKey = state.apiKeys.gemini;

        console.log(
          "Uploading video to Gemini Files API for audio analysis..."
        );
        console.log(
          "Video size:",
          (videoFile.size / 1024 / 1024).toFixed(2),
          "MB"
        );

        // For videos under 20MB, use inline data directly
        if (videoFile.size < 20 * 1024 * 1024) {
          console.log("Using inline video data (small file)");
          const base64 = await fileToBase64(videoFile);
          return {
            inlineData: true,
            base64: base64,
            mimeType: videoFile.type || "video/mp4",
          };
        }

        // For larger files, try the multipart upload
        try {
          const metadata = {
            file: { displayName: videoFile.name },
          };

          const formData = new FormData();
          formData.append(
            "metadata",
            new Blob([JSON.stringify(metadata)], { type: "application/json" })
          );
          formData.append("file", videoFile);

          const response = await fetch(
            `https://generativelanguage.googleapis.com/upload/v1beta/files?uploadType=multipart&key=${apiKey}`,
            {
              method: "POST",
              body: formData,
            }
          );

          if (!response.ok) {
            const errText = await response.text();
            console.error("Upload error:", errText);
            throw new Error("Upload failed: " + errText);
          }

          const uploadData = await response.json();
          console.log("Upload response:", uploadData);

          const fileUri = uploadData.file?.uri;
          const fileName = uploadData.file?.name;

          if (!fileUri) {
            throw new Error("No fileUri returned from upload");
          }

          // Wait for processing
          let fileState = uploadData.file?.state;
          let attempts = 0;
          while (fileState === "PROCESSING" && attempts < 60) {
            await new Promise((r) => setTimeout(r, 2000));
            attempts++;
            setState({
              processingStatus: `Processing video... (${attempts * 2}s)`,
            });

            const statusResponse = await fetch(
              `https://generativelanguage.googleapis.com/v1beta/${fileName}?key=${apiKey}`
            );
            const statusData = await statusResponse.json();
            fileState = statusData.state;
            console.log("File state:", fileState);
          }

          if (fileState !== "ACTIVE") {
            throw new Error(`Video processing failed: ${fileState}`);
          }

          console.log("Video uploaded and ready:", fileUri);
          return { fileUri, fileName, inlineData: false };
        } catch (error) {
          console.warn(
            "File upload failed, falling back to inline data:",
            error
          );
          // Fallback to inline data even for larger files (may fail for very large)
          const base64 = await fileToBase64(videoFile);
          return {
            inlineData: true,
            base64: base64,
            mimeType: videoFile.type || "video/mp4",
          };
        }
      }

      function fileToBase64(file) {
        return new Promise((resolve, reject) => {
          const reader = new FileReader();
          reader.onload = () => {
            const base64 = reader.result.split(",")[1];
            resolve(base64);
          };
          reader.onerror = reject;
          reader.readAsDataURL(file);
        });
      }

      // ============================================
      // SCENE SEGMENTATION with AUDIO using Gemini
      // ============================================
      async function analyzeVideoWithGemini(videoFileInfo, frames, duration) {
        const apiKey = state.apiKeys.gemini;
        if (!apiKey) throw new Error("Gemini API key not set");

        // Calculate scene count (aim for ~7 seconds per scene)
        const targetSceneDuration = 7;
        const estimatedScenes = Math.max(
          1,
          Math.ceil(duration / targetSceneDuration)
        );

        const prompt = `Analyze this video WITH AUDIO for recreation. Duration: ${duration.toFixed(
          1
        )} seconds.

TASK: Listen to the AUDIO and transcribe dialogue. Divide into ${estimatedScenes} scenes of ~${targetSceneDuration} seconds each.

For EACH scene provide:
- scene_id: S1, S2, etc
- start: timestamp in seconds (number)
- end: timestamp in seconds (number)  
- visual_summary: What we see - subject appearance, background, lighting (use single quotes for any quotes, no double quotes)
- action_summary: Movements and gestures (use single quotes, no double quotes)
- camera_summary: Shot type and camera movement
- dialogue_transcript: Exact words spoken OR [no dialogue] - IMPORTANT: replace any double quotes with single quotes
- text_on_screen: Visible text (use single quotes)
- audio_elements: Music and sounds description

CRITICAL JSON RULES:
- Return ONLY valid JSON, no markdown
- Use double quotes for JSON property names and string delimiters ONLY
- Inside string VALUES, use single quotes instead of double quotes
- No line breaks inside string values
- No trailing commas

Example format:
{"scenes":[{"scene_id":"S1","start":0,"end":7,"visual_summary":"Woman in white shirt speaking to camera","action_summary":"She gestures with hands while talking","camera_summary":"medium close-up, static","dialogue_transcript":"Hey guys, welcome back to my channel","text_on_screen":"","audio_elements":"upbeat background music"}]}`;

        // Build request parts based on whether we have inline data or file URI
        const parts = [{ text: prompt }];

        if (videoFileInfo.inlineData) {
          // Use inline video data
          console.log("Using inline video data for analysis");
          parts.push({
            inlineData: {
              mimeType: videoFileInfo.mimeType,
              data: videoFileInfo.base64,
            },
          });
        } else {
          // Use file URI reference
          console.log("Using file URI for analysis:", videoFileInfo.fileUri);
          parts.push({
            fileData: {
              mimeType: "video/mp4",
              fileUri: videoFileInfo.fileUri,
            },
          });
        }

        // Also add keyframes for additional visual reference
        frames.forEach((frame, i) => {
          if (frame.dataUrl && frame.dataUrl.includes(",")) {
            parts.push({
              inlineData: {
                mimeType: "image/jpeg",
                data: frame.dataUrl.split(",")[1],
              },
            });
          }
        });

        parts.push({
          text: `\n\nKeyframe timestamps for reference: ${frames
            .map((f, i) => `Frame ${i + 1}: ${f.time.toFixed(1)}s`)
            .join(
              ", "
            )}\n\nNOW LISTEN TO THE AUDIO CAREFULLY AND TRANSCRIBE ALL DIALOGUE. Analyze and return the JSON.`,
        });

        console.log("Sending video to Gemini for analysis with audio...");

        // Use gemini-1.5-flash for video analysis (better for multimodal)
        const response = await fetch(
          `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`,
          {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
              contents: [{ parts }],
              generationConfig: {
                temperature: 0.1, // Lower temperature for more accurate transcription
                maxOutputTokens: 8000,
              },
            }),
          }
        );

        if (!response.ok) {
          const error = await response.json();
          console.error("Gemini error:", error);
          throw new Error(error.error?.message || "Gemini API error");
        }

        const data = await response.json();
        const text = data.candidates?.[0]?.content?.parts?.[0]?.text || "";

        console.log("=== GEMINI VIDEO ANALYSIS RESPONSE ===");
        console.log(text);
        console.log("======================================");

        // Parse JSON with error handling
        const result = safeParseJSON(text, "video analysis");

        // Log the transcribed dialogue for verification
        console.log("=== TRANSCRIBED DIALOGUE BY SCENE ===");
        result.scenes.forEach((s) => {
          console.log(
            `Scene ${s.scene_id} (${s.start}s - ${s.end}s): "${s.dialogue_transcript}"`
          );
        });
        console.log("=====================================");

        return result;
      }

      // ============================================
      // SAFE JSON PARSING - handles common LLM output issues
      // ============================================
      function safeParseJSON(text, context = "response") {
        if (!text || typeof text !== "string") {
          throw new Error(`No text to parse in ${context}`);
        }

        console.log(`=== Parsing JSON for ${context} ===`);
        console.log("Raw text length:", text.length);

        // Step 1: Remove markdown code fences and trim
        let cleaned = text
          .replace(/```json\s*/gi, "")
          .replace(/```\s*/g, "")
          .trim();

        // Step 2: Find the JSON object
        const firstBrace = cleaned.indexOf("{");
        const lastBrace = cleaned.lastIndexOf("}");

        if (firstBrace === -1 || lastBrace === -1 || lastBrace <= firstBrace) {
          console.error(
            "Could not find JSON braces in:",
            cleaned.substring(0, 300)
          );
          throw new Error(`Could not find JSON in ${context}`);
        }

        let jsonStr = cleaned.substring(firstBrace, lastBrace + 1);

        // Step 3: Clean the JSON string
        // Remove actual newlines, carriage returns, tabs
        jsonStr = jsonStr.replace(/[\r\n\t]/g, " ");

        // Collapse multiple spaces
        jsonStr = jsonStr.replace(/\s+/g, " ");

        // Fix trailing commas
        jsonStr = jsonStr.replace(/,\s*}/g, "}").replace(/,\s*]/g, "]");

        console.log("Cleaned JSON (first 300):", jsonStr.substring(0, 300));

        // Step 4: Try to parse
        try {
          const result = JSON.parse(jsonStr);
          console.log("JSON parsed successfully");
          return result;
        } catch (e1) {
          console.log("Parse failed:", e1.message);

          // Step 5: Try to fix unescaped quotes in string values
          // This is a simplified approach - find string values and escape internal quotes
          try {
            // Replace problematic characters that might be in dialogue
            let fixed = jsonStr;

            // Common problematic patterns
            fixed = fixed
              // Fix curly quotes
              .replace(/[\u201C\u201D]/g, "'")
              .replace(/[\u2018\u2019]/g, "'")
              // Fix em-dashes
              .replace(/[\u2014\u2013]/g, "-");

            // Try a more aggressive fix: find quoted strings and escape any internal double quotes
            // Match: "key": "value with "problem" quotes"
            // This regex finds values after colons and tries to fix them
            fixed = fixed.replace(/:\s*"([^"]*)"/g, (match, content) => {
              // If the content seems cut off (no closing structure), it might have internal quotes
              return ': "' + content + '"';
            });

            const result = JSON.parse(fixed);
            console.log("JSON parsed after character fix");
            return result;
          } catch (e2) {
            console.log("Character fix failed:", e2.message);
          }

          // Step 6: Try to manually build the object from key-value pairs
          try {
            console.log("Attempting manual extraction...");
            const result = {};

            // For scenes array
            const scenesMatch = jsonStr.match(/"scenes"\s*:\s*\[/);
            if (scenesMatch) {
              // Try to extract individual scene objects
              const scenes = [];
              const sceneRegex = /\{[^{}]*"scene_id"\s*:\s*"([^"]+)"[^{}]*\}/g;
              let sceneMatch;
              while ((sceneMatch = sceneRegex.exec(jsonStr)) !== null) {
                try {
                  // Clean this individual scene
                  let sceneStr = sceneMatch[0]
                    .replace(/[\r\n\t]/g, " ")
                    .replace(/,\s*}/g, "}");
                  scenes.push(JSON.parse(sceneStr));
                } catch (se) {
                  console.log(
                    "Could not parse scene:",
                    sceneMatch[0].substring(0, 100)
                  );
                }
              }
              if (scenes.length > 0) {
                result.scenes = scenes;
                console.log("Extracted", scenes.length, "scenes manually");
                return result;
              }
            }

            // For image_prompt
            const promptMatch = jsonStr.match(
              /"image_prompt"\s*:\s*"([^"]*(?:\\.[^"]*)*)"/
            );
            if (promptMatch) {
              result.image_prompt = promptMatch[1];
              const negMatch = jsonStr.match(
                /"negative_prompt"\s*:\s*"([^"]*)"/
              );
              result.negative_prompt = negMatch ? negMatch[1] : "";
              console.log("Extracted image prompt manually");
              return result;
            }
          } catch (e3) {
            console.log("Manual extraction failed:", e3.message);
          }

          // Final fallback - show the actual error
          console.error(
            "All parsing attempts failed. Raw JSON:",
            jsonStr.substring(0, 500)
          );
          throw new Error(`Failed to parse ${context}. Please try again.`);
        }
      }

      // ============================================
      // EXTRACT FIRST FRAME OF EACH SCENE
      // ============================================
      async function extractSceneFirstFrames(videoFile, scenes) {
        return new Promise((resolve, reject) => {
          const video = document.createElement("video");
          video.preload = "metadata";
          video.muted = true;
          video.playsInline = true;

          const url = URL.createObjectURL(videoFile);
          video.src = url;

          video.onloadedmetadata = async () => {
            const framesMap = {};

            for (const scene of scenes) {
              // Get the first frame of this scene
              const time = scene.start + 0.1; // Slightly after start to avoid black frames
              await seekToTime(video, time);

              const canvas = document.createElement("canvas");
              canvas.width = video.videoWidth;
              canvas.height = video.videoHeight;
              const ctx = canvas.getContext("2d");
              ctx.drawImage(video, 0, 0);

              framesMap[scene.scene_id] = canvas.toDataURL("image/jpeg", 0.9);
            }

            URL.revokeObjectURL(url);
            resolve(framesMap);
          };

          video.onerror = () => {
            URL.revokeObjectURL(url);
            reject(new Error("Failed to extract scene frames"));
          };
        });
      }

      // ============================================
      // IMAGE PROMPT GENERATION using Gemini (Hyper-detailed)
      // ============================================
      async function generateImagePrompt(
        scene,
        creativeDirection,
        repurposeLevel
      ) {
        const apiKey = state.apiKeys.gemini;
        if (!apiKey) throw new Error("Gemini API key not set");

        console.log(
          "generateImagePrompt called, has originalKeyframe:",
          !!scene.originalKeyframe
        );

        // Build the request with the original keyframe image for hyper-detailed analysis
        const imageAnalysisPrompt = `Analyze this image and write an image generation prompt.

Describe:
- Subject: appearance, clothing, expression, pose, age
- Setting: location, background, props
- Lighting: direction, quality, color temperature
- Composition: camera angle, framing
- Style: aesthetic, colors, mood

Creative direction: ${creativeDirection}
Repurpose level: ${repurposeLevel}/10

CRITICAL JSON RULES:
- Return ONLY valid JSON
- NO double quotes inside the prompt text - use single quotes only
- NO line breaks inside strings
- NO markdown formatting

Return exactly this format:
{"image_prompt":"your detailed prompt here using single quotes only","negative_prompt":"text, watermark, blurry, low quality, distorted"}`;

        // Include the original keyframe for visual analysis
        // Safely extract base64 data from data URL
        let imageBase64 = "";
        if (scene.originalKeyframe) {
          if (scene.originalKeyframe.includes(",")) {
            imageBase64 = scene.originalKeyframe.split(",")[1];
          } else {
            imageBase64 = scene.originalKeyframe;
          }
        }

        // Build request - with or without image
        const parts = [];

        if (imageBase64) {
          parts.push({
            inlineData: {
              mimeType: "image/jpeg",
              data: imageBase64,
            },
          });
          parts.push({ text: imageAnalysisPrompt });
        } else {
          // Fallback: use text-based prompt generation if no image available
          console.log("No image available, using text-based prompt generation");
          const textOnlyPrompt = `Write an image generation prompt based on this scene:

- Visual: ${scene.segmentationData?.visualSummary || "A professional scene"}
- Action: ${scene.segmentationData?.actionSummary || "Subject in action"}
- Camera: ${scene.segmentationData?.cameraSummary || "Medium shot"}

CRITICAL: Return ONLY valid JSON. NO double quotes inside prompt text - use single quotes only.

{"image_prompt":"detailed prompt here with single quotes only","negative_prompt":"text, watermark, blurry, low quality"}`;
          parts.push({ text: textOnlyPrompt });
        }

        const requestBody = {
          contents: [{ parts }],
          generationConfig: { temperature: 0.7, maxOutputTokens: 1500 },
        };

        const response = await fetch(
          `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`,
          {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify(requestBody),
          }
        );

        if (!response.ok) throw new Error("Failed to generate image prompt");

        const data = await response.json();
        const text = data.candidates?.[0]?.content?.parts?.[0]?.text || "";
        console.log("Image prompt analysis:", text.substring(0, 500));

        return safeParseJSON(text, "image prompt");
      }

      // ============================================
      // IMAGE GENERATION using Gemini 3 Pro Image Preview (Nano Banana Pro)
      // ============================================
      async function generateImage(
        imagePrompt,
        negativePrompt,
        referenceImage = null
      ) {
        const apiKey = state.apiKeys.gemini;
        if (!apiKey) throw new Error("Gemini API key not set");

        const fullPrompt = `Generate a photorealistic 9:16 vertical image for a TikTok/Instagram video: ${imagePrompt}${
          negativePrompt ? `. Avoid: ${negativePrompt}` : ""
        }`;
        console.log(
          "Generating image with Nano Banana Pro:",
          fullPrompt.substring(0, 150) + "..."
        );
        console.log("Using reference image:", !!referenceImage);

        // Build request parts
        const parts = [];

        // If reference image is provided, include it for character consistency
        if (referenceImage) {
          let refBase64 = referenceImage;
          let refMimeType = "image/jpeg";

          if (referenceImage.startsWith("data:")) {
            const mimeMatch = referenceImage.match(/data:([^;]+);/);
            if (mimeMatch) refMimeType = mimeMatch[1];
            refBase64 = referenceImage.split(",")[1];
          }

          parts.push({
            inlineData: {
              mimeType: refMimeType,
              data: refBase64,
            },
          });
          parts.push({
            text: `Reference image above shows the exact character/subject to recreate. Generate a new image maintaining the SAME PERSON with identical face, features, skin tone, hair, and overall appearance. The new scene should be: ${fullPrompt}`,
          });
        } else {
          parts.push({ text: fullPrompt });
        }

        const response = await fetch(
          `https://generativelanguage.googleapis.com/v1beta/models/gemini-3-pro-image-preview:generateContent?key=${apiKey}`,
          {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
              contents: [{ parts }],
              generationConfig: {
                responseModalities: ["TEXT", "IMAGE"],
              },
            }),
          }
        );

        const responseText = await response.text();
        console.log(
          "Gemini response (first 500 chars):",
          responseText.substring(0, 500)
        );

        if (!response.ok) {
          let errorMsg = `API error: ${response.status}`;
          try {
            const errorData = JSON.parse(responseText);
            errorMsg = errorData.error?.message || errorMsg;
          } catch (e) {}
          throw new Error(errorMsg);
        }

        const data = JSON.parse(responseText);

        // Check for blocked content
        if (data.candidates?.[0]?.finishReason === "SAFETY") {
          throw new Error(
            "Image blocked by safety filters. Try simplifying the prompt."
          );
        }

        if (!data.candidates?.[0]?.content?.parts) {
          console.error("No parts in response:", data);
          throw new Error(
            "No content in response. Model may have rejected the prompt."
          );
        }

        // Find the image part in the response
        const dataParts = data.candidates[0].content.parts;
        const imagePart = dataParts.find((p) =>
          p.inlineData?.mimeType?.startsWith("image/")
        );

        if (imagePart) {
          const base64Image = `data:${imagePart.inlineData.mimeType};base64,${imagePart.inlineData.data}`;
          console.log(
            "Image generated successfully (base64 length:",
            imagePart.inlineData.data.length,
            ")"
          );
          return base64Image;
        }

        // Check if there's text explaining why no image
        const textPart = dataParts.find((p) => p.text);
        if (textPart) {
          console.log("Model returned text instead of image:", textPart.text);
          throw new Error(
            `No image generated. Model said: ${textPart.text.substring(0, 150)}`
          );
        }

        throw new Error("No image in response");
      }

      // ============================================
      // PRODUCT SWAP using Gemini Image Editing
      // ============================================
      async function swapProductInImage(
        sceneImage,
        productImage,
        sceneDescription
      ) {
        const apiKey = state.apiKeys.gemini;
        if (!apiKey) throw new Error("Gemini API key not set");

        console.log("Swapping product in scene using Gemini...");

        // Extract base64 data from data URLs
        const getBase64Data = (dataUrl) => {
          if (dataUrl.startsWith("data:")) {
            return dataUrl.split(",")[1];
          }
          return dataUrl;
        };

        const getMimeType = (dataUrl) => {
          if (dataUrl.startsWith("data:")) {
            const match = dataUrl.match(/data:([^;]+);/);
            return match ? match[1] : "image/jpeg";
          }
          return "image/jpeg";
        };

        const prompt = `You are an expert image editor. I'm providing two images:

IMAGE 1 (Scene): A generated scene for a TikTok/Instagram video showing: ${sceneDescription}

IMAGE 2 (Product): The actual product I want to place in this scene.

TASK: Edit the SCENE image to replace any placeholder/generic product with MY PRODUCT from Image 2.

Requirements:
- Keep the exact same scene composition, lighting, background, and style
- Replace only the product with my product from Image 2
- Match the product's lighting, shadows, and perspective to fit naturally
- Maintain the 9:16 vertical aspect ratio
- The result should look like a professional product photo in this scene

Generate the edited image.`;

        const response = await fetch(
          `https://generativelanguage.googleapis.com/v1beta/models/gemini-3-pro-image-preview:generateContent?key=${apiKey}`,
          {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
              contents: [
                {
                  parts: [
                    { text: prompt },
                    {
                      inlineData: {
                        mimeType: getMimeType(sceneImage),
                        data: getBase64Data(sceneImage),
                      },
                    },
                    {
                      inlineData: {
                        mimeType: getMimeType(productImage),
                        data: getBase64Data(productImage),
                      },
                    },
                  ],
                },
              ],
              generationConfig: {
                responseModalities: ["TEXT", "IMAGE"],
              },
            }),
          }
        );

        const responseText = await response.text();
        console.log(
          "Product swap response (first 500 chars):",
          responseText.substring(0, 500)
        );

        if (!response.ok) {
          let errorMsg = `API error: ${response.status}`;
          try {
            const errorData = JSON.parse(responseText);
            errorMsg = errorData.error?.message || errorMsg;
          } catch (e) {}
          throw new Error(errorMsg);
        }

        const data = JSON.parse(responseText);

        if (data.candidates?.[0]?.finishReason === "SAFETY") {
          throw new Error("Product swap blocked by safety filters.");
        }

        if (!data.candidates?.[0]?.content?.parts) {
          throw new Error("No content in response.");
        }

        const parts = data.candidates[0].content.parts;
        const imagePart = parts.find((p) =>
          p.inlineData?.mimeType?.startsWith("image/")
        );

        if (imagePart) {
          const base64Image = `data:${imagePart.inlineData.mimeType};base64,${imagePart.inlineData.data}`;
          console.log("Product swap successful!");
          return base64Image;
        }

        const textPart = parts.find((p) => p.text);
        if (textPart) {
          throw new Error(
            `Product swap failed: ${textPart.text.substring(0, 150)}`
          );
        }

        throw new Error("No image in response");
      }

      // ============================================
      // CLEAN IMAGE - Remove social media text overlays using AI
      // Only removes TikTok/Instagram style captions, NOT product labels
      // ============================================
      async function cleanImageWithAI(sourceImage) {
        const apiKey = state.apiKeys.gemini;
        if (!apiKey) throw new Error("Gemini API key not set");

        console.log(
          "[CleanImage] Starting - removing social media overlays only..."
        );

        // Extract base64 data from data URL
        const getBase64Data = (dataUrl) => {
          if (!dataUrl) {
            throw new Error("No image data to clean");
          }
          if (dataUrl.startsWith("data:")) {
            return dataUrl.split(",")[1];
          }
          return dataUrl;
        };

        const getMimeType = (dataUrl) => {
          if (dataUrl && dataUrl.startsWith("data:")) {
            const match = dataUrl.match(/data:([^;]+);/);
            return match ? match[1] : "image/jpeg";
          }
          return "image/jpeg";
        };

        let base64Data, mimeType;
        try {
          base64Data = getBase64Data(sourceImage);
          mimeType = getMimeType(sourceImage);
          console.log(
            "[CleanImage] Image prepared, mime:",
            mimeType,
            "size:",
            base64Data.length
          );
        } catch (e) {
          console.error("[CleanImage] Error extracting image data:", e);
          throw e;
        }

        // Specific prompt for social media overlay removal ONLY
        const cleanPrompt = `Look at this image from a TikTok or Instagram video. Generate a NEW clean version that removes ONLY the social media text overlays.

REMOVE ONLY THESE (social media additions):
- TikTok/Instagram/Snapchat style caption text overlays
- Subtitles or captions added during video editing
- "POV:" text, trending hashtags, or viral caption text
- Emojis or stickers added as overlays
- Username watermarks or social media handles
- Any text that appears to float ON TOP of the image (not part of the real scene)

KEEP EVERYTHING ELSE - DO NOT REMOVE:
- Product labels and brand names ON the actual products
- Text printed on packaging, bottles, boxes
- Store signs, street signs, any signage in the background
- Text on clothing (t-shirt prints, logos)
- Any text that is physically part of the real-world scene
- All people, products, objects, backgrounds exactly as they are

The goal is to remove the "TikTok caption layer" while keeping the authentic scene underneath intact.

Generate a photorealistic 9:16 vertical image that looks exactly like this scene but without the social media text overlays.`;

        console.log("[CleanImage] Sending request to Gemini...");

        const response = await fetch(
          `https://generativelanguage.googleapis.com/v1beta/models/gemini-3-pro-image-preview:generateContent?key=${apiKey}`,
          {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
              contents: [
                {
                  parts: [
                    {
                      inlineData: {
                        mimeType: mimeType,
                        data: base64Data,
                      },
                    },
                    { text: cleanPrompt },
                  ],
                },
              ],
              generationConfig: {
                responseModalities: ["IMAGE", "TEXT"],
              },
            }),
          }
        );

        const responseText = await response.text();
        console.log("[CleanImage] Response status:", response.status);
        console.log(
          "[CleanImage] Response preview:",
          responseText.substring(0, 500)
        );

        if (!response.ok) {
          let errorMsg = `API error: ${response.status}`;
          try {
            const errorData = JSON.parse(responseText);
            errorMsg = errorData.error?.message || errorMsg;
            console.error("[CleanImage] API error details:", errorData);
          } catch (e) {}
          throw new Error(errorMsg);
        }

        const data = JSON.parse(responseText);

        // Check for safety blocks
        if (data.candidates?.[0]?.finishReason === "SAFETY") {
          console.error("[CleanImage] Safety block:", data.candidates[0]);
          throw new Error(
            "Image blocked by safety filters. Try a different image."
          );
        }

        if (data.promptFeedback?.blockReason) {
          console.error("[CleanImage] Prompt blocked:", data.promptFeedback);
          throw new Error(
            `Request blocked: ${data.promptFeedback.blockReason}`
          );
        }

        if (!data.candidates?.[0]?.content?.parts) {
          console.error(
            "[CleanImage] No content:",
            JSON.stringify(data, null, 2)
          );
          throw new Error("No content in response.");
        }

        const parts = data.candidates[0].content.parts;
        console.log(
          "[CleanImage] Response parts:",
          parts.map((p) =>
            p.text
              ? `text(${p.text.length})`
              : p.inlineData
              ? `image(${p.inlineData.mimeType})`
              : "unknown"
          )
        );

        const imagePart = parts.find((p) =>
          p.inlineData?.mimeType?.startsWith("image/")
        );

        if (imagePart) {
          const base64Image = `data:${imagePart.inlineData.mimeType};base64,${imagePart.inlineData.data}`;
          console.log(
            "[CleanImage] Success! Clean image generated, size:",
            imagePart.inlineData.data.length
          );
          return base64Image;
        }

        // If no image, check text response
        const textPart = parts.find((p) => p.text);
        if (textPart) {
          console.log("[CleanImage] Model returned text:", textPart.text);
          throw new Error(
            `Model couldn't generate image: ${textPart.text.substring(0, 150)}`
          );
        }

        throw new Error("No image in response");
      }

      /**
       * Clean image for a specific scene - removes text/watermarks
       */
      window.cleanImage = async (sceneId) => {
        console.log("[CleanImage] Called for scene:", sceneId);

        const scene = state.scenes.find((s) => s.id === sceneId);
        if (!scene) {
          console.error("[CleanImage] Scene not found:", sceneId);
          alert("Error: Scene not found");
          return;
        }

        if (!scene.generatedImage) {
          console.error("[CleanImage] No image in scene:", sceneId);
          alert("Error: No image to clean. Generate or upload an image first.");
          return;
        }

        console.log(
          "[CleanImage] Image found, length:",
          scene.generatedImage.length
        );
        console.log(
          "[CleanImage] Image preview:",
          scene.generatedImage.substring(0, 100)
        );

        // Store original in case user wants to revert
        if (!scene.originalGeneratedImage) {
          scene.originalGeneratedImage = scene.generatedImage;
        }

        updateScene(sceneId, { status: "cleaning_image", error: null });

        try {
          console.log("[CleanImage] Calling cleanImageWithAI...");
          const cleanedImage = await cleanImageWithAI(scene.generatedImage);

          console.log(
            "[CleanImage] Success! Cleaned image length:",
            cleanedImage.length
          );

          updateScene(sceneId, {
            generatedImage: cleanedImage,
            status: "image_done",
            error: null,
            wasClean: true, // Flag that this image was cleaned
          });

          console.log(
            "[CleanImage] Scene",
            sceneId,
            "image cleaned successfully"
          );
        } catch (error) {
          console.error("[CleanImage] Error:", error);
          updateScene(sceneId, {
            status: "image_done", // Keep the original image
            error: `Clean failed: ${error.message}`,
          });
          alert(`Clean failed: ${error.message}`);
        }
      };

      /**
       * Revert to original uncleaned image
       */
      window.revertToOriginalImage = (sceneId) => {
        const scene = state.scenes.find((s) => s.id === sceneId);
        if (!scene || !scene.originalGeneratedImage) {
          console.log("No original image to revert to for scene:", sceneId);
          return;
        }

        updateScene(sceneId, {
          generatedImage: scene.originalGeneratedImage,
          wasClean: false,
          error: null,
        });

        console.log("Scene", sceneId, "reverted to original image");
      };

      // ============================================
      // MOTION PROMPT GENERATION
      // ============================================
      async function generateMotionPrompt(scene, baseImageDesc, tone) {
        const apiKey = state.apiKeys.gemini;
        if (!apiKey) throw new Error("Gemini API key not set");

        const dialogueIntent = scene.segmentationData.dialogueIntent || "";
        const textOnScreen = scene.segmentationData.textOnScreen || "";

        // Check if we have actual dialogue (not just "[no dialogue]")
        const hasDialogue =
          dialogueIntent &&
          !dialogueIntent.toLowerCase().includes("[no dialogue]") &&
          !dialogueIntent.toLowerCase().includes("no dialogue") &&
          dialogueIntent.length > 5;

        const prompt = `You are an expert cinematographer writing prompts for Google Veo 3.1 AI video generation.

Veo 3.1 BEST PRACTICES:
- Use specific cinematic camera language
- Camera movements: dolly shot, tracking shot, crane shot, slow pan, handheld, POV shot, push-in, pull-out
- Shot types: wide shot, medium shot, close-up, extreme close-up, two-shot, over-the-shoulder
- Lens/Focus: shallow depth of field, rack focus, soft focus, deep focus
- Lighting keywords: golden hour, soft diffused light, dramatic side lighting, chiaroscuro, rim light, backlit
- Style: cinematic, photorealistic, film grain, anamorphic, 35mm film look
- Motion: smooth, subtle, gentle, dynamic, energetic
- Audio: ambient sounds, SFX descriptions, (no subtitles)

Scene Context from Original Video:
- Visual: ${scene.segmentationData.visualSummary}
- Action: ${scene.segmentationData.actionSummary}
- Camera: ${scene.segmentationData.cameraSummary}
- Duration: ${scene.duration.toFixed(1)} seconds
${
  hasDialogue
    ? `- DETECTED DIALOGUE: "${dialogueIntent}"`
    : "- No dialogue detected"
}
${textOnScreen ? `- TEXT ON SCREEN (ignore in prompt): "${textOnScreen}"` : ""}

Generate a DETAILED Veo 3.1 motion prompt following this structure:
[CAMERA MOVEMENT] + [SHOT TYPE/COMPOSITION] + [SUBJECT ACTION] + [LIGHTING/ATMOSPHERE] + [AUDIO NOTES]

MOTION PROMPT should include:
1. Specific camera movement (e.g., "Slow dolly-in" not just "zoom")
2. Shot composition (e.g., "medium close-up, shallow depth of field")
3. Subject animation details (gestures, expressions, subtle movements)
4. Atmospheric elements (lighting changes, particle effects, environment)
5. Pacing descriptor (smooth, dynamic, subtle)
6. Audio direction if no dialogue: (ambient sounds, SFX: description)
7. Add "(no subtitles)" at the end

SPEECH: ${
          hasDialogue
            ? `Use the detected dialogue verbatim: "${dialogueIntent}"
     Format exactly as: Character says: "dialogue here" (no subtitles)`
            : `Write [none] - describe ambient audio in the motion prompt instead`
        }

Return ONLY in this exact format:
MOTION: [Complete cinematic motion description, 2-4 sentences, ending with (no subtitles)]
SPEECH: [exact dialogue OR [none]]`;

        const response = await fetch(
          `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`,
          {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
              contents: [{ parts: [{ text: prompt }] }],
              generationConfig: { temperature: 0.6, maxOutputTokens: 800 },
            }),
          }
        );

        if (!response.ok) throw new Error("Failed to generate motion prompt");

        const data = await response.json();
        const text = data.candidates?.[0]?.content?.parts?.[0]?.text || "";

        console.log("Motion prompt response:", text);

        const motionMatch = text.match(/MOTION:\s*(.+?)(?=SPEECH:|$)/s);
        const speechMatch = text.match(/SPEECH:\s*(.+?)$/s);

        let motion = motionMatch?.[1]?.trim() || "";
        // Ensure motion prompt ends with (no subtitles) if not already there
        if (motion && !motion.toLowerCase().includes("no subtitles")) {
          motion += " (no subtitles)";
        }
        if (!motion) {
          motion =
            "Smooth dolly-in shot, medium close-up with shallow depth of field, subject in gentle motion with natural breathing, soft ambient lighting with subtle highlights, cinematic 9:16 vertical frame (no subtitles)";
        }

        let speech = speechMatch?.[1]?.trim() || "[none]";
        // If we had detected dialogue, make sure we use it even if the model didn't
        if (hasDialogue && (speech === "[none]" || speech.length < 5)) {
          speech = `Character says: "${dialogueIntent}" (no subtitles)`;
        }

        return {
          motion: motion,
          speech: speech,
        };
      }

      // ============================================
      // FILE UPLOAD using imgbb.com (free image hosting)
      // ============================================
      async function uploadImageToHost(imageSource) {
        console.log("Uploading image to host...");
        console.log(
          "Image source type:",
          imageSource.startsWith("data:") ? "base64" : "url"
        );

        // If it's already a URL, return it directly
        if (!imageSource.startsWith("data:")) {
          console.log("Image is already a URL, using directly:", imageSource);
          return imageSource;
        }

        // Extract just the base64 data (remove data:image/xxx;base64, prefix)
        const base64Data = imageSource.split(",")[1];

        // Use imgbb.com API (free, no auth required for basic use)
        const formData = new FormData();
        formData.append("image", base64Data);

        try {
          const response = await fetch(
            "https://api.imgbb.com/1/upload?key=d36eb6591370ae7f9089d85875e56b22",
            {
              method: "POST",
              body: formData,
            }
          );

          const data = await response.json();
          console.log("imgbb response:", data);

          if (data.success && data.data?.url) {
            console.log("Image uploaded to imgbb:", data.data.url);
            return data.data.url;
          }
          throw new Error("imgbb upload failed");
        } catch (error) {
          console.error("Image upload failed:", error);
          throw new Error(`Failed to upload image: ${error.message}`);
        }
      }

      // ============================================
      // VIDEO GENERATION using Google Veo 3.1 (Gemini API)
      // Image-to-Video: Sends image inline as base64
      // ============================================

      async function generateVideo(scene) {
        const apiKey = state.apiKeys.gemini;
        if (!apiKey) throw new Error("Gemini API key not set");

        if (!scene.generatedImage) {
          throw new Error("Generate image first before creating video");
        }

        setState({ processingStatus: "Preparing image for Veo 3.1..." });

        // Extract base64 image data
        let imageBase64 = scene.generatedImage;
        let imageMimeType = "image/png";

        if (imageBase64.startsWith("data:")) {
          const parts = imageBase64.split(",");
          const mimeMatch = parts[0].match(/data:([^;]+)/);
          if (mimeMatch) imageMimeType = mimeMatch[1];
          imageBase64 = parts[1];
        }

        console.log(
          "Image prepared for Veo 3.1, mime type:",
          imageMimeType,
          "base64 length:",
          imageBase64.length
        );

        // Build the video generation prompt
        setState({
          processingStatus: "Starting Veo 3.1 image-to-video generation...",
        });

        const motionPrompt =
          scene.motionPrompt?.motion ||
          "Subtle movement, camera slowly zooms in";
        const speechPrompt = scene.motionPrompt?.speech || "";

        // Build full prompt with motion and speech
        let fullPrompt = motionPrompt;

        // Add speech/dialogue if present
        if (
          speechPrompt &&
          speechPrompt !== "[none]" &&
          !speechPrompt.toLowerCase().includes("[none]")
        ) {
          let dialogue = speechPrompt;
          if (!dialogue.toLowerCase().includes("says:")) {
            dialogue = `Person says: "${dialogue}"`;
          }
          fullPrompt += ` ${dialogue}`;
        }

        // Ensure no subtitles
        if (!fullPrompt.toLowerCase().includes("no subtitles")) {
          fullPrompt += " (no subtitles)";
        }

        console.log("Veo 3.1 full prompt:", fullPrompt);

        // Try multiple payload formats for image-to-video
        const payloadFormats = [
          // Format 1: bytesBase64Encoded (REST API standard)
          {
            instances: [
              {
                prompt: fullPrompt,
                image: {
                  bytesBase64Encoded: imageBase64,
                  mimeType: imageMimeType,
                },
              },
            ],
            parameters: {
              aspectRatio: "9:16",
              durationSeconds: 8,
            },
          },
          // Format 2: imageBytes (SDK-style)
          {
            instances: [
              {
                prompt: fullPrompt,
                image: {
                  imageBytes: imageBase64,
                  mimeType: imageMimeType,
                },
              },
            ],
            parameters: {
              aspectRatio: "9:16",
              durationSeconds: 8,
            },
          },
          // Format 3: rawBytes
          {
            instances: [
              {
                prompt: fullPrompt,
                image: {
                  rawBytes: imageBase64,
                  mimeType: imageMimeType,
                },
              },
            ],
            parameters: {
              aspectRatio: "9:16",
              durationSeconds: 8,
            },
          },
        ];

        let response;
        let responseText;
        let successFormat = null;

        for (let i = 0; i < payloadFormats.length; i++) {
          const payload = payloadFormats[i];
          console.log(`Trying Veo 3.1 image-to-video format ${i + 1}...`);

          response = await fetch(
            `https://generativelanguage.googleapis.com/v1beta/models/veo-3.1-generate-preview:predictLongRunning?key=${apiKey}`,
            {
              method: "POST",
              headers: { "Content-Type": "application/json" },
              body: JSON.stringify(payload),
            }
          );

          responseText = await response.text();
          console.log(
            `Veo 3.1 format ${i + 1} response:`,
            responseText.substring(0, 500)
          );

          if (response.ok) {
            successFormat = i + 1;
            console.log(`SUCCESS with format ${successFormat}`);
            break;
          }
        }

        // If all image formats fail, fall back to text-to-video with detailed prompt
        if (!response.ok) {
          console.log(
            "All image formats failed, falling back to text-to-video..."
          );

          // Build a more detailed prompt that describes the scene from the image
          const sceneDescription =
            scene.segmentationData?.visualSummary || "A professional scene";
          const detailedPrompt = `${sceneDescription}. ${fullPrompt}`;

          const textOnlyPayload = {
            instances: [
              {
                prompt: detailedPrompt,
              },
            ],
            parameters: {
              aspectRatio: "9:16",
              durationSeconds: 8,
            },
          };

          console.log(
            "Veo 3.1 text-to-video fallback:",
            JSON.stringify(textOnlyPayload, null, 2)
          );

          response = await fetch(
            `https://generativelanguage.googleapis.com/v1beta/models/veo-3.1-generate-preview:predictLongRunning?key=${apiKey}`,
            {
              method: "POST",
              headers: { "Content-Type": "application/json" },
              body: JSON.stringify(textOnlyPayload),
            }
          );
          responseText = await response.text();
          console.log("Veo 3.1 text-to-video response:", responseText);

          if (response.ok) {
            successFormat = "text-to-video";
          }
        }

        // If everything fails, throw error
        if (!response.ok) {
          let errorMsg = `All Veo 3.1 formats failed. Status: ${response.status}`;
          try {
            const errorData = JSON.parse(responseText);
            errorMsg = errorData.error?.message || errorMsg;
          } catch (e) {}

          console.error("Veo 3.1 failed:", errorMsg);
          throw new Error(`Veo 3.1 video generation failed: ${errorMsg}`);
        }

        const data = JSON.parse(responseText);
        const operationName = data.name;

        if (!operationName) {
          throw new Error("No operation name in response");
        }

        console.log(
          "Veo 3.1 operation started:",
          operationName,
          "(using format",
          successFormat,
          ")"
        );

        // Poll for completion
        setState({
          processingStatus:
            "Generating video with Veo 3.1 (this may take 1-3 minutes)...",
        });

        const videoUrl = await pollVeoOperation(operationName, apiKey);
        return videoUrl;
      }

      // Poll Veo 3.1 operation until complete
      async function pollVeoOperation(operationName, apiKey) {
        const maxAttempts = 120; // 20 minutes max (10 second intervals)
        let attempts = 0;

        while (attempts < maxAttempts) {
          await new Promise((r) => setTimeout(r, 10000)); // Wait 10 seconds
          attempts++;

          try {
            const response = await fetch(
              `https://generativelanguage.googleapis.com/v1beta/${operationName}?key=${apiKey}`,
              { method: "GET" }
            );

            const data = await response.json();
            console.log(
              `Veo poll attempt ${attempts}:`,
              data.done ? "DONE" : "processing...",
              data
            );

            if (data.done) {
              // Check for error
              if (data.error) {
                throw new Error(
                  data.error.message || "Video generation failed"
                );
              }

              // Get the video URI from response
              // Response path: response.generateVideoResponse.generatedSamples[0].video.uri
              const videoUri =
                data.response?.generateVideoResponse?.generatedSamples?.[0]
                  ?.video?.uri;

              if (!videoUri) {
                console.error("Full response:", JSON.stringify(data, null, 2));
                throw new Error("No video URI in response");
              }

              console.log("Veo 3.1 video URI:", videoUri);

              // Download the video - need to add API key
              try {
                const videoResponse = await fetch(videoUri, {
                  headers: { "x-goog-api-key": apiKey },
                });
                if (videoResponse.ok) {
                  const videoBlob = await videoResponse.blob();
                  const blobUrl = URL.createObjectURL(videoBlob);
                  console.log("Video downloaded, blob URL:", blobUrl);
                  return blobUrl;
                }
              } catch (downloadError) {
                console.log("Direct download failed:", downloadError);
              }

              // Return URI with key as fallback
              return `${videoUri}${
                videoUri.includes("?") ? "&" : "?"
              }key=${apiKey}`;
            }

            // Update status
            setState({
              processingStatus: `Generating video... (${attempts * 10}s)`,
            });
          } catch (pollError) {
            console.error("Poll error:", pollError);
            if (attempts >= maxAttempts - 1) throw pollError;
          }
        }

        throw new Error("Video generation timed out after 20 minutes");
      }

      // ============================================
      // KIE.AI VIDEO GENERATION (Kling 2.6 & Veo 3.1)
      // ============================================

      // Upload image to Kie.ai to get a public URL
      async function getPublicImageUrl(imageBase64, mimeType) {
        const apiKey = state.apiKeys.kieai;

        // Prepare base64 data with data URI prefix
        let base64Data = imageBase64;
        if (!base64Data.startsWith("data:")) {
          base64Data = `data:${mimeType};base64,${imageBase64}`;
        }

        const uploadUrl = "https://kieai.redpandaai.co/api/file-base64-upload";
        const requestBody = {
          base64Data: base64Data,
          uploadPath: "sceneforge",
          fileName: `image_${Date.now()}.png`,
        };

        console.log("Attempting file upload to:", uploadUrl);

        try {
          // Try direct request first
          const response = await fetch(uploadUrl, {
            method: "POST",
            headers: {
              Authorization: `Bearer ${apiKey}`,
              "Content-Type": "application/json",
            },
            body: JSON.stringify(requestBody),
          });

          if (!response.ok) {
            const errorText = await response.text();
            console.error(
              "File upload error response:",
              response.status,
              errorText
            );
            throw new Error(
              `File upload failed: ${response.status} - ${errorText}`
            );
          }

          const data = await response.json();
          console.log("Kie.ai file upload response:", data);

          // Return the download URL
          if (data.success && data.data?.downloadUrl) {
            return data.data.downloadUrl;
          }

          throw new Error(data.msg || "File upload failed - no download URL");
        } catch (error) {
          console.error("Kie.ai file upload error:", error);

          // Check if it's a CORS error
          if (
            error.message.includes("Failed to fetch") ||
            error.name === "TypeError"
          ) {
            console.log("Possible CORS error, trying with CORS proxy...");

            // Try with CORS proxy
            try {
              const proxyUrl =
                "https://corsproxy.io/?" + encodeURIComponent(uploadUrl);
              const proxyResponse = await fetch(proxyUrl, {
                method: "POST",
                headers: {
                  Authorization: `Bearer ${apiKey}`,
                  "Content-Type": "application/json",
                },
                body: JSON.stringify(requestBody),
              });

              if (!proxyResponse.ok) {
                throw new Error(`Proxy upload failed: ${proxyResponse.status}`);
              }

              const proxyData = await proxyResponse.json();
              console.log("CORS proxy upload response:", proxyData);

              if (proxyData.success && proxyData.data?.downloadUrl) {
                return proxyData.data.downloadUrl;
              }

              throw new Error(proxyData.msg || "Proxy upload failed");
            } catch (proxyError) {
              console.error("CORS proxy also failed:", proxyError);
              throw new Error(
                "File upload blocked by CORS. The Kie.ai API may not support browser requests. Try using Veo 3.1 (Google) instead."
              );
            }
          }

          throw error;
        }
      }

      // Generate video using Kling 2.6 via Kie.ai
      async function generateVideoKling(scene) {
        const apiKey = state.apiKeys.kieai;
        if (!apiKey) throw new Error("Kie.ai API key not set");

        if (!scene.generatedImage) {
          throw new Error("Generate image first before creating video");
        }

        setState({ processingStatus: "Preparing image for Kling 2.6..." });

        // Extract base64 image data
        let imageBase64 = scene.generatedImage;
        let imageMimeType = "image/png";

        if (imageBase64.startsWith("data:")) {
          const parts = imageBase64.split(",");
          const mimeMatch = parts[0].match(/data:([^;]+)/);
          if (mimeMatch) imageMimeType = mimeMatch[1];
          imageBase64 = parts[1];
        }

        // Upload image to get public URL
        setState({ processingStatus: "Uploading image to Kie.ai..." });
        let imageUrl;
        try {
          imageUrl = await getPublicImageUrl(imageBase64, imageMimeType);
          console.log("Image uploaded to Kie.ai:", imageUrl);
        } catch (uploadError) {
          console.error("Image upload failed:", uploadError);
          throw new Error(`Image upload failed: ${uploadError.message}`);
        }

        // Build the prompt
        const motionPrompt =
          scene.motionPrompt?.motion ||
          "Subtle movement, camera slowly zooms in";
        const speechPrompt = scene.motionPrompt?.speech || "";

        let fullPrompt = motionPrompt;
        if (
          speechPrompt &&
          speechPrompt !== "[none]" &&
          !speechPrompt.toLowerCase().includes("[none]")
        ) {
          let dialogue = speechPrompt;
          if (!dialogue.toLowerCase().includes("says:")) {
            dialogue = `Person says: "${dialogue}"`;
          }
          fullPrompt += ` ${dialogue}`;
        }

        console.log("Kling 2.6 prompt:", fullPrompt);

        // Create task
        setState({
          processingStatus: "Starting Kling 2.6 video generation...",
        });

        const payload = {
          model: "kling-2.6/image-to-video",
          input: {
            prompt: fullPrompt,
            image_urls: [imageUrl],
            sound: true,
            duration: "5",
          },
        };

        console.log("Kling 2.6 request:", JSON.stringify(payload, null, 2));

        try {
          const response = await fetch(
            "https://api.kie.ai/api/v1/jobs/createTask",
            {
              method: "POST",
              headers: {
                Authorization: `Bearer ${apiKey}`,
                "Content-Type": "application/json",
              },
              body: JSON.stringify(payload),
            }
          );

          const responseText = await response.text();
          console.log("Kling 2.6 raw response:", responseText);

          let data;
          try {
            data = JSON.parse(responseText);
          } catch (e) {
            throw new Error(
              `Invalid JSON response: ${responseText.substring(0, 200)}`
            );
          }

          console.log("Kling 2.6 createTask response:", data);

          if (data.code !== 200 || !data.data?.taskId) {
            throw new Error(
              data.msg || `Failed to create Kling 2.6 task (code: ${data.code})`
            );
          }

          const taskId = data.data.taskId;
          console.log("Kling 2.6 task ID:", taskId);

          // Poll for completion
          setState({
            processingStatus:
              "Generating video with Kling 2.6 (this may take 1-3 minutes)...",
          });
          return await pollKieaiTask(taskId, apiKey);
        } catch (fetchError) {
          console.error("Kling API fetch error:", fetchError);
          if (fetchError.message.includes("Failed to fetch")) {
            throw new Error(
              "Cannot connect to Kie.ai API. This may be a CORS issue - the API may not support browser requests."
            );
          }
          throw fetchError;
        }
      }

      // Generate video using Veo 3.1 via Kie.ai
      async function generateVideoVeoKieai(scene, model = "veo3") {
        const apiKey = state.apiKeys.kieai;
        if (!apiKey) throw new Error("Kie.ai API key not set");

        if (!scene.generatedImage) {
          throw new Error("Generate image first before creating video");
        }

        const modelName =
          model === "veo3_fast" ? "Veo 3.1 Fast" : "Veo 3.1 Quality";
        setState({ processingStatus: `Preparing image for ${modelName}...` });

        // Extract base64 image data
        let imageBase64 = scene.generatedImage;
        let imageMimeType = "image/png";

        if (imageBase64.startsWith("data:")) {
          const parts = imageBase64.split(",");
          const mimeMatch = parts[0].match(/data:([^;]+)/);
          if (mimeMatch) imageMimeType = mimeMatch[1];
          imageBase64 = parts[1];
        }

        // Upload image to get public URL
        setState({ processingStatus: "Uploading image to Kie.ai..." });
        let imageUrl;
        try {
          imageUrl = await getPublicImageUrl(imageBase64, imageMimeType);
          console.log("Image uploaded to Kie.ai:", imageUrl);
        } catch (uploadError) {
          console.error("Image upload failed:", uploadError);
          throw new Error(`Image upload failed: ${uploadError.message}`);
        }

        // Build the prompt
        const motionPrompt =
          scene.motionPrompt?.motion ||
          "Subtle movement, camera slowly zooms in";
        const speechPrompt = scene.motionPrompt?.speech || "";

        let fullPrompt = motionPrompt;
        if (
          speechPrompt &&
          speechPrompt !== "[none]" &&
          !speechPrompt.toLowerCase().includes("[none]")
        ) {
          let dialogue = speechPrompt;
          if (!dialogue.toLowerCase().includes("says:")) {
            dialogue = `Person says: "${dialogue}"`;
          }
          fullPrompt += ` ${dialogue}`;
        }

        console.log(`${modelName} prompt:`, fullPrompt);

        // Create task
        setState({
          processingStatus: `Starting ${modelName} video generation...`,
        });

        const payload = {
          prompt: fullPrompt,
          imageUrls: [imageUrl],
          model: model, // 'veo3' or 'veo3_fast'
          aspectRatio: "9:16",
          generationType: "FIRST_AND_LAST_FRAMES_2_VIDEO",
          enableTranslation: true,
        };

        console.log(`${modelName} request:`, JSON.stringify(payload, null, 2));

        try {
          const response = await fetch(
            "https://api.kie.ai/api/v1/veo/generate",
            {
              method: "POST",
              headers: {
                Authorization: `Bearer ${apiKey}`,
                "Content-Type": "application/json",
              },
              body: JSON.stringify(payload),
            }
          );

          const responseText = await response.text();
          console.log(`${modelName} raw response:`, responseText);

          let data;
          try {
            data = JSON.parse(responseText);
          } catch (e) {
            throw new Error(
              `Invalid JSON response: ${responseText.substring(0, 200)}`
            );
          }

          console.log(`${modelName} generate response:`, data);

          if (data.code !== 200 || !data.data?.taskId) {
            throw new Error(
              data.msg ||
                `Failed to create ${modelName} task (code: ${data.code})`
            );
          }

          const taskId = data.data.taskId;
          console.log(`${modelName} task ID:`, taskId);

          // Poll for completion
          setState({
            processingStatus: `Generating video with ${modelName} (this may take 1-3 minutes)...`,
          });
          return await pollKieaiVeoTask(taskId, apiKey, modelName);
        } catch (fetchError) {
          console.error("Veo API fetch error:", fetchError);
          if (fetchError.message.includes("Failed to fetch")) {
            throw new Error(
              "Cannot connect to Kie.ai API. This may be a CORS issue - the API may not support browser requests."
            );
          }
          throw fetchError;
        }
      }

      // Poll Kie.ai task (for Kling) until complete
      async function pollKieaiTask(taskId, apiKey) {
        const maxAttempts = 120; // 20 minutes max
        let attempts = 0;

        while (attempts < maxAttempts) {
          await new Promise((r) => setTimeout(r, 10000)); // Wait 10 seconds
          attempts++;

          try {
            // Correct endpoint is /api/v1/jobs/recordInfo
            const response = await fetch(
              `https://api.kie.ai/api/v1/jobs/recordInfo?taskId=${taskId}`,
              {
                method: "GET",
                headers: {
                  Authorization: `Bearer ${apiKey}`,
                },
              }
            );

            const responseText = await response.text();
            let data;
            try {
              data = JSON.parse(responseText);
            } catch (e) {
              console.error(
                "Invalid JSON from poll:",
                responseText.substring(0, 200)
              );
              continue;
            }

            console.log(`Kie.ai poll attempt ${attempts}:`, data);

            if (data.code === 200 && data.data) {
              const taskData = data.data;
              const state = taskData.state;

              // Check for success
              if (state === "success" || state === "completed") {
                // Parse resultJson to get video URL
                let resultUrls = [];
                if (taskData.resultJson) {
                  try {
                    const resultData =
                      typeof taskData.resultJson === "string"
                        ? JSON.parse(taskData.resultJson)
                        : taskData.resultJson;
                    resultUrls =
                      resultData.resultUrls || resultData.result_urls || [];
                  } catch (e) {
                    console.error(
                      "Failed to parse resultJson:",
                      taskData.resultJson
                    );
                  }
                }

                // Get video URL from various possible locations
                const videoUrl =
                  resultUrls[0] ||
                  taskData.output?.video_url ||
                  taskData.result?.video_url ||
                  taskData.video_url ||
                  taskData.videoUrl;

                if (!videoUrl) {
                  console.error(
                    "Full response:",
                    JSON.stringify(data, null, 2)
                  );
                  throw new Error("No video URL in response");
                }

                console.log("Kling 2.6 video URL:", videoUrl);
                return videoUrl;
              }

              // Check for failure
              if (state === "failed" || state === "fail" || state === "error") {
                const errorMsg =
                  taskData.failMsg ||
                  taskData.error ||
                  taskData.message ||
                  "Video generation failed";
                console.error(
                  "Kie.ai task failed. Full response:",
                  JSON.stringify(data, null, 2)
                );
                throw new Error(
                  `Kie.ai error: ${errorMsg}. This is a server-side error - please try again.`
                );
              }

              // Still processing
              console.log(`Task state: "${state}", continuing to poll...`);
              setState({
                processingStatus: `Generating video (state: ${state})... (${
                  attempts * 10
                }s)`,
              });
            }
          } catch (pollError) {
            console.error("Poll error:", pollError);
            if (attempts >= maxAttempts - 1) throw pollError;
          }
        }

        throw new Error("Video generation timed out after 20 minutes");
      }

      // Poll Kie.ai Veo task until complete
      async function pollKieaiVeoTask(taskId, apiKey, modelName) {
        const maxAttempts = 120; // 20 minutes max
        let attempts = 0;

        while (attempts < maxAttempts) {
          await new Promise((r) => setTimeout(r, 10000)); // Wait 10 seconds
          attempts++;

          try {
            // Correct endpoint is /api/v1/veo/record-info
            const response = await fetch(
              `https://api.kie.ai/api/v1/veo/record-info?taskId=${taskId}`,
              {
                method: "GET",
                headers: {
                  Authorization: `Bearer ${apiKey}`,
                },
              }
            );

            const responseText = await response.text();
            let data;
            try {
              data = JSON.parse(responseText);
            } catch (e) {
              console.error(
                "Invalid JSON from poll:",
                responseText.substring(0, 200)
              );
              continue;
            }

            console.log(`Kie.ai Veo poll attempt ${attempts}:`, data);

            if (data.code === 200 && data.data) {
              const taskData = data.data;
              const successFlag = taskData.successFlag;

              // successFlag: 0 = processing, 1 = success, 2/3 = failed
              if (successFlag === 1) {
                // Parse resultUrls to get video URL
                let videoUrls = [];
                if (taskData.resultUrls) {
                  try {
                    videoUrls =
                      typeof taskData.resultUrls === "string"
                        ? JSON.parse(taskData.resultUrls)
                        : taskData.resultUrls;
                  } catch (e) {
                    console.error(
                      "Failed to parse resultUrls:",
                      taskData.resultUrls
                    );
                  }
                }

                const videoUrl =
                  videoUrls[0] || taskData.videoUrl || taskData.video_url;

                if (!videoUrl) {
                  console.error(
                    "Full response:",
                    JSON.stringify(data, null, 2)
                  );
                  throw new Error("No video URL in response");
                }

                console.log(`${modelName} video URL:`, videoUrl);
                return videoUrl;
              }

              // Check for failure
              if (successFlag === 2 || successFlag === 3) {
                throw new Error(
                  taskData.errorMessage || data.msg || "Video generation failed"
                );
              }

              // Still processing (successFlag === 0)
              console.log(
                `Task successFlag: ${successFlag}, continuing to poll...`
              );
            }

            // Update status
            setState({
              processingStatus: `Generating video with ${modelName}... (${
                attempts * 10
              }s)`,
            });
          } catch (pollError) {
            console.error("Poll error:", pollError);
            if (attempts >= maxAttempts - 1) throw pollError;
          }
        }

        throw new Error("Video generation timed out after 20 minutes");
      }

      // ============================================
      // MAIN PROCESSING PIPELINE
      // ============================================
      async function processVideo(file) {
        console.log(
          "processVideo called with:",
          file.name,
          file.size,
          file.type
        );

        if (!state.apiKeys.gemini) {
          alert(
            "Gemini API key is required for video analysis.\n\nPlease add it in API Settings."
          );
          return;
        }

        // Store the video file for later use
        setState({
          videoFile: file,
          videoUrl: URL.createObjectURL(file),
        });

        try {
          // Step 1: Extract keyframes for visual reference
          setState({
            processingStatus: "Extracting keyframes...",
            isGenerating: true,
          });
          console.log("Extracting keyframes...");

          const { frames, duration } = await extractKeyframes(file, 8); // More frames for better coverage
          console.log(
            "Extracted",
            frames.length,
            "frames from",
            duration,
            "second video"
          );
          setState({ videoDuration: duration });

          // Step 2: Upload video to Gemini for audio analysis
          setState({
            processingStatus: "Uploading video for audio analysis...",
          });
          console.log("Uploading video to Gemini...");

          let videoFileInfo;
          try {
            videoFileInfo = await uploadVideoToGemini(file);
          } catch (uploadError) {
            console.warn(
              "Video upload failed, falling back to frame-only analysis:",
              uploadError
            );
            // Fallback: analyze without audio
            videoFileInfo = null;
          }

          // Step 3: Analyze video with audio
          setState({
            processingStatus:
              "Analyzing video with AI (extracting dialogue)...",
          });
          console.log("Analyzing video with Gemini...");

          let analysis;
          if (videoFileInfo) {
            analysis = await analyzeVideoWithGemini(
              videoFileInfo,
              frames,
              duration
            );
          } else {
            // Fallback to frame-only analysis
            analysis = await analyzeVideoFramesOnly(frames, duration);
          }
          console.log("Gemini returned", analysis.scenes?.length, "scenes");

          // Step 4: Extract first frame for each scene
          setState({ processingStatus: "Extracting scene frames..." });
          const sceneFrames = await extractSceneFirstFrames(
            file,
            analysis.scenes
          );

          // Step 5: Build scenes with proper data
          const scenes = await Promise.all(
            analysis.scenes.map(async (s, i) => {
              setState({
                processingStatus: `Generating prompts for scene ${i + 1}/${
                  analysis.scenes.length
                }...`,
              });
              console.log(`Processing scene ${i + 1}: ${s.start}s - ${s.end}s`);
              console.log(
                `  Dialogue: ${
                  s.dialogue_transcript || s.dialogue_intent || "[none]"
                }`
              );

              // Use the first frame of this specific scene
              const sceneFirstFrame = sceneFrames[s.scene_id];

              // Build scene data with audio transcription
              const sceneData = {
                segmentationData: {
                  visualSummary: s.visual_summary,
                  actionSummary: s.action_summary,
                  cameraSummary: s.camera_summary,
                  styleTags: s.style_tags || [],
                  dialogueTranscript:
                    s.dialogue_transcript ||
                    s.dialogue_intent ||
                    "[no dialogue]",
                  textOnScreen: s.text_on_screen || "",
                  audioDescription: s.audio_description || "",
                },
                duration: s.end - s.start,
                originalKeyframe: sceneFirstFrame,
              };

              // Generate image prompt using the actual scene frame
              const imagePromptData = await generateImagePrompt(
                sceneData,
                state.creativeDirection,
                state.repurposeLevel
              );

              // Generate motion prompt with actual dialogue
              const motionPromptData = await generateMotionPromptWithDialogue(
                sceneData
              );

              return {
                id: `s${i + 1}`,
                order: i,
                startTime: s.start,
                endTime: s.end,
                duration: s.end - s.start,
                originalKeyframe: sceneFirstFrame,
                segmentationData: sceneData.segmentationData,
                imagePrompt: imagePromptData,
                motionPrompt: motionPromptData,
                generatedImage: null,
                generatedVideo: null,
                status: "pending",
                trim: { in: 0, out: 1 },
              };
            })
          );

          console.log("All scenes processed, moving to pipeline view");
          setState({
            scenes,
            step: 1,
            processingStatus: "",
            isGenerating: false,
          });
        } catch (error) {
          console.error("Processing error:", error);
          alert("Error processing video: " + error.message);
          setState({
            processingStatus: `Error: ${error.message}`,
            isGenerating: false,
          });
        }
      }

      // Fallback analysis without audio (frame-only)
      async function analyzeVideoFramesOnly(frames, duration) {
        const apiKey = state.apiKeys.gemini;
        const targetSceneDuration = 7;
        const estimatedScenes = Math.max(
          1,
          Math.ceil(duration / targetSceneDuration)
        );

        const prompt = `Analyze these video frames. Duration: ${duration.toFixed(
          1
        )}s. Create ${estimatedScenes} scenes.

For each scene: scene_id, start, end, visual_summary, action_summary, camera_summary, dialogue_transcript (or [no dialogue]), text_on_screen, audio_elements.

CRITICAL: Return ONLY valid JSON. Use single quotes inside string values, NO double quotes inside values.

Example: {"scenes":[{"scene_id":"S1","start":0,"end":7,"visual_summary":"description here","action_summary":"movements","camera_summary":"medium shot","dialogue_transcript":"[no dialogue]","text_on_screen":"","audio_elements":"background music"}]}`;

        const parts = [{ text: prompt }];
        frames.forEach((frame) => {
          if (frame.dataUrl?.includes(",")) {
            parts.push({
              inlineData: {
                mimeType: "image/jpeg",
                data: frame.dataUrl.split(",")[1],
              },
            });
          }
        });

        const response = await fetch(
          `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`,
          {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
              contents: [{ parts }],
              generationConfig: { temperature: 0.3, maxOutputTokens: 3000 },
            }),
          }
        );

        const data = await response.json();
        const text = data.candidates?.[0]?.content?.parts?.[0]?.text || "";
        return safeParseJSON(text, "frame analysis");
      }

      // Generate motion prompt with actual transcribed dialogue
      async function generateMotionPromptWithDialogue(scene) {
        const apiKey = state.apiKeys.gemini;
        if (!apiKey) throw new Error("Gemini API key not set");

        const dialogue = scene.segmentationData.dialogueTranscript || "";
        const hasDialogue =
          dialogue &&
          !dialogue.toLowerCase().includes("[no dialogue]") &&
          dialogue.length > 3;

        const prompt = `Create a VEO 3.1 video generation prompt to EXACTLY recreate this scene.

=== SCENE ANALYSIS ===
VISUAL: ${scene.segmentationData.visualSummary}
ACTION: ${scene.segmentationData.actionSummary}  
CAMERA: ${scene.segmentationData.cameraSummary}
DURATION: ${scene.duration?.toFixed(1) || "7"} seconds
${
  hasDialogue
    ? `DIALOGUE (VERBATIM): "${dialogue}"`
    : "AUDIO: No speech - ambient/music only"
}
${
  scene.segmentationData.audioElements
    ? `AUDIO ELEMENTS: ${scene.segmentationData.audioElements}`
    : ""
}

=== VEO 3.1 PROMPT REQUIREMENTS ===

MOTION PROMPT must include ALL of these in order:
1. CAMERA: Specific movement (static, slow dolly in, tracking left, crane up, handheld, push-in)
2. SHOT: Type and framing (extreme close-up on face, medium shot waist-up, wide establishing)
3. SUBJECT: Exact actions with timing (raises hand to face, turns head left, smiles gradually)
4. LIGHTING: Quality and direction (soft key light from left, warm golden hour, dramatic side light)
5. ATMOSPHERE: Mood elements (intimate, energetic, peaceful, cinematic depth of field)
6. STYLE: Visual treatment (photorealistic, shallow DOF, slight film grain, 9:16 vertical)

${
  hasDialogue
    ? `
SPEECH PROMPT must be the EXACT dialogue for Veo 3.1's native audio:
- Use the VERBATIM transcription: "${dialogue}"
- Format: Person says: "[exact words]"
- Include any tone indicators in parentheses
`
    : `
SPEECH: [none]
- Describe ambient audio in the motion prompt instead
- Include: (ambient room tone) or (subtle background music) or specific sounds
`
}

CRITICAL: End motion prompt with (no subtitles) to prevent text overlays.

=== OUTPUT FORMAT ===
Return ONLY these two lines:
MOTION: [2-4 detailed sentences covering all 6 elements above, ending with (no subtitles)]
SPEECH: [Exact verbatim dialogue as "Person says: ..." OR [none]]`;

        const response = await fetch(
          `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`,
          {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
              contents: [{ parts: [{ text: prompt }] }],
              generationConfig: { temperature: 0.3, maxOutputTokens: 800 },
            }),
          }
        );

        const data = await response.json();
        const text = data.candidates?.[0]?.content?.parts?.[0]?.text || "";

        console.log("Motion prompt generation result:", text);

        const motionMatch = text.match(/MOTION:\s*(.+?)(?=SPEECH:|$)/s);
        const speechMatch = text.match(/SPEECH:\s*(.+?)$/s);

        let motion = motionMatch?.[1]?.trim() || "";
        if (motion && !motion.toLowerCase().includes("no subtitles")) {
          motion += " (no subtitles)";
        }
        if (!motion) {
          // Generate a default based on the scene data
          motion = `${
            scene.segmentationData.cameraSummary || "Static medium shot"
          }, ${
            scene.segmentationData.actionSummary?.substring(0, 100) ||
            "subject in gentle motion"
          }, soft cinematic lighting, photorealistic 9:16 vertical frame (no subtitles)`;
        }

        let speech = speechMatch?.[1]?.trim() || "[none]";
        // Ensure we use the actual transcribed dialogue
        if (hasDialogue && (speech === "[none]" || speech.length < 5)) {
          speech = `Person says: "${dialogue}"`;
        }

        return { motion, speech };
      }

      async function generateSceneImage(sceneId) {
        const scene = state.scenes.find((s) => s.id === sceneId);
        if (!scene) return;

        try {
          updateScene(sceneId, { status: "generating_image", error: null });
          console.log("Generating image for scene:", sceneId);
          console.log("Image prompt:", scene.imagePrompt);

          // Determine which reference image to use
          let referenceImage = null;

          if (state.consistentCharacter && state.referenceSceneId) {
            // Use the selected reference scene's GENERATED image for character consistency
            const refScene = state.scenes.find(
              (s) => s.id === state.referenceSceneId
            );
            if (refScene?.generatedImage) {
              referenceImage = refScene.generatedImage;
              console.log(
                "Using CONSISTENT CHARACTER reference from scene:",
                state.referenceSceneId
              );
            } else {
              console.log(
                "Reference scene has no generated image, falling back to own keyframe"
              );
              referenceImage = state.useReferenceImage
                ? scene.originalKeyframe
                : null;
            }
          } else if (state.useReferenceImage) {
            // Use this scene's own original keyframe
            referenceImage = scene.originalKeyframe;
            console.log("Using CHARACTER LOCK (own keyframe as reference)");
          }

          const imageUrl = await generateImage(
            scene.imagePrompt.image_prompt ||
              scene.imagePrompt.positive ||
              JSON.stringify(scene.imagePrompt),
            scene.imagePrompt.negative_prompt ||
              scene.imagePrompt.negative ||
              "",
            referenceImage
          );

          console.log("Image generated successfully");
          updateScene(sceneId, {
            generatedImage: imageUrl,
            status: "image_done",
            error: null,
          });
        } catch (error) {
          console.error("Image generation error for", sceneId, ":", error);
          const errorMsg = error.message || "Unknown error";
          updateScene(sceneId, {
            status: "error",
            error: errorMsg,
          });
          // Show alert with error
          alert(`Image generation failed for ${sceneId}:\n\n${errorMsg}`);
        }
      }

      async function generateSceneVideo(sceneId) {
        const scene = state.scenes.find((s) => s.id === sceneId);
        if (!scene || !scene.generatedImage) return;

        try {
          updateScene(sceneId, { status: "generating_video", error: null });

          let videoUrl;

          // Route to appropriate video generation API based on selected model
          switch (state.videoModel) {
            case "kling-kieai":
              if (!state.apiKeys.kieai) {
                throw new Error(
                  "Kie.ai API key not set. Please add it in API Settings."
                );
              }
              videoUrl = await generateVideoKling(scene);
              break;

            case "veo-kieai":
              if (!state.apiKeys.kieai) {
                throw new Error(
                  "Kie.ai API key not set. Please add it in API Settings."
                );
              }
              videoUrl = await generateVideoVeoKieai(scene, "veo3");
              break;

            case "veo-fast-kieai":
              if (!state.apiKeys.kieai) {
                throw new Error(
                  "Kie.ai API key not set. Please add it in API Settings."
                );
              }
              videoUrl = await generateVideoVeoKieai(scene, "veo3_fast");
              break;

            case "veo-gemini":
            default:
              if (!state.apiKeys.gemini) {
                throw new Error(
                  "Gemini API key not set. Please add it in API Settings."
                );
              }
              videoUrl = await generateVideo(scene);
              break;
          }

          updateScene(sceneId, {
            generatedVideo: videoUrl,
            status: "done",
          });

          setState({ processingStatus: "" });
        } catch (error) {
          console.error("Video generation error:", error);
          updateScene(sceneId, {
            status: "error",
            error: error.message,
          });
          setState({ processingStatus: "" });
        }
      }

      function updateScene(sceneId, updates) {
        setState({
          scenes: state.scenes.map((s) =>
            s.id === sceneId ? { ...s, ...updates } : s
          ),
        });
      }

      // ============================================
      // UI COMPONENTS
      // ============================================
      function render() {
        const app = document.getElementById("app");

        // Check authentication first
        if (!state.isAuthenticated) {
          app.innerHTML = renderLoginScreen();
          return;
        }

        if (state.showSettings) {
          app.innerHTML = renderSettings();
        } else if (state.step === 0) {
          app.innerHTML = renderUploadScreen();
        } else if (state.step === 1) {
          app.innerHTML = renderPipelineScreen();
        } else {
          app.innerHTML = renderEditorScreen();
          // Initialize timeline scrubbing after editor renders
          setTimeout(() => initTimelineScrubbing(), 50);
        }

        // Add fullscreen image modal if needed
        if (state.fullscreenImage) {
          const modal = document.createElement("div");
          modal.id = "fullscreen-modal";
          modal.innerHTML = renderFullscreenModal();
          document.body.appendChild(modal);
        } else {
          const existingModal = document.getElementById("fullscreen-modal");
          if (existingModal) existingModal.remove();
        }

        attachEventListeners();
      }

      function renderUserMenu() {
        const user = state.user;
        if (!user) return "";

        return `
        <div class="user-menu">
          <img 
            id="user-avatar"
            class="user-avatar"
            src="${
              user.picture ||
              "https://ui-avatars.com/api/?name=" +
                encodeURIComponent(user.name) +
                "&background=8b5cf6&color=fff"
            }"
            alt="${user.name}"
            onclick="GoogleAuth.toggleDropdown()"
          />
          <div id="user-dropdown" class="user-dropdown">
            <div class="user-info">
              <div class="user-name">${user.name}</div>
              <div class="user-email">${user.email}</div>
            </div>
            <button class="logout-btn" onclick="GoogleAuth.signOut()">
              <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M17 16l4-4m0 0l-4-4m4 4H7m6 4v1a3 3 0 01-3 3H6a3 3 0 01-3-3V7a3 3 0 013-3h4a3 3 0 013 3v1"/></svg>
              Sair
            </button>
          </div>
        </div>
      `;
      }

      function renderLoginScreen() {
        return `
        <div class="login-screen">
          <div class="login-card">
            <!-- Logo -->
            <div class="inline-flex items-center justify-center gap-3 mb-6">
              <div class="w-16 h-16 rounded-2xl bg-gradient-to-br from-violet-600 to-fuchsia-600 flex items-center justify-center shadow-lg shadow-violet-500/30">
                <svg class="w-8 h-8 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10"/></svg>
              </div>
            </div>
            
            <h1 class="text-3xl font-bold text-white mb-2">SceneForge</h1>
            <p class="text-zinc-400 mb-8">Recreate any video with AI-generated scenes</p>
            
            <!-- Google Login Button -->
            <button onclick="GoogleAuth.signIn()" class="google-btn">
              <svg viewBox="0 0 24 24">
                <path fill="#4285F4" d="M22.56 12.25c0-.78-.07-1.53-.2-2.25H12v4.26h5.92c-.26 1.37-1.04 2.53-2.21 3.31v2.77h3.57c2.08-1.92 3.28-4.74 3.28-8.09z"/>
                <path fill="#34A853" d="M12 23c2.97 0 5.46-.98 7.28-2.66l-3.57-2.77c-.98.66-2.23 1.06-3.71 1.06-2.86 0-5.29-1.93-6.16-4.53H2.18v2.84C3.99 20.53 7.7 23 12 23z"/>
                <path fill="#FBBC05" d="M5.84 14.09c-.22-.66-.35-1.36-.35-2.09s.13-1.43.35-2.09V7.07H2.18C1.43 8.55 1 10.22 1 12s.43 3.45 1.18 4.93l2.85-2.22.81-.62z"/>
                <path fill="#EA4335" d="M12 5.38c1.62 0 3.06.56 4.21 1.64l3.15-3.15C17.45 2.09 14.97 1 12 1 7.7 1 3.99 3.47 2.18 7.07l3.66 2.84c.87-2.6 3.3-4.53 6.16-4.53z"/>
              </svg>
              Continue with Google
            </button>
            
            <p class="text-zinc-500 text-sm mt-6">
              By signing in, you agree to our Terms of Service
            </p>
            
            <!-- Features list -->
            <div class="mt-8 pt-6 border-t border-zinc-800">
              <div class="grid grid-cols-2 gap-4 text-left">
                <div class="flex items-start gap-2">
                  <svg class="w-5 h-5 text-violet-400 mt-0.5 flex-shrink-0" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"/></svg>
                  <span class="text-zinc-400 text-sm">AI Scene Analysis</span>
                </div>
                <div class="flex items-start gap-2">
                  <svg class="w-5 h-5 text-violet-400 mt-0.5 flex-shrink-0" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"/></svg>
                  <span class="text-zinc-400 text-sm">Image Generation</span>
                </div>
                <div class="flex items-start gap-2">
                  <svg class="w-5 h-5 text-fuchsia-400 mt-0.5 flex-shrink-0" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"/></svg>
                  <span class="text-zinc-400 text-sm">Video Recreation</span>
                </div>
                <div class="flex items-start gap-2">
                  <svg class="w-5 h-5 text-fuchsia-400 mt-0.5 flex-shrink-0" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"/></svg>
                  <span class="text-zinc-400 text-sm">NLE Editor</span>
                </div>
              </div>
            </div>
          </div>
        </div>
      `;
      }

      function renderFullscreenModal() {
        return `
        <div class="fixed inset-0 z-[100] flex items-center justify-center" onclick="closeFullscreenImage()">
          <!-- Backdrop with blur -->
          <div class="absolute inset-0 bg-black/90 backdrop-blur-md"></div>
          
          <!-- Close button -->
          <button onclick="closeFullscreenImage()" 
                  class="absolute top-4 right-4 z-10 p-3 bg-black/50 hover:bg-black/70 rounded-full transition-colors group"
                  title="Close (ESC)">
            <svg class="w-6 h-6 text-white group-hover:text-red-400 transition-colors" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"/>
            </svg>
          </button>
          
          <!-- Hint text -->
          <div class="absolute bottom-4 left-1/2 -translate-x-1/2 text-zinc-400 text-sm bg-black/50 px-4 py-2 rounded-full">
            Press <kbd class="px-2 py-0.5 bg-zinc-700 rounded text-white mx-1">ESC</kbd> or click anywhere to close
          </div>
          
          <!-- Image container -->
          <div class="relative z-10 max-w-[90vw] max-h-[90vh] p-4" onclick="event.stopPropagation()">
            <img src="${state.fullscreenImage}" 
                 alt="Fullscreen preview" 
                 class="max-w-full max-h-[85vh] object-contain rounded-lg shadow-2xl"
                 style="aspect-ratio: 9/16;">
          </div>
        </div>
      `;
      }

      function renderSettings() {
        return `
        <div class="fixed inset-0 bg-black/80 flex items-center justify-center p-4 z-50">
          <div class="bg-zinc-900 rounded-2xl p-6 max-w-lg w-full border border-zinc-800 max-h-[90vh] overflow-y-auto">
            <div class="flex items-center justify-between mb-6">
              <h2 class="text-xl font-bold">API Settings</h2>
              <button onclick="closeSettings()" class="p-2 hover:bg-zinc-800 rounded-lg">
                <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"/></svg>
              </button>
            </div>
            
            <div class="space-y-4">
              <div>
                <label class="block text-sm text-zinc-400 mb-2">Gemini API Key</label>
                <input type="password" id="gemini-key" value="${
                  state.apiKeys.gemini
                }" 
                  placeholder="AIza..." 
                  class="w-full bg-zinc-800 border border-zinc-700 rounded-lg px-4 py-3 text-white placeholder-zinc-500 focus:outline-none focus:border-violet-500">
                <p class="text-xs text-zinc-500 mt-1">Used for scene analysis and image generation. Get one at <a href="https://aistudio.google.com/apikey" target="_blank" class="text-violet-400 hover:underline">Google AI Studio</a></p>
              </div>
              
              <div class="border-t border-zinc-800 pt-4">
                <label class="block text-sm text-zinc-400 mb-2">Kie.ai API Key <span class="text-zinc-600">(optional)</span></label>
                <input type="password" id="kieai-key" value="${
                  state.apiKeys.kieai
                }" 
                  placeholder="kie_..." 
                  class="w-full bg-zinc-800 border border-zinc-700 rounded-lg px-4 py-3 text-white placeholder-zinc-500 focus:outline-none focus:border-violet-500">
                <p class="text-xs text-zinc-500 mt-1">For Kling 2.6 and Veo 3.1 video generation via Kie.ai. Get one at <a href="https://kie.ai/api-key" target="_blank" class="text-violet-400 hover:underline">Kie.ai</a></p>
              </div>
              
              <div class="border-t border-zinc-800 pt-4">
                <label class="block text-sm text-zinc-400 mb-2">Video Generation Model</label>
                <select id="video-model" class="w-full bg-zinc-800 border border-zinc-700 rounded-lg px-4 py-3 text-white focus:outline-none focus:border-violet-500">
                  <option value="veo-gemini" ${
                    state.videoModel === "veo-gemini" ? "selected" : ""
                  }>Veo 3.1 (Google Gemini API)</option>
                  <option value="kling-kieai" ${
                    state.videoModel === "kling-kieai" ? "selected" : ""
                  } ${
          !state.apiKeys.kieai ? "disabled" : ""
        }>Kling 2.6 (Kie.ai) ${
          !state.apiKeys.kieai ? "- Add API key" : ""
        }</option>
                  <option value="veo-kieai" ${
                    state.videoModel === "veo-kieai" ? "selected" : ""
                  } ${
          !state.apiKeys.kieai ? "disabled" : ""
        }>Veo 3.1 Quality (Kie.ai) ${
          !state.apiKeys.kieai ? "- Add API key" : ""
        }</option>
                  <option value="veo-fast-kieai" ${
                    state.videoModel === "veo-fast-kieai" ? "selected" : ""
                  } ${
          !state.apiKeys.kieai ? "disabled" : ""
        }>Veo 3.1 Fast (Kie.ai) ${
          !state.apiKeys.kieai ? "- Add API key" : ""
        }</option>
                </select>
                <p class="text-xs text-zinc-500 mt-1">Choose which AI model to use for video generation</p>
              </div>
              
              <div class="pt-4 flex gap-3">
                <button onclick="saveSettings()" class="flex-1 bg-gradient-to-r from-violet-600 to-fuchsia-600 text-white font-medium py-3 rounded-xl hover:from-violet-500 hover:to-fuchsia-500">
                  Save Settings
                </button>
              </div>
            </div>
          </div>
        </div>
      `;
      }

      function renderUploadScreen() {
        const hasGemini = state.apiKeys.gemini;
        const user = state.user;

        return `
        <div class="min-h-screen flex flex-col">
          <!-- User Menu Header -->
          <div class="absolute top-4 right-4 z-50">
            ${renderUserMenu()}
          </div>
          
          <div class="flex-1 flex items-center justify-center p-6">
          <div class="max-w-xl w-full">
            <div class="text-center mb-8">
              <div class="inline-flex items-center gap-3 mb-4">
                <div class="w-12 h-12 rounded-2xl bg-gradient-to-br from-violet-600 to-fuchsia-600 flex items-center justify-center">
                  <svg class="w-6 h-6 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10"/></svg>
                </div>
                <h1 class="text-3xl font-bold">SceneForge</h1>
              </div>
              <p class="text-zinc-400 text-lg">Recreate any short-form video with AI-generated scenes</p>
              <p class="text-zinc-500 text-sm mt-2">Powered by Gemini (Analysis + Nano Banana Pro Images) + ${
                state.videoModel === "kling-kieai"
                  ? "Kling 2.6 (Kie.ai)"
                  : state.videoModel === "veo-kieai"
                  ? "Veo 3.1 Quality (Kie.ai)"
                  : state.videoModel === "veo-fast-kieai"
                  ? "Veo 3.1 Fast (Kie.ai)"
                  : "Veo 3.1 (Google)"
              } Video</p>
            </div>

            ${
              !hasGemini
                ? `
              <div class="mb-4 p-4 bg-amber-500/10 border border-amber-500/30 rounded-xl">
                <div class="flex items-start gap-3">
                  <svg class="w-5 h-5 text-amber-400 mt-0.5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 16l4.586-4.586a2 2 0 012.828 0L16 16m-2-2l1.586-1.586a2 2 0 012.828 0L20 14m-6-6h.01M6 20h12a2 2 0 002-2V6a2 2 0 00-2-2H6a2 2 0 00-2 2v12a2 2 0 002 2z"/></svg>
                  <div>
                    <p class="text-amber-400 font-medium">Gemini API Key Required</p>
                    <p class="text-amber-400/70 text-sm mt-1">For scene analysis and image generation.</p>
                  </div>
                </div>
              </div>
            `
                : ""
            }

            ${
              !hasGemini
                ? `
              <div class="mb-6 text-center">
                <button onclick="openSettings()" class="text-sm text-violet-400 hover:text-violet-300 font-medium">
                  Configure Gemini API Key
                </button>
              </div>
            `
                : ""
            }

            <div id="dropzone" class="border-2 border-dashed border-zinc-700 hover:border-violet-500 rounded-3xl p-12 text-center cursor-pointer transition-all bg-zinc-900/50 hover:bg-violet-500/5">
              ${
                state.isGenerating
                  ? `
                <div class="space-y-4">
                  <div class="w-16 h-16 mx-auto rounded-2xl bg-violet-500/20 flex items-center justify-center">
                    <svg class="w-8 h-8 text-violet-400 animate-spin" fill="none" viewBox="0 0 24 24"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"/><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4z"/></svg>
                  </div>
                  <div>
                    <p class="text-white font-medium mb-2">${state.processingStatus}</p>
                    <div class="h-1.5 bg-zinc-800 rounded-full max-w-xs mx-auto overflow-hidden">
                      <div class="h-full bg-gradient-to-r from-violet-500 to-fuchsia-500 animate-pulse" style="width: 60%"></div>
                    </div>
                  </div>
                </div>
              `
                  : `
                <div class="w-20 h-20 mx-auto mb-6 rounded-3xl bg-gradient-to-br from-zinc-800 to-zinc-900 flex items-center justify-center border border-zinc-700">
                  <svg class="w-10 h-10 text-zinc-400" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 16a4 4 0 01-.88-7.903A5 5 0 1115.9 6L16 6a5 5 0 011 9.9M15 13l-3-3m0 0l-3 3m3-3v12"/></svg>
                </div>
                <p class="text-xl font-medium mb-2">Drop your video here</p>
                <p class="text-zinc-500 mb-6">or click to browse</p>
                <div class="flex items-center justify-center gap-6 text-sm text-zinc-600">
                  <span>MP4, MOV, WebM</span>
                  <span>Up to 60s</span>
                </div>
              `
              }
              <input type="file" id="video-input" accept="video/*" class="hidden">
            </div>

            <div class="mt-6 flex justify-center gap-4">
              <button onclick="openSettings()" class="text-sm text-zinc-500 hover:text-zinc-300 flex items-center gap-2">
                <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10.325 4.317c.426-1.756 2.924-1.756 3.35 0a1.724 1.724 0 002.573 1.066c1.543-.94 3.31.826 2.37 2.37a1.724 1.724 0 001.065 2.572c1.756.426 1.756 2.924 0 3.35a1.724 1.724 0 00-1.066 2.573c.94 1.543-.826 3.31-2.37 2.37a1.724 1.724 0 00-2.572 1.065c-.426 1.756-2.924 1.756-3.35 0a1.724 1.724 0 00-2.573-1.066c-1.543.94-3.31-.826-2.37-2.37a1.724 1.724 0 00-1.065-2.572c-1.756-.426-1.756-2.924 0-3.35a1.724 1.724 0 001.066-2.573c-.94-1.543.826-3.31 2.37-2.37.996.608 2.296.07 2.572-1.065z"/><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 12a3 3 0 11-6 0 3 3 0 016 0z"/></svg>
                API Settings
              </button>
              <span class="text-zinc-700">|</span>
              <button onclick="goToEditor()" class="text-sm text-violet-400 hover:text-violet-300 flex items-center gap-2">
                <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 10l4.553-2.276A1 1 0 0121 8.618v6.764a1 1 0 01-1.447.894L15 14M5 18h8a2 2 0 002-2V8a2 2 0 00-2-2H5a2 2 0 00-2 2v8a2 2 0 002 2z"/></svg>
                Go to Editor
              </button>
            </div>
          </div>
          </div>
        </div>
      `;
      }

      function renderPipelineScreen() {
        const imgsDone = state.scenes.filter((s) => s.generatedImage).length;
        const vidsDone = state.scenes.filter((s) => s.generatedVideo).length;
        const totalDuration = state.scenes.reduce((a, s) => a + s.duration, 0);

        return `
        <div class="min-h-screen flex flex-col">
          <!-- Header -->
          <div class="border-b border-zinc-800 bg-zinc-900/80 backdrop-blur sticky top-0 z-10">
            <div class="px-6 py-4 flex items-center justify-between flex-wrap gap-4">
              <div class="flex items-center gap-4 flex-wrap">
                <div class="flex items-center gap-2">
                  <div class="w-8 h-8 rounded-xl bg-gradient-to-br from-violet-600 to-fuchsia-600 flex items-center justify-center">
                    <svg class="w-4 h-4 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10"/></svg>
                  </div>
                  <span class="font-semibold">SceneForge Pipeline</span>
                </div>
                <div class="flex items-center gap-2 pl-2 border-l border-zinc-700">
                  <label class="flex items-center gap-2 cursor-pointer" title="Use original scene frame as reference for character consistency">
                    <input type="checkbox" id="use-reference-image" ${
                      state.useReferenceImage ? "checked" : ""
                    } class="w-4 h-4 accent-violet-500 rounded">
                    <span class="text-xs text-zinc-400">🎭 Character Lock</span>
                  </label>
                </div>
                <div class="flex items-center gap-2 pl-2 border-l border-zinc-700">
                  <label class="flex items-center gap-2 cursor-pointer" title="Use a generated image as reference for all scenes - click 👤 on a scene to set reference">
                    <input type="checkbox" id="consistent-character" ${
                      state.consistentCharacter ? "checked" : ""
                    } class="w-4 h-4 accent-fuchsia-500 rounded">
                    <span class="text-xs text-zinc-400">👤 Consistent Character</span>
                  </label>
                  ${
                    state.consistentCharacter && state.referenceSceneId
                      ? `
                    <span class="text-xs text-fuchsia-400">(Ref: ${state.referenceSceneId})</span>
                    <button onclick="syncPromptsToReference()" class="px-2 py-1 bg-fuchsia-600 hover:bg-fuchsia-500 rounded text-xs font-medium" title="Sync all scene prompts to match reference environment, lighting, clothing">
                      🔄 Sync All
                    </button>
                  `
                      : ""
                  }
                </div>
              </div>
              <div class="flex items-center gap-3">
                <button onclick="openSettings()" class="p-2 hover:bg-zinc-800 rounded-lg text-zinc-400">
                  <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10.325 4.317c.426-1.756 2.924-1.756 3.35 0a1.724 1.724 0 002.573 1.066c1.543-.94 3.31.826 2.37 2.37a1.724 1.724 0 001.065 2.572c1.756.426 1.756 2.924 0 3.35a1.724 1.724 0 00-1.066 2.573c.94 1.543-.826 3.31-2.37 2.37a1.724 1.724 0 00-2.572 1.065c-.426 1.756-2.924 1.756-3.35 0a1.724 1.724 0 00-2.573-1.066c-1.543.94-3.31-.826-2.37-2.37a1.724 1.724 0 00-1.065-2.572c-1.756-.426-1.756-2.924 0-3.35a1.724 1.724 0 001.066-2.573c-.94-1.543.826-3.31 2.37-2.37.996.608 2.296.07 2.572-1.065z"/><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 12a3 3 0 11-6 0 3 3 0 016 0z"/></svg>
                </button>
                <button onclick="downloadAllAssets()" class="px-4 py-2 bg-zinc-800 hover:bg-zinc-700 rounded-xl text-sm font-medium flex items-center gap-2 border border-emerald-600/50 text-emerald-400 hover:text-emerald-300" ${
                  state.scenes.length === 0 ? "disabled opacity-50" : ""
                } title="Download all images, prompts, and videos as ZIP">
                  <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 16v1a3 3 0 003 3h10a3 3 0 003-3v-1m-4-4l-4 4m0 0l-4-4m4 4V4"/></svg>
                  Download All
                </button>
                <button onclick="generateAllImages()" class="px-4 py-2 bg-zinc-800 hover:bg-zinc-700 rounded-xl text-sm font-medium flex items-center gap-2 border border-zinc-700" ${
                  state.isGenerating ? "disabled" : ""
                }>
                  <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 16l4.586-4.586a2 2 0 012.828 0L16 16m-2-2l1.586-1.586a2 2 0 012.828 0L20 14m-6-6h.01M6 20h12a2 2 0 002-2V6a2 2 0 00-2-2H6a2 2 0 00-2 2v12a2 2 0 002 2z"/></svg>
                  Generate All Images
                </button>
                <button onclick="generateAllVideos()" class="px-4 py-2 bg-zinc-800 hover:bg-zinc-700 rounded-xl text-sm font-medium flex items-center gap-2 border border-zinc-700" ${
                  state.isGenerating || imgsDone === 0 ? "disabled" : ""
                } title="Using: ${
          state.videoModel === "kling-kieai"
            ? "Kling 2.6 (Kie.ai)"
            : state.videoModel === "veo-kieai"
            ? "Veo 3.1 Quality (Kie.ai)"
            : state.videoModel === "veo-fast-kieai"
            ? "Veo 3.1 Fast (Kie.ai)"
            : "Veo 3.1 (Google)"
        }">
                  <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 10l4.553-2.276A1 1 0 0121 8.618v6.764a1 1 0 01-1.447.894L15 14M5 18h8a2 2 0 002-2V8a2 2 0 00-2-2H5a2 2 0 00-2 2v8a2 2 0 002 2z"/></svg>
                  Generate Videos (${
                    state.videoModel === "kling-kieai"
                      ? "Kling"
                      : state.videoModel === "veo-kieai"
                      ? "Veo Quality"
                      : state.videoModel === "veo-fast-kieai"
                      ? "Veo Fast"
                      : "Veo"
                  })
                </button>
                <button id="proceed-to-editor-btn" class="px-4 py-2 bg-gradient-to-r from-violet-600 to-fuchsia-600 hover:from-violet-500 hover:to-fuchsia-500 rounded-xl text-sm font-medium flex items-center gap-2" ${
                  state.scenes.length === 0 ? "disabled" : ""
                }>
                  Proceed to Editor ${
                    vidsDone > 0 ? `(${vidsDone} videos)` : ""
                  }
                  <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 7l5 5m0 0l-5 5m5-5H6"/></svg>
                </button>
                
                <!-- User Menu -->
                ${renderUserMenu()}
              </div>
            </div>
            <div class="px-6 pb-3 flex items-center gap-4 text-sm flex-wrap">
              <span class="text-zinc-400">${
                state.scenes.length
              } scenes · ${totalDuration.toFixed(1)}s total</span>
              <div class="flex-1"></div>
              <span class="text-zinc-500">Images: ${imgsDone}/${
          state.scenes.length
        }</span>
              <div class="w-24 h-1 bg-zinc-800 rounded-full overflow-hidden">
                <div class="h-full bg-gradient-to-r from-violet-500 to-fuchsia-500" style="width: ${
                  (imgsDone / state.scenes.length) * 100
                }%"></div>
              </div>
              <span class="text-zinc-500">Videos: ${vidsDone}/${
          state.scenes.length
        }</span>
              <div class="w-24 h-1 bg-zinc-800 rounded-full overflow-hidden">
                <div class="h-full bg-gradient-to-r from-violet-500 to-fuchsia-500" style="width: ${
                  (vidsDone / state.scenes.length) * 100
                }%"></div>
              </div>
            </div>
            
            <!-- Product Upload Section -->
            <div class="px-6 pb-4 border-t border-zinc-800 pt-3">
              <div class="flex items-center gap-4 flex-wrap">
                <div class="flex items-center gap-3">
                  <span class="text-sm text-zinc-400">🛍️ My Product:</span>
                  ${
                    state.productImage
                      ? `<div class="flex items-center gap-3">
                         <div class="w-12 h-12 rounded-lg overflow-hidden border border-orange-500/50 bg-zinc-800">
                           <img src="${state.productImage}" alt="My Product" class="w-full h-full object-cover">
                         </div>
                         <button onclick="removeProductImage()" class="text-xs text-red-400 hover:text-red-300">✕ Remove</button>
                       </div>`
                      : `<label class="px-3 py-1.5 bg-orange-600 hover:bg-orange-700 rounded-lg text-xs font-medium cursor-pointer flex items-center gap-1">
                         📤 Upload Product Image
                         <input type="file" accept="image/*" class="hidden" onchange="uploadProductImage(event)">
                       </label>`
                  }
                </div>
                ${
                  state.productImage && imgsDone > 0
                    ? `
                  <button onclick="swapAllProducts()" class="px-3 py-1.5 bg-gradient-to-r from-orange-500 to-pink-500 hover:from-orange-400 hover:to-pink-400 rounded-lg text-xs font-medium flex items-center gap-1" ${
                    state.isGenerating ? "disabled opacity-50" : ""
                  }>
                    🔄 Swap Product in All Scenes
                  </button>
                `
                    : ""
                }
                ${
                  state.productImage
                    ? `
                  <span class="text-xs text-zinc-500 italic">Click "🔄 Swap" on each scene to replace the product with yours</span>
                `
                    : `
                  <span class="text-xs text-zinc-500 italic">Upload your product to enable product placement swap</span>
                `
                }
              </div>
            </div>
          </div>

          <!-- Scene Cards with horizontal scrollbar -->
          <div class="flex-1 flex flex-col overflow-hidden">
            <!-- Scene count header -->
            <div class="px-6 py-2 flex items-center justify-between text-xs text-zinc-500">
              <span>🎬 Scene Timeline</span>
              <span>${state.scenes.length} scenes</span>
            </div>
            
            <!-- HORIZONTAL SCROLLBAR - Very prominent -->
            <div class="px-6 py-2">
              <div class="relative h-4 rounded-full cursor-pointer" id="scrollbar-track" style="background: #3f3f46; border: 1px solid #52525b;">
                <div class="absolute top-0.5 bottom-0.5 rounded-full cursor-grab active:cursor-grabbing" 
                     id="scrollbar-thumb" 
                     style="width: 30%; left: 0%; background: linear-gradient(90deg, #8b5cf6, #d946ef); min-width: 60px;"></div>
              </div>
            </div>
            
            <!-- Scrollable container (hidden native scrollbar) -->
            <div class="flex-1 overflow-y-auto">
              <div class="scene-scroll-hidden px-6 pb-6 overflow-x-auto" id="scene-container">
                <div class="flex gap-4" style="min-width: max-content">
                  ${state.scenes
                    .map((scene, i) => renderSceneCard(scene, i))
                    .join("")}
                </div>
              </div>
            </div>
          </div>
        </div>
      `;
      }

      function renderSceneCard(scene, index) {
        const statusBadge =
          {
            pending:
              '<span class="px-2 py-0.5 rounded-full text-xs bg-zinc-800 text-zinc-300">Ready</span>',
            generating_image:
              '<span class="px-2 py-0.5 rounded-full text-xs bg-violet-500/20 text-violet-400 border border-violet-500/30 flex items-center gap-1"><svg class="w-3 h-3 animate-spin" fill="none" viewBox="0 0 24 24"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"/><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4z"/></svg>Image...</span>',
            swapping_product:
              '<span class="px-2 py-0.5 rounded-full text-xs bg-orange-500/20 text-orange-400 border border-orange-500/30 flex items-center gap-1"><svg class="w-3 h-3 animate-spin" fill="none" viewBox="0 0 24 24"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"/><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4z"/></svg>Swapping...</span>',
            image_done:
              '<span class="px-2 py-0.5 rounded-full text-xs bg-amber-500/20 text-amber-400 border border-amber-500/30">Image ✓</span>',
            generating_video:
              '<span class="px-2 py-0.5 rounded-full text-xs bg-violet-500/20 text-violet-400 border border-violet-500/30 flex items-center gap-1"><svg class="w-3 h-3 animate-spin" fill="none" viewBox="0 0 24 24"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"/><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4z"/></svg>Video...</span>',
            done: '<span class="px-2 py-0.5 rounded-full text-xs bg-emerald-500/20 text-emerald-400 border border-emerald-500/30">✓ Done</span>',
            error:
              '<span class="px-2 py-0.5 rounded-full text-xs bg-red-500/20 text-red-400 border border-red-500/30">Error</span>',
          }[scene.status] ||
          '<span class="px-2 py-0.5 rounded-full text-xs bg-zinc-800 text-zinc-300">Ready</span>';

        const imagePromptText =
          scene.imagePrompt?.image_prompt ||
          scene.imagePrompt?.positive ||
          "No prompt generated";
        const negativePrompt =
          scene.imagePrompt?.negative_prompt ||
          scene.imagePrompt?.negative ||
          "";
        const motionText = scene.motionPrompt?.motion || "No motion prompt";
        const speechText = scene.motionPrompt?.speech || "[none]";
        const dialogueTranscript =
          scene.segmentationData?.dialogueTranscript ||
          scene.segmentationData?.dialogueIntent ||
          "[no dialogue]";
        const textOnScreen = scene.segmentationData?.textOnScreen || "";

        return `
        <div class="flex-shrink-0 w-80 bg-zinc-900 rounded-2xl border border-zinc-800 overflow-hidden">
          <!-- Header -->
          <div class="px-4 py-3 border-b border-zinc-800 flex items-center justify-between">
            <div class="flex items-center gap-2">
              <span class="font-medium">Scene ${index + 1}</span>
              <span class="text-zinc-500 text-sm">${scene.startTime.toFixed(
                1
              )}s - ${scene.endTime.toFixed(1)}s</span>
            </div>
            <div class="flex items-center gap-2">
              ${statusBadge}
              <button onclick="deleteScene('${
                scene.id
              }')" class="p-1 text-zinc-500 hover:text-red-400 hover:bg-red-500/10 rounded transition-colors" title="Delete scene">
                <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 7l-.867 12.142A2 2 0 0116.138 21H7.862a2 2 0 01-1.995-1.858L5 7m5 4v6m4-6v6m1-10V4a1 1 0 00-1-1h-4a1 1 0 00-1 1v3M4 7h16"/></svg>
              </button>
            </div>
          </div>

          <!-- Two Column Layout: Original | Generated -->
          <div class="grid grid-cols-2 gap-2 p-3 border-b border-zinc-800">
            <!-- Original Keyframe -->
            <div>
              <div class="text-xs text-zinc-500 mb-1">📹 Original</div>
              <div class="aspect-[9/16] rounded-lg overflow-hidden bg-zinc-800 relative group cursor-pointer" onclick="openFullscreenImage('${
                scene.originalKeyframe
              }')">
                <img src="${
                  scene.originalKeyframe
                }" alt="Original" class="w-full h-full object-cover">
                <!-- Expand icon on hover -->
                <div class="absolute inset-0 bg-black/0 group-hover:bg-black/40 transition-colors flex items-center justify-center">
                  <div class="opacity-0 group-hover:opacity-100 transition-opacity bg-black/60 p-2 rounded-full">
                    <svg class="w-5 h-5 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 8V4m0 0h4M4 4l5 5m11-1V4m0 0h-4m4 0l-5 5M4 16v4m0 0h4m-4 0l5-5m11 5l-5-5m5 5v-4m0 4h-4"/></svg>
                  </div>
                </div>
              </div>
            </div>
            
            <!-- Generated Image with Upload Overlay -->
            <div>
              <div class="flex items-center justify-between mb-1">
                <span class="text-xs text-zinc-500">🖼️ Generated</span>
                ${
                  state.consistentCharacter
                    ? `
                  <button onclick="setCharacterReference('${scene.id}')" 
                          class="p-1 rounded transition-all ${
                            state.referenceSceneId === scene.id
                              ? "bg-fuchsia-500/20 text-fuchsia-400 ring-1 ring-fuchsia-500/50"
                              : "text-zinc-500 hover:text-fuchsia-400 hover:bg-fuchsia-500/10"
                          }"
                          title="${
                            state.referenceSceneId === scene.id
                              ? "This is the character reference"
                              : "Set as character reference"
                          }">
                    <svg class="w-4 h-4" fill="${
                      state.referenceSceneId === scene.id
                        ? "currentColor"
                        : "none"
                    }" stroke="currentColor" viewBox="0 0 24 24">
                      <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z"/>
                    </svg>
                  </button>
                `
                    : ""
                }
              </div>
              <div class="aspect-[9/16] rounded-lg overflow-hidden bg-zinc-800 relative group ${
                state.referenceSceneId === scene.id && state.consistentCharacter
                  ? "ring-2 ring-fuchsia-500"
                  : ""
              }" id="image-container-${scene.id}">
                ${
                  scene.generatedImage
                    ? `<img src="${
                        scene.generatedImage
                      }" alt="Generated" class="w-full h-full object-cover">
                     ${
                       scene.wasClean
                         ? '<div class="absolute top-1 right-1 bg-amber-500 text-white text-[9px] px-1.5 py-0.5 rounded font-medium">NO CAPTION</div>'
                         : ""
                     }
                     <!-- Hover overlay with actions -->
                     <div class="absolute inset-0 bg-black/70 opacity-0 group-hover:opacity-100 transition-opacity flex flex-col items-center justify-center gap-2 p-2">
                       ${
                         scene.wasClean && scene.originalGeneratedImage
                           ? `<!-- Revert to original -->
                            <button onclick="event.stopPropagation(); revertToOriginalImage('${scene.id}')" 
                                    class="w-full px-3 py-2 bg-zinc-500 hover:bg-zinc-400 rounded-lg text-xs font-medium flex items-center justify-center gap-2 transition-colors"
                                    title="Restore the original image before cleaning">
                              <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 10h10a8 8 0 018 8v2M3 10l6 6m-6-6l6-6"/></svg>
                              Revert to Original
                            </button>`
                           : `<!-- Clean Caption button -->
                            <button onclick="event.stopPropagation(); cleanImage('${
                              scene.id
                            }')" 
                                    class="w-full px-3 py-2 bg-amber-500 hover:bg-amber-400 rounded-lg text-xs font-medium flex items-center justify-center gap-2 transition-colors disabled:opacity-50"
                                    ${
                                      scene.status === "cleaning_image"
                                        ? "disabled"
                                        : ""
                                    }
                                    title="Remove TikTok/Instagram caption overlays only (keeps product labels, signs, etc.)">
                              ${
                                scene.status === "cleaning_image"
                                  ? '<svg class="w-4 h-4 animate-spin" fill="none" viewBox="0 0 24 24"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"/><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4z"/></svg> Cleaning...'
                                  : '<svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"/></svg> Clean Caption'
                              }
                            </button>`
                       }
                       <!-- View Fullscreen button -->
                       <button onclick="event.stopPropagation(); openFullscreenImage('${
                         scene.generatedImage
                       }')" 
                               class="w-full px-3 py-2 bg-violet-600 hover:bg-violet-500 rounded-lg text-xs font-medium flex items-center justify-center gap-2 transition-colors"
                               title="View image fullscreen">
                         <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 8V4m0 0h4M4 4l5 5m11-1V4m0 0h-4m4 0l-5 5M4 16v4m0 0h4m-4 0l5-5m11 5l-5-5m5 5v-4m0 4h-4"/></svg>
                         View Fullscreen
                       </button>
                       <!-- Upload replacement -->
                       <label onclick="event.stopPropagation()" class="w-full px-3 py-2 bg-zinc-600 hover:bg-zinc-500 rounded-lg text-xs font-medium flex items-center justify-center gap-2 cursor-pointer transition-colors">
                         <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 16v1a3 3 0 003 3h10a3 3 0 003-3v-1m-4-8l-4-4m0 0L8 8m4-4v12"/></svg>
                         Upload Replacement
                         <input type="file" accept="image/*" class="hidden" onchange="uploadCustomImage('${
                           scene.id
                         }', event)">
                       </label>
                       <p class="text-[10px] text-zinc-400 text-center mt-1">${
                         scene.wasClean
                           ? "Caption overlays removed"
                           : "Removes TikTok/IG captions only"
                       }</p>
                     </div>`
                    : scene.status === "generating_image" ||
                      scene.status === "cleaning_image"
                    ? '<div class="w-full h-full flex items-center justify-center"><svg class="w-8 h-8 text-violet-400 animate-spin" fill="none" viewBox="0 0 24 24"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"/><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4z"/></svg></div>'
                    : scene.status === "error"
                    ? '<div class="w-full h-full flex items-center justify-center border border-red-500/50 rounded-lg"><svg class="w-8 h-8 text-red-400" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 9v2m0 4h.01m-6.938 4h13.856c1.54 0 2.502-1.667 1.732-3L13.732 4c-.77-1.333-2.694-1.333-3.464 0L3.34 16c-.77 1.333.192 3 1.732 3z"/></svg></div>'
                    : '<div class="w-full h-full flex items-center justify-center"><svg class="w-8 h-8 text-zinc-700" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 16l4.586-4.586a2 2 0 012.828 0L16 16m-2-2l1.586-1.586a2 2 0 012.828 0L20 14m-6-6h.01M6 20h12a2 2 0 002-2V6a2 2 0 00-2-2H6a2 2 0 00-2 2v12a2 2 0 002 2z"/></svg></div>'
                }
              </div>
            </div>
          </div>
          
          <!-- Dialogue Detection -->
          <div class="px-3 py-2 border-b border-zinc-800 bg-zinc-800/30">
            <p class="text-xs text-fuchsia-400">🗣️ ${dialogueTranscript}</p>
            ${
              textOnScreen
                ? `<p class="text-xs text-amber-400 mt-1">📝 "${textOnScreen}"</p>`
                : ""
            }
          </div>

          <!-- Image Prompt with Copy and Regenerate -->
          <div class="p-3 border-b border-zinc-800">
            <div class="flex items-center justify-between mb-2">
              <span class="text-xs text-zinc-500">Image Prompt</span>
              <div class="flex items-center gap-2">
                <button onclick="openEditPromptModal('${
                  scene.id
                }')" class="p-1 text-blue-400 hover:text-blue-300 hover:bg-blue-500/10 rounded transition-colors" title="Edit prompts">
                  <svg class="w-3.5 h-3.5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M11 5H6a2 2 0 00-2 2v11a2 2 0 002 2h11a2 2 0 002-2v-5m-1.414-9.414a2 2 0 112.828 2.828L11.828 15H9v-2.828l8.586-8.586z"/></svg>
                </button>
                <button onclick="regeneratePrompt('${
                  scene.id
                }')" class="p-1 text-amber-400 hover:text-amber-300 hover:bg-amber-500/10 rounded transition-colors" title="Regenerate prompt">
                  <svg class="w-3.5 h-3.5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 4v5h.582m15.356 2A8.001 8.001 0 004.582 9m0 0H9m11 11v-5h-.581m0 0a8.003 8.003 0 01-15.357-2m15.357 2H15"/></svg>
                </button>
                <button onclick="copyFullPrompt('${
                  scene.id
                }', 'image')" class="text-xs text-violet-400 hover:text-violet-300">📋 Copy</button>
              </div>
            </div>
            <div class="bg-zinc-800/50 rounded p-2 max-h-20 overflow-y-auto">
              <p class="text-xs text-zinc-300">${imagePromptText.substring(
                0,
                200
              )}${imagePromptText.length > 200 ? "..." : ""}</p>
            </div>
            ${
              scene.error && !scene.generatedImage
                ? `<p class="mt-2 text-xs text-red-400">${scene.error}</p>`
                : ""
            }
            
            <!-- Generate/Upload/Swap Buttons -->
            <div class="mt-2 flex gap-2">
              <button onclick="generateImage('${
                scene.id
              }')" class="flex-1 px-3 py-2 bg-violet-600 hover:bg-violet-700 rounded-lg text-xs font-medium flex items-center justify-center gap-1" ${
          state.isGenerating ? "disabled opacity-50" : ""
        }>
                ✨ ${scene.generatedImage ? "Regen" : "Generate"}
              </button>
              ${
                scene.generatedImage && state.productImage
                  ? `
                <button onclick="swapProduct('${
                  scene.id
                }')" class="px-3 py-2 bg-gradient-to-r from-orange-500 to-pink-500 hover:from-orange-400 hover:to-pink-400 rounded-lg text-xs font-medium flex items-center justify-center gap-1" ${
                      state.isGenerating ? "disabled opacity-50" : ""
                    }>
                  🔄 Swap
                </button>
              `
                  : ""
              }
              <label class="px-3 py-2 bg-zinc-700 hover:bg-zinc-600 rounded-lg text-xs font-medium flex items-center justify-center gap-1 cursor-pointer">
                📤
                <input type="file" accept="image/*" class="hidden" onchange="uploadCustomImage('${
                  scene.id
                }', event)">
              </label>
            </div>
          </div>

          <!-- Motion Prompt with Copy -->
          <div class="p-3 border-b border-zinc-800">
            <div class="flex items-center justify-between mb-2">
              <span class="text-xs text-zinc-500">Motion & Speech</span>
              <button onclick="copyFullPrompt('${
                scene.id
              }', 'motion')" class="text-xs text-fuchsia-400 hover:text-fuchsia-300">📋 Copy</button>
            </div>
            <div class="bg-zinc-800/50 rounded p-2 max-h-20 overflow-y-auto space-y-1">
              <p class="text-xs"><span class="text-violet-400">Motion:</span> <span class="text-zinc-300">${motionText.substring(
                0,
                100
              )}...</span></p>
              <p class="text-xs"><span class="text-fuchsia-400">Speech:</span> <span class="text-zinc-300">${speechText}</span></p>
            </div>
          </div>

          <!-- Video Section -->
          <div class="p-3">
            <div class="text-xs text-zinc-500 mb-2">🎬 Generated Video</div>
            <div class="aspect-[9/16] rounded-lg overflow-hidden bg-zinc-800 flex items-center justify-center">
              ${
                scene.generatedVideo
                  ? `<video src="${scene.generatedVideo}" class="w-full h-full object-contain bg-black" controls muted playsinline></video>`
                  : scene.status === "generating_video"
                  ? '<svg class="w-8 h-8 text-fuchsia-400 animate-spin" fill="none" viewBox="0 0 24 24"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"/><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4z"/></svg>'
                  : '<svg class="w-8 h-8 text-zinc-700" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M14.752 11.168l-3.197-2.132A1 1 0 0010 9.87v4.263a1 1 0 001.555.832l3.197-2.132a1 1 0 000-1.664z"/><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 12a9 9 0 11-18 0 9 9 0 0118 0z"/></svg>'
              }
            </div>
            <button onclick="generateVideo('${
              scene.id
            }')" class="w-full mt-2 px-3 py-2 bg-fuchsia-600 hover:bg-fuchsia-700 rounded-lg text-xs font-medium flex items-center justify-center gap-1 disabled:opacity-50" ${
          !scene.generatedImage || state.isGenerating ? "disabled" : ""
        } title="Using: ${
          state.videoModel === "kling-kieai"
            ? "Kling 2.6 (Kie.ai)"
            : state.videoModel === "veo-kieai"
            ? "Veo 3.1 Quality (Kie.ai)"
            : state.videoModel === "veo-fast-kieai"
            ? "Veo 3.1 Fast (Kie.ai)"
            : "Veo 3.1 (Google)"
        }">
              🎬 ${scene.generatedVideo ? "Regenerate" : "Generate"} Video
            </button>
            ${
              scene.error && scene.generatedImage
                ? `<p class="mt-2 text-xs text-red-400">${scene.error}</p>`
                : ""
            }
          </div>
        </div>
      `;
      }

      function renderEditorScreen() {
        const scenesWithVideo = state.scenes
          .filter((s) => s.generatedVideo)
          .sort((a, b) => a.order - b.order);
        const totalDuration = scenesWithVideo.reduce((sum, s) => {
          const trimDuration = s.duration * (s.trim.out - s.trim.in);
          return sum + trimDuration;
        }, 0);

        // Get current zoom level (pixels per second)
        const pxPerSecond =
          typeof timelineZoom !== "undefined" ? timelineZoom : 60;

        // Calculate cumulative start times for timeline
        let cumulativeTime = 0;
        const timelineData = scenesWithVideo.map((scene, i) => {
          const clipDuration =
            scene.duration * (scene.trim.out - scene.trim.in);
          const data = {
            ...scene,
            startTime: cumulativeTime,
            clipDuration,
            index: i,
          };
          cumulativeTime += clipDuration;
          return data;
        });

        const selectedClip = timelineData[state.selectedScene];

        if (scenesWithVideo.length === 0) {
          return `
          <div class="h-screen flex flex-col bg-zinc-950">
            <div class="border-b border-zinc-800 bg-zinc-900/80 p-4 flex items-center justify-between">
              <div class="flex items-center">
                <button onclick="backToPipeline()" class="p-2 hover:bg-zinc-800 rounded-lg">← Back</button>
                <span class="ml-4 text-lg font-bold">Video Editor</span>
              </div>
              <!-- User Menu -->
              ${renderUserMenu()}
            </div>
            <div class="flex-1 flex items-center justify-center">
              <div class="text-center">
                <svg class="w-16 h-16 mx-auto text-zinc-700 mb-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 10l4.553-2.276A1 1 0 0121 8.618v6.764a1 1 0 01-1.447.894L15 14M5 18h8a2 2 0 002-2V8a2 2 0 00-2-2H5a2 2 0 00-2 2v8a2 2 0 002 2z"/></svg>
                <h2 class="text-xl text-zinc-400 mb-2">No clips in editor</h2>
                <p class="text-zinc-500 mb-6">Upload videos or generate them from scenes</p>
                <div class="flex gap-3 justify-center">
                  <button onclick="document.getElementById('editor-video-upload').click()" class="px-4 py-2 bg-gradient-to-r from-violet-600 to-fuchsia-600 hover:from-violet-500 hover:to-fuchsia-500 rounded-xl flex items-center gap-2">
                    <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 16a4 4 0 01-.88-7.903A5 5 0 1115.9 6L16 6a5 5 0 011 9.9M15 13l-3-3m0 0l-3 3m3-3v12"/></svg>
                    Upload Videos
                  </button>
                  <button onclick="backToPipeline()" class="px-4 py-2 bg-zinc-800 hover:bg-zinc-700 rounded-xl">← Back to Pipeline</button>
                </div>
                <input type="file" id="editor-video-upload" accept="video/*" multiple class="hidden" onchange="handleEditorVideoUpload(event)">
              </div>
            </div>
          </div>
        `;
        }

        return `
        <div class="h-screen flex flex-col bg-zinc-950 overflow-hidden">
          <!-- Header -->
          <div class="border-b border-zinc-800 bg-zinc-900/80 backdrop-blur flex-shrink-0">
            <div class="px-4 py-2 flex items-center justify-between">
              <div class="flex items-center gap-3">
                <button onclick="state.step = 1; render();" class="px-3 py-1.5 bg-zinc-800 hover:bg-zinc-700 rounded-lg text-sm font-medium transition-colors">
                  ← Back to Pipeline
                </button>
                <h1 class="text-lg font-bold">Video Editor</h1>
                <span class="text-zinc-500 text-sm">${
                  scenesWithVideo.length
                } clips · ${totalDuration.toFixed(1)}s</span>
              </div>
              
              <!-- Center Toolbar -->
              <div class="flex items-center gap-2">
                <!-- Tool Selection -->
                <div class="flex items-center gap-1 bg-zinc-800 rounded-lg p-1">
                  <button onclick="setActiveTool('select')" 
                          class="p-2 rounded ${
                            state.activeTool === "blade"
                              ? "hover:bg-zinc-700 text-zinc-400"
                              : "bg-zinc-700 text-white"
                          }" 
                          title="Selection Tool (A)">
                    <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 15l-2 5L9 9l11 4-5 2zm0 0l5 5M7.188 2.239l.777 2.897M5.136 7.965l-2.898-.777M13.95 4.05l-2.122 2.122m-5.657 5.656l-2.12 2.122"/></svg>
                  </button>
                  <button onclick="setActiveTool('blade')" 
                          class="p-2 rounded ${
                            state.activeTool === "blade"
                              ? "bg-red-600 text-white"
                              : "hover:bg-zinc-700 text-zinc-400"
                          }" 
                          title="Blade/Cut Tool (B)">
                    <svg class="w-4 h-4" viewBox="0 0 24 24" fill="currentColor"><path d="M9.64 7.64c.23-.5.36-1.05.36-1.64 0-2.21-1.79-4-4-4S2 3.79 2 6s1.79 4 4 4c.59 0 1.14-.13 1.64-.36L10 12l-2.36 2.36C7.14 14.13 6.59 14 6 14c-2.21 0-4 1.79-4 4s1.79 4 4 4 4-1.79 4-4c0-.59-.13-1.14-.36-1.64L12 14l7 7h3v-1L9.64 7.64zM6 8c-1.1 0-2-.89-2-2s.9-2 2-2 2 .89 2 2-.9 2-2 2zm0 12c-1.1 0-2-.89-2-2s.9-2 2-2 2 .89 2 2-.9 2-2 2zm6-7.5c-.28 0-.5-.22-.5-.5s.22-.5.5-.5.5.22.5.5-.22.5-.5.5zM19 3l-6 6 2 2 7-7V3h-3z"/></svg>
                  </button>
                </div>
                
                <div class="w-px h-6 bg-zinc-700"></div>
                
                <!-- Edit Actions -->
                <div class="flex items-center gap-1">
                  <button onclick="undo()" class="p-2 hover:bg-zinc-800 rounded text-zinc-400 hover:text-white disabled:opacity-30 disabled:cursor-not-allowed" title="Undo (Ctrl+Z)" ${
                    !canUndo() ? "disabled" : ""
                  }>
                    <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 10h10a8 8 0 018 8v2M3 10l6 6m-6-6l6-6"/></svg>
                  </button>
                  <button onclick="redo()" class="p-2 hover:bg-zinc-800 rounded text-zinc-400 hover:text-white disabled:opacity-30 disabled:cursor-not-allowed" title="Redo (Ctrl+Shift+Z)" ${
                    !canRedo() ? "disabled" : ""
                  }>
                    <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 10h-10a8 8 0 00-8 8v2M21 10l-6 6m6-6l-6-6"/></svg>
                  </button>
                  <div class="w-px h-4 bg-zinc-700 mx-1"></div>
                  <button onclick="splitAtPlayhead()" class="p-2 hover:bg-zinc-800 rounded text-zinc-400 hover:text-white" title="Split at Playhead (Ctrl+B)">
                    <svg class="w-4 h-4" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M12 2v20M5 12h14"/></svg>
                  </button>
                  <button onclick="duplicateSelectedClip()" class="p-2 hover:bg-zinc-800 rounded text-zinc-400 hover:text-white" title="Duplicate Clip (Ctrl+D)">
                    <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z"/></svg>
                  </button>
                  <button onclick="deleteSelectedClip()" class="p-2 hover:bg-zinc-800 rounded text-zinc-400 hover:text-red-400" title="Delete Clip (Delete)">
                    <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 7l-.867 12.142A2 2 0 0116.138 21H7.862a2 2 0 01-1.995-1.858L5 7m5 4v6m4-6v6m1-10V4a1 1 0 00-1-1h-4a1 1 0 00-1 1v3M4 7h16"/></svg>
                  </button>
                </div>
                
                <div class="w-px h-6 bg-zinc-700"></div>
                
                <!-- Playback Controls -->
                <div class="flex items-center gap-1">
                  <button onclick="jumpToStart()" class="p-2 hover:bg-zinc-800 rounded text-zinc-400 hover:text-white" title="Go to Start (Home)">
                    <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24"><path d="M6 6h2v12H6zm3.5 6l8.5 6V6z"/></svg>
                  </button>
                  <button onclick="stepBackward()" class="p-2 hover:bg-zinc-800 rounded text-zinc-400 hover:text-white" title="Step Back (←)">
                    <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24"><path d="M6 6h2v12H6zm3.5 6l8.5 6V6z"/></svg>
                  </button>
                  <button onclick="editorPlayAll()" class="p-2 hover:bg-zinc-800 rounded text-zinc-400 hover:text-white" title="Play/Stop (Space)">
                    <svg class="w-5 h-5" fill="currentColor" viewBox="0 0 24 24"><path d="M8 5v14l11-7z"/></svg>
                  </button>
                  <button onclick="stepForward()" class="p-2 hover:bg-zinc-800 rounded text-zinc-400 hover:text-white" title="Step Forward (→)">
                    <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24"><path d="M16 18h2V6h-2zM6 18l8.5-6L6 6z"/></svg>
                  </button>
                  <button onclick="jumpToEnd()" class="p-2 hover:bg-zinc-800 rounded text-zinc-400 hover:text-white" title="Go to End (End)">
                    <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24"><path d="M16 18h2V6h-2zM6 18l8.5-6L6 6z"/></svg>
                  </button>
                </div>
                
                ${
                  state.activeTool === "blade"
                    ? '<span class="text-xs text-red-400 ml-2 bg-red-500/10 px-2 py-1 rounded">✂️ BLADE MODE</span>'
                    : ""
                }
              </div>
              
              <!-- Right Actions -->
              <div class="flex items-center gap-2">
                <button onclick="document.getElementById('editor-video-upload-main').click()" class="px-3 py-2 bg-zinc-800 hover:bg-zinc-700 rounded-lg text-sm font-medium flex items-center gap-2">
                  <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 4v16m8-8H4"/></svg>
                  Add Clips
                </button>
                <input type="file" id="editor-video-upload-main" accept="video/*" multiple class="hidden" onchange="handleEditorVideoUpload(event)">
                <button onclick="downloadAllAssets()" class="px-3 py-2 bg-zinc-800 hover:bg-zinc-700 rounded-lg text-sm font-medium flex items-center gap-2 border border-emerald-600/50 text-emerald-400" title="Download all assets as ZIP">
                  <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 16v1a3 3 0 003 3h10a3 3 0 003-3v-1m-4-4l-4 4m0 0l-4-4m4 4V4"/></svg>
                  ZIP
                </button>
                <button onclick="exportMergedVideo()" class="px-4 py-2 bg-gradient-to-r from-violet-600 to-fuchsia-600 hover:from-violet-500 hover:to-fuchsia-500 rounded-lg text-sm font-medium flex items-center gap-2">
                  <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 16v1a3 3 0 003 3h10a3 3 0 003-3v-1m-4-4l-4 4m0 0l-4-4m4 4V4"/></svg>
                  Export MP4
                </button>
                
                <!-- User Menu -->
                ${renderUserMenu()}
              </div>
            </div>
            
            <!-- Keyboard Shortcuts Bar -->
            <div class="px-4 py-1.5 bg-zinc-900/50 border-t border-zinc-800/50 text-xs text-zinc-500 flex items-center gap-4">
              <span><kbd class="px-1 py-0.5 bg-zinc-800 rounded text-zinc-400">Space</kbd> Play/Pause</span>
              <span><kbd class="px-1 py-0.5 bg-zinc-800 rounded text-zinc-400">B</kbd> Blade</span>
              <span><kbd class="px-1 py-0.5 bg-zinc-800 rounded text-zinc-400">A</kbd> Select</span>
              <span><kbd class="px-1 py-0.5 bg-zinc-800 rounded text-zinc-400">Ctrl+B</kbd> Split</span>
              <span><kbd class="px-1 py-0.5 bg-zinc-800 rounded text-zinc-400">Del</kbd> Delete</span>
              <span><kbd class="px-1 py-0.5 bg-zinc-800 rounded text-zinc-400">J/K/L</kbd> Shuttle</span>
              <span><kbd class="px-1 py-0.5 bg-zinc-800 rounded text-zinc-400">←/→</kbd> Frame Step</span>
            </div>
          </div>

          <!-- Main Editor Area -->
          <div class="flex-1 flex overflow-hidden">
            <!-- Left: Clip List -->
            <div class="w-56 border-r border-zinc-800 bg-zinc-900/50 flex flex-col overflow-hidden">
              <div class="p-3 border-b border-zinc-800 flex items-center justify-between">
                <div>
                  <h3 class="text-sm font-medium text-zinc-400">Clips</h3>
                  <p class="text-xs text-zinc-500">Drag to reorder</p>
                </div>
                <button onclick="fitTimelineToView()" class="p-1.5 hover:bg-zinc-700 rounded text-zinc-400 hover:text-white" title="Fit to View">
                  <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 8V4m0 0h4M4 4l5 5m11-1V4m0 0h-4m4 0l-5 5M4 16v4m0 0h4m-4 0l5-5m11 5l-5-5m5 5v-4m0 4h-4"/></svg>
                </button>
              </div>
              <div class="flex-1 overflow-y-auto p-2 space-y-1.5" id="clip-list">
                ${timelineData
                  .map(
                    (scene, i) => `
                  <div class="clip-item bg-zinc-800/50 rounded-lg border transition-all cursor-pointer ${
                    i === state.selectedScene
                      ? "border-violet-500 ring-1 ring-violet-500/30"
                      : "border-zinc-700 hover:border-zinc-600"
                  }"
                       onclick="editorSelectScene(${i})"
                       draggable="true"
                       ondragstart="handleClipDragStart(event, ${i})"
                       ondragover="handleClipDragOver(event, ${i})"
                       ondragend="handleClipDragEnd(event)"
                       data-clip-index="${i}">
                    <div class="flex gap-2 p-2">
                      <div class="w-10 h-14 rounded overflow-hidden bg-zinc-900 flex-shrink-0 relative">
                        <video src="${
                          scene.generatedVideo
                        }" class="w-full h-full object-cover pointer-events-none" muted></video>
                        <div class="absolute bottom-0 left-0 right-0 bg-black/60 text-[10px] text-center py-0.5">${scene.clipDuration.toFixed(
                          1
                        )}s</div>
                      </div>
                      <div class="flex-1 min-w-0 flex flex-col justify-between py-0.5">
                        <span class="text-xs font-medium truncate" title="${
                          scene.fileName || "Clip " + (i + 1)
                        }">${
                      scene.isUploaded
                        ? scene.fileName?.substring(0, 12) || "Upload"
                        : "Clip " + (i + 1)
                    }</span>
                        <div class="flex items-center gap-1">
                          <button onclick="event.stopPropagation(); duplicateClip('${
                            scene.id
                          }')" 
                                  class="p-1 hover:bg-zinc-600 rounded text-zinc-400 hover:text-white" title="Duplicate">
                            <svg class="w-3 h-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z"/></svg>
                          </button>
                          <button onclick="event.stopPropagation(); editorRemoveClip('${
                            scene.id
                          }')" 
                                  class="p-1 hover:bg-red-500/20 rounded text-zinc-400 hover:text-red-400" title="Delete">
                            <svg class="w-3 h-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 7l-.867 12.142A2 2 0 0116.138 21H7.862a2 2 0 01-1.995-1.858L5 7m5 4v6m4-6v6m1-10V4a1 1 0 00-1-1h-4a1 1 0 00-1 1v3M4 7h16"/></svg>
                          </button>
                        </div>
                      </div>
                    </div>
                  </div>
                `
                  )
                  .join("")}
              </div>
            </div>
            
            <!-- Center: Video Preview -->
            <div class="flex-1 flex flex-col overflow-hidden">
              <!-- Video Player Area -->
              <div class="flex-1 flex items-center justify-center bg-black p-4 relative min-h-0">
                <div class="h-full max-h-full flex items-center justify-center" style="max-width: 40vh;">
                  <video id="preview-player" class="max-h-full max-w-full object-contain rounded-lg" playsinline>
                    <source src="${
                      selectedClip?.generatedVideo || ""
                    }" type="video/mp4">
                  </video>
                </div>
                
                <!-- Timecode Display -->
                <div class="absolute top-4 left-4 bg-black/70 px-3 py-1.5 rounded-lg font-mono text-sm">
                  <span id="current-timecode" class="text-white">00:00:00:00</span>
                  <span class="text-zinc-500"> / </span>
                  <span id="total-timecode" class="text-zinc-400">${formatTimecode(
                    totalDuration
                  )}</span>
                </div>
                
                <!-- Clip Info Overlay -->
                <div class="absolute bottom-4 left-4 bg-black/70 px-3 py-1.5 rounded-lg text-sm">
                  <span class="text-zinc-400">Clip ${
                    state.selectedScene + 1
                  }:</span>
                  <span class="text-white ml-1">${
                    selectedClip?.isUploaded
                      ? selectedClip.fileName || "Upload"
                      : "Generated"
                  }</span>
                </div>
                
                <!-- Export Overlay -->
                <div id="export-overlay" class="absolute inset-0 bg-black/80 flex items-center justify-center rounded-lg hidden">
                  <div class="text-center">
                    <svg class="w-12 h-12 text-violet-400 animate-spin mx-auto mb-3" fill="none" viewBox="0 0 24 24"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"/><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4z"/></svg>
                    <p class="text-white font-medium" id="export-status">Preparing export...</p>
                    <p class="text-zinc-400 text-sm mt-1" id="export-progress">0%</p>
                  </div>
                </div>
              </div>
              
              <!-- Playback Controls -->
              <div class="bg-zinc-900 border-t border-zinc-800 p-3 flex-shrink-0">
                <div class="flex items-center justify-center gap-4 mb-2">
                  <button onclick="editorPrevScene()" class="p-2 hover:bg-zinc-800 rounded-lg transition-colors ${
                    state.selectedScene === 0 ? "opacity-30" : ""
                  }">
                    <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M11 19l-7-7 7-7m8 14l-7-7 7-7"/></svg>
                  </button>
                  <button onclick="editorPlayPause()" class="p-3 bg-violet-600 hover:bg-violet-500 rounded-full transition-colors" id="play-pause-btn">
                    <svg class="w-6 h-6" fill="currentColor" viewBox="0 0 24 24"><path d="M8 5v14l11-7z"/></svg>
                  </button>
                  <button onclick="editorPlayAll()" class="px-3 py-2 bg-zinc-800 hover:bg-zinc-700 rounded-lg text-sm font-medium flex items-center gap-2">
                    <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M14.752 11.168l-3.197-2.132A1 1 0 0010 9.87v4.263a1 1 0 001.555.832l3.197-2.132a1 1 0 000-1.664z"/></svg>
                    Play All
                  </button>
                  <button onclick="editorNextScene()" class="p-2 hover:bg-zinc-800 rounded-lg transition-colors ${
                    state.selectedScene >= scenesWithVideo.length - 1
                      ? "opacity-30"
                      : ""
                  }">
                    <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 5l7 7-7 7M5 5l7 7-7 7"/></svg>
                  </button>
                </div>
                
                <!-- Current clip time -->
                <div class="text-center text-xs text-zinc-400">
                  <span id="current-time">0:00</span> / <span id="total-time">${
                    selectedClip?.clipDuration?.toFixed(1) || "0"
                  }s</span>
                  <span class="text-zinc-600 mx-2">|</span>
                  Clip ${state.selectedScene + 1} of ${scenesWithVideo.length}
                </div>
              </div>
            </div>
            
            <!-- Right: Clip Inspector -->
            <div class="w-72 border-l border-zinc-800 bg-zinc-900/50 flex flex-col overflow-hidden">
              <div class="p-3 border-b border-zinc-800">
                <h3 class="text-sm font-medium text-zinc-400">Clip Inspector</h3>
              </div>
              <div class="flex-1 overflow-y-auto p-4">
                ${
                  selectedClip
                    ? `
                  <div class="space-y-5">
                    <!-- Clip Info -->
                    <div class="bg-zinc-800/50 rounded-lg p-3">
                      <div class="text-xs text-zinc-500 mb-1">Clip Name</div>
                      <div class="text-sm text-white truncate">${
                        selectedClip.isUploaded
                          ? selectedClip.fileName || "Uploaded Clip"
                          : "Generated Clip " + (state.selectedScene + 1)
                      }</div>
                    </div>
                    
                    <!-- Trim Controls -->
                    <div class="space-y-4">
                      <div class="text-xs font-medium text-zinc-400 uppercase tracking-wider">Trim</div>
                      
                      <!-- Mark In/Out Buttons -->
                      <div class="flex gap-2">
                        <button onclick="markInPoint()" class="flex-1 px-3 py-2 bg-violet-600/20 hover:bg-violet-600/30 border border-violet-500/30 rounded-lg text-sm text-violet-400 flex items-center justify-center gap-1" title="Mark In (I)">
                          <span class="font-mono">[</span> Mark In
                        </button>
                        <button onclick="markOutPoint()" class="flex-1 px-3 py-2 bg-fuchsia-600/20 hover:bg-fuchsia-600/30 border border-fuchsia-500/30 rounded-lg text-sm text-fuchsia-400 flex items-center justify-center gap-1" title="Mark Out (O)">
                          Mark Out <span class="font-mono">]</span>
                        </button>
                      </div>
                      
                      <!-- Trim Start -->
                      <div>
                        <div class="flex justify-between text-xs text-zinc-500 mb-2">
                          <span>In Point</span>
                          <span class="text-violet-400 font-mono">${formatTimecode(
                            selectedClip.trim.in * selectedClip.duration
                          )}</span>
                        </div>
                        <input type="range" min="0" max="100" step="0.1"
                               value="${selectedClip.trim.in * 100}"
                               class="w-full h-2 bg-zinc-700 rounded-lg appearance-none cursor-pointer accent-violet-500"
                               oninput="editorLiveTrim('${
                                 selectedClip.id
                               }', 'in', this.value / 100)"
                               onchange="editorUpdateTrim('${
                                 selectedClip.id
                               }', 'in', this.value / 100)">
                      </div>
                      
                      <!-- Trim End -->
                      <div>
                        <div class="flex justify-between text-xs text-zinc-500 mb-2">
                          <span>Out Point</span>
                          <span class="text-fuchsia-400 font-mono">${formatTimecode(
                            selectedClip.trim.out * selectedClip.duration
                          )}</span>
                        </div>
                        <input type="range" min="0" max="100" step="0.1"
                               value="${selectedClip.trim.out * 100}"
                               class="w-full h-2 bg-zinc-700 rounded-lg appearance-none cursor-pointer accent-fuchsia-500"
                               oninput="editorLiveTrim('${
                                 selectedClip.id
                               }', 'out', this.value / 100)"
                               onchange="editorUpdateTrim('${
                                 selectedClip.id
                               }', 'out', this.value / 100)">
                      </div>
                    </div>
                    
                    <!-- Duration Info -->
                    <div class="bg-zinc-800/50 rounded-lg p-3 space-y-2">
                      <div class="flex justify-between text-sm">
                        <span class="text-zinc-500">Source Duration</span>
                        <span class="text-zinc-400 font-mono">${selectedClip.duration.toFixed(
                          2
                        )}s</span>
                      </div>
                      <div class="flex justify-between text-sm">
                        <span class="text-zinc-500">Trimmed Duration</span>
                        <span class="text-white font-medium font-mono">${selectedClip.clipDuration.toFixed(
                          2
                        )}s</span>
                      </div>
                      <div class="flex justify-between text-sm">
                        <span class="text-zinc-500">Frame Rate</span>
                        <span class="text-zinc-400 font-mono">${
                          NLE.fps
                        } fps</span>
                      </div>
                    </div>
                    
                    <!-- Actions -->
                    <div class="space-y-2">
                      <div class="text-xs font-medium text-zinc-400 uppercase tracking-wider mb-2">Actions</div>
                      
                      <button onclick="splitAtPlayhead()" 
                              class="w-full px-3 py-2 bg-zinc-800 hover:bg-zinc-700 rounded-lg text-sm flex items-center justify-center gap-2">
                        <svg class="w-4 h-4" viewBox="0 0 24 24" fill="currentColor"><path d="M9.64 7.64c.23-.5.36-1.05.36-1.64 0-2.21-1.79-4-4-4S2 3.79 2 6s1.79 4 4 4c.59 0 1.14-.13 1.64-.36L10 12l-2.36 2.36C7.14 14.13 6.59 14 6 14c-2.21 0-4 1.79-4 4s1.79 4 4 4 4-1.79 4-4c0-.59-.13-1.14-.36-1.64L12 14l7 7h3v-1L9.64 7.64zM6 8c-1.1 0-2-.89-2-2s.9-2 2-2 2 .89 2 2-.9 2-2 2zm0 12c-1.1 0-2-.89-2-2s.9-2 2-2 2 .89 2 2-.9 2-2 2z"/></svg>
                        Split at Playhead
                      </button>
                      
                      <button onclick="duplicateClip('${selectedClip.id}')" 
                              class="w-full px-3 py-2 bg-zinc-800 hover:bg-zinc-700 rounded-lg text-sm flex items-center justify-center gap-2">
                        <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z"/></svg>
                        Duplicate Clip
                      </button>
                      
                      <button onclick="editorResetTrim('${selectedClip.id}')" 
                              class="w-full px-3 py-2 bg-zinc-800 hover:bg-zinc-700 rounded-lg text-sm flex items-center justify-center gap-2">
                        <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 4v5h.582m15.356 2A8.001 8.001 0 004.582 9m0 0H9m11 11v-5h-.581m0 0a8.003 8.003 0 01-15.357-2m15.357 2H15"/></svg>
                        Reset Trim
                      </button>
                      
                      <button onclick="editorRemoveClip('${selectedClip.id}')" 
                              class="w-full px-3 py-2 bg-red-900/30 hover:bg-red-900/50 border border-red-800/50 rounded-lg text-sm text-red-400 flex items-center justify-center gap-2">
                        <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 7l-.867 12.142A2 2 0 0116.138 21H7.862a2 2 0 01-1.995-1.858L5 7m5 4v6m4-6v6m1-10V4a1 1 0 00-1-1h-4a1 1 0 00-1 1v3M4 7h16"/></svg>
                        Delete Clip
                      </button>
                    </div>
                    
                    <!-- Keyboard Hints -->
                    <div class="bg-zinc-800/30 rounded-lg p-3 text-xs text-zinc-500 space-y-1">
                      <div><kbd class="px-1 bg-zinc-700 rounded">I</kbd> Mark In</div>
                      <div><kbd class="px-1 bg-zinc-700 rounded">O</kbd> Mark Out</div>
                      <div><kbd class="px-1 bg-zinc-700 rounded">Ctrl+B</kbd> Split</div>
                      <div><kbd class="px-1 bg-zinc-700 rounded">Ctrl+D</kbd> Duplicate</div>
                      <div><kbd class="px-1 bg-zinc-700 rounded">Del</kbd> Delete</div>
                    </div>
                  </div>
                `
                    : '<p class="text-zinc-500 text-sm text-center py-8">Select a clip to edit</p>'
                }
              </div>
            </div>
          </div>
          
          <!-- Bottom Timeline - DaVinci Resolve Style -->
          <div class="h-40 border-t border-zinc-800 bg-zinc-900 flex-shrink-0 flex flex-col">
            <!-- Timeline toolbar -->
            <div class="h-8 border-b border-zinc-800 flex items-center px-4 bg-zinc-900/80 gap-4">
              <span class="text-xs text-zinc-500">Timeline</span>
              
              <!-- Zoom controls -->
              <div class="flex items-center gap-2 ml-auto">
                <button onclick="zoomTimeline(-1)" class="p-1 hover:bg-zinc-700 rounded text-zinc-400 hover:text-white" title="Zoom Out">
                  <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0zM13 10H7"/></svg>
                </button>
                <input type="range" id="timeline-zoom" min="20" max="200" value="60" 
                       class="w-20 h-1 accent-violet-500" 
                       oninput="setTimelineZoom(this.value)"
                       title="Zoom Level">
                <button onclick="zoomTimeline(1)" class="p-1 hover:bg-zinc-700 rounded text-zinc-400 hover:text-white" title="Zoom In">
                  <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0zM10 7v6m3-3H7"/></svg>
                </button>
                <span class="text-xs text-zinc-600 ml-2">${totalDuration.toFixed(
                  1
                )}s</span>
              </div>
            </div>
            
            <!-- Time ruler - clickable for seeking -->
            <div class="h-6 border-b border-zinc-700 bg-zinc-800/50 overflow-hidden cursor-pointer" id="time-ruler-container" 
                 onmousedown="handleTimelineMouseDown(event)">
              <div class="h-full relative" id="time-ruler" style="width: ${
                totalDuration * pxPerSecond
              }px;">
                ${generateTimeRuler(totalDuration, pxPerSecond)}
              </div>
            </div>
            
            <!-- Timeline track -->
            <div class="flex-1 overflow-x-auto bg-zinc-950/50 cursor-pointer" id="timeline-track" onscroll="syncTimelineScroll()">
              <div class="h-full relative py-2" style="width: ${Math.max(
                totalDuration * pxPerSecond + 100,
                800
              )}px; min-width: 100%;">
                <!-- Playhead - draggable -->
                <div id="timeline-playhead" class="absolute top-0 bottom-0 w-0.5 bg-red-500 z-20" style="left: 8px;">
                  <div class="absolute -top-0 left-1/2 -translate-x-1/2 w-4 h-5 bg-red-500 cursor-ew-resize hover:bg-red-400 transition-colors"
                       style="clip-path: polygon(0 0, 100% 0, 100% 60%, 50% 100%, 0 60%);"
                       onmousedown="startPlayheadDrag(event)"
                       title="Drag to seek"></div>
                </div>
                
                <!-- Video track -->
                <div class="absolute left-0 right-0 top-2 bottom-2 flex items-start gap-0.5 px-2">
                  ${timelineData
                    .map((scene, i) => {
                      const clipWidth = scene.clipDuration * pxPerSecond; // 60px per second
                      return `
                    <div class="timeline-clip-container h-full relative group flex-shrink-0"
                         style="width: ${clipWidth}px;"
                         data-scene-id="${scene.id}"
                         data-index="${i}"
                         draggable="true"
                         ondragstart="editorDragStart(event, ${i})"
                         ondragover="editorDragOver(event)"
                         ondrop="editorDrop(event, ${i})"
                         ondragend="editorDragEnd(event)">
                      <!-- Left trim handle -->
                      <div class="trim-handle trim-handle-left absolute left-0 top-0 bottom-0 w-2 cursor-ew-resize z-10 bg-violet-500 opacity-0 group-hover:opacity-80 hover:opacity-100 transition-opacity rounded-l"
                           onmousedown="startTrimDrag(event, '${
                             scene.id
                           }', 'in', ${i})"
                           title="Drag to trim start"></div>
                      
                      <!-- Clip body -->
                      <div class="timeline-block h-full rounded transition-all overflow-hidden ${
                        i === state.selectedScene
                          ? "ring-2 ring-violet-500 ring-offset-1 ring-offset-zinc-900"
                          : "hover:ring-1 hover:ring-zinc-500"
                      }"
                           style="background: linear-gradient(180deg, ${
                             scene.isUploaded ? "#5b21b6" : "#4b5563"
                           } 0%, ${
                        scene.isUploaded ? "#3b0764" : "#1f2937"
                      } 100%); cursor: ${
                        state.activeTool === "blade" ? "crosshair" : "pointer"
                      };"
                           onclick="handleTimelineClipClick(event, '${
                             scene.id
                           }', ${i})">
                        <!-- Video thumbnail strip -->
                        <div class="h-2/3 bg-black/30 flex items-center justify-center overflow-hidden pointer-events-none">
                          <video src="${
                            scene.generatedVideo
                          }" class="h-full w-full object-cover pointer-events-none" muted></video>
                        </div>
                        <!-- Clip info -->
                        <div class="h-1/3 px-2 py-1 flex items-center justify-between pointer-events-none">
                          <span class="text-xs font-medium text-white truncate">${
                            scene.isUploaded
                              ? scene.fileName?.substring(0, 12) || "Clip"
                              : "Clip " + (i + 1)
                          }</span>
                          <span class="text-xs text-zinc-400">${scene.clipDuration.toFixed(
                            1
                          )}s</span>
                        </div>
                      </div>
                      
                      <!-- Right trim handle -->
                      <div class="trim-handle trim-handle-right absolute right-0 top-0 bottom-0 w-2 cursor-ew-resize z-10 bg-fuchsia-500 opacity-0 group-hover:opacity-80 hover:opacity-100 transition-opacity rounded-r"
                           onmousedown="startTrimDrag(event, '${
                             scene.id
                           }', 'out', ${i})"
                           title="Drag to trim end"></div>
                    </div>
                  `;
                    })
                    .join("")}
                </div>
              </div>
            </div>
          </div>
        </div>
      `;
      }

      // Generate time ruler marks
      function generateTimeRuler(totalDuration, pxPerSecond = 60) {
        let html = "";
        // Calculate good interval based on zoom level
        let interval;
        if (pxPerSecond < 30) interval = 10;
        else if (pxPerSecond < 50) interval = 5;
        else if (pxPerSecond < 100) interval = 2;
        else interval = 1;

        for (let t = 0; t <= totalDuration; t += interval) {
          const left = t * pxPerSecond;
          const mins = Math.floor(t / 60);
          const secs = Math.floor(t % 60);
          html += `<div class="absolute top-0 bottom-0 flex flex-col items-center" style="left: ${left}px;">
          <div class="w-px h-2 bg-zinc-600"></div>
          <span class="text-xs text-zinc-500" style="font-size: 10px;">${mins}:${secs
            .toString()
            .padStart(2, "0")}</span>
        </div>`;
        }
        return html;
      }

      // Timeline zoom state
      let timelineZoom = 60; // pixels per second

      window.setTimelineZoom = (value) => {
        timelineZoom = parseInt(value);
        render(); // Re-render with new zoom
      };

      window.zoomTimeline = (direction) => {
        const zoomSlider = document.getElementById("timeline-zoom");
        if (zoomSlider) {
          const newValue = Math.max(
            20,
            Math.min(200, timelineZoom + direction * 20)
          );
          zoomSlider.value = newValue;
          setTimelineZoom(newValue);
        }
      };

      window.syncTimelineScroll = () => {
        const track = document.getElementById("timeline-track");
        const rulerContainer = document.getElementById("time-ruler-container");
        if (track && rulerContainer) {
          rulerContainer.scrollLeft = track.scrollLeft;
        }
      };

      // ============================================
      // EDITOR FUNCTIONS
      // ============================================
      let draggedSceneIndex = -1;
      let isPlayingAll = false;
      let playAllController = null;

      // Timeline trim drag state
      let trimDragState = null;

      // Start trim drag from timeline handles
      window.startTrimDrag = (e, sceneId, point, index) => {
        e.preventDefault();
        e.stopPropagation();

        const scene = state.scenes.find((s) => s.id === sceneId);
        if (!scene) return;

        // Save state to history BEFORE starting trim (for undo)
        pushHistory();

        // Select this clip
        setState({ selectedScene: index });

        const container = e.target.closest(".timeline-clip-container");
        const trackEl = document.getElementById("timeline-track");
        const startX = e.clientX;
        const startTrim = point === "in" ? scene.trim.in : scene.trim.out;

        trimDragState = {
          sceneId,
          point,
          index,
          startX,
          startTrim,
          sceneDuration: scene.duration,
        };

        // Add visual feedback
        document.body.classList.add("trim-dragging");

        // Seek video to current trim point for live preview
        const player = document.getElementById("preview-player");
        if (player && player.duration) {
          player.currentTime = startTrim * player.duration;
          player.pause();
        }

        document.addEventListener("mousemove", handleTrimDrag);
        document.addEventListener("mouseup", endTrimDrag);
      };

      function handleTrimDrag(e) {
        if (!trimDragState) return;

        const { sceneId, point, startX, startTrim, sceneDuration } =
          trimDragState;
        const deltaX = e.clientX - startX;

        // Convert pixel delta to time delta using current zoom level
        const pxPerSecond =
          typeof timelineZoom !== "undefined" ? timelineZoom : 60;
        const timeDelta = deltaX / pxPerSecond;
        const trimDelta = timeDelta / sceneDuration;

        let newTrim = startTrim + trimDelta;

        // Get current scene
        const scene = state.scenes.find((s) => s.id === sceneId);
        if (!scene) return;

        // Frame-accurate: snap to nearest frame boundary
        const frameSnapped =
          NLE.snapToFrame(newTrim * sceneDuration) / sceneDuration;
        newTrim = frameSnapped;

        // Constrain trim values (minimum 2 frames of content)
        const minFrames = (2 * NLE.frameTime) / sceneDuration;
        if (point === "in") {
          // Can't trim past source start (0) or too close to out point
          newTrim = Math.max(0, Math.min(newTrim, scene.trim.out - minFrames));
        } else {
          // Can't trim past source end (1) or too close to in point
          newTrim = Math.max(scene.trim.in + minFrames, Math.min(newTrim, 1));
        }

        // Update scene trim
        const scenes = state.scenes.map((s) => {
          if (s.id === sceneId) {
            const newTrimObj = { ...s.trim };
            newTrimObj[point] = newTrim;
            return { ...s, trim: newTrimObj };
          }
          return s;
        });

        // Live preview - seek video to trim point for immediate feedback
        const player = document.getElementById("preview-player");
        if (player && player.duration) {
          player.currentTime = newTrim * player.duration;
        }

        // Update state
        setState({ scenes });
      }

      function endTrimDrag(e) {
        if (!trimDragState) return;

        document.body.classList.remove("trim-dragging");
        trimDragState = null;

        // Log the trim operation for debugging
        console.log("[NLE] Trim operation complete");

        document.removeEventListener("mousemove", handleTrimDrag);
        document.removeEventListener("mouseup", endTrimDrag);
      }

      window.editorSelectScene = (index) => {
        setState({ selectedScene: index });
        const player = document.getElementById("preview-player");
        if (player) {
          const scenesWithVideo = state.scenes
            .filter((s) => s.generatedVideo)
            .sort((a, b) => a.order - b.order);
          const scene = scenesWithVideo[index];
          if (scene) {
            // Store current scene info for playback bounds
            player.dataset.sceneId = scene.id;
            player.dataset.trimIn = scene.trim.in;
            player.dataset.trimOut = scene.trim.out;

            player.src = scene.generatedVideo;
            player.load();
            player.onloadedmetadata = () => {
              player.currentTime = scene.trim.in * player.duration;
              updateTimeDisplay();
            };
            // Use the bounded time update handler
            player.ontimeupdate = handleBoundedPlayback;
          }
        }
      };

      // Handle playback within trim bounds
      function handleBoundedPlayback() {
        const player = document.getElementById("preview-player");
        if (!player || !player.duration) return;

        updateTimeDisplay();
        updatePlayhead();

        // Check if we've reached the trim out point
        const trimOut = parseFloat(player.dataset.trimOut || 1);
        const outTime = trimOut * player.duration;

        if (player.currentTime >= outTime - 0.05) {
          player.pause();
          player.currentTime = outTime;
          // Reset play button
          const btn = document.getElementById("play-pause-btn");
          if (btn) {
            btn.innerHTML =
              '<svg class="w-6 h-6" fill="currentColor" viewBox="0 0 24 24"><path d="M8 5v14l11-7z"/></svg>';
          }
        }
      }

      // Update playhead position on timeline
      function updatePlayhead() {
        const player = document.getElementById("preview-player");
        const playhead = document.getElementById("timeline-playhead");
        if (!player || !playhead || !player.duration) return;

        const scenesWithVideo = state.scenes
          .filter((s) => s.generatedVideo)
          .sort((a, b) => a.order - b.order);

        // Calculate total elapsed time up to current position
        let totalElapsed = 0;
        for (let i = 0; i < state.selectedScene; i++) {
          const s = scenesWithVideo[i];
          totalElapsed += s.duration * (s.trim.out - s.trim.in);
        }

        // Add current clip progress
        const currentScene = scenesWithVideo[state.selectedScene];
        if (currentScene) {
          const clipProgress =
            (player.currentTime / player.duration - currentScene.trim.in) /
            (currentScene.trim.out - currentScene.trim.in);
          const clipDuration =
            currentScene.duration *
            (currentScene.trim.out - currentScene.trim.in);
          totalElapsed += Math.max(0, Math.min(clipProgress, 1)) * clipDuration;
        }

        // Position playhead (60px per second default, but use timelineZoom if available)
        const pxPerSecond =
          typeof timelineZoom !== "undefined" ? timelineZoom : 60;
        playhead.style.left = `${totalElapsed * pxPerSecond + 8}px`; // +8 for padding
      }

      function updateTimeDisplay() {
        const player = document.getElementById("preview-player");
        const currentTimeEl = document.getElementById("current-time");
        if (player && currentTimeEl) {
          const mins = Math.floor(player.currentTime / 60);
          const secs = Math.floor(player.currentTime % 60);
          currentTimeEl.textContent = `${mins}:${secs
            .toString()
            .padStart(2, "0")}`;
        }
      }

      window.editorPlayPause = () => {
        const player = document.getElementById("preview-player");
        const btn = document.getElementById("play-pause-btn");
        if (!player) return;

        const scenesWithVideo = state.scenes
          .filter((s) => s.generatedVideo)
          .sort((a, b) => a.order - b.order);
        const scene = scenesWithVideo[state.selectedScene];
        if (!scene) return;

        if (player.paused) {
          // If at or past trim out, reset to trim in
          const trimIn = scene.trim.in * player.duration;
          const trimOut = scene.trim.out * player.duration;

          if (player.currentTime >= trimOut - 0.1) {
            player.currentTime = trimIn;
          }

          player.play();
          btn.innerHTML =
            '<svg class="w-6 h-6" fill="currentColor" viewBox="0 0 24 24"><path d="M6 4h4v16H6zM14 4h4v16h-4z"/></svg>';
        } else {
          player.pause();
          btn.innerHTML =
            '<svg class="w-6 h-6" fill="currentColor" viewBox="0 0 24 24"><path d="M8 5v14l11-7z"/></svg>';
        }
      };

      window.editorPrevScene = () => {
        if (state.selectedScene > 0) {
          editorSelectScene(state.selectedScene - 1);
        }
      };

      window.editorNextScene = () => {
        const scenesWithVideo = state.scenes.filter((s) => s.generatedVideo);
        if (state.selectedScene < scenesWithVideo.length - 1) {
          editorSelectScene(state.selectedScene + 1);
        }
      };

      window.editorUpdateTrim = (sceneId, point, value) => {
        const val = parseFloat(value);
        const scenes = state.scenes.map((s) => {
          if (s.id === sceneId) {
            const newTrim = { ...s.trim };
            if (point === "in") {
              newTrim.in = Math.min(val, s.trim.out - 0.02);
            } else {
              newTrim.out = Math.max(val, s.trim.in + 0.02);
            }
            return { ...s, trim: newTrim };
          }
          return s;
        });
        setState({ scenes });
      };

      // Live preview while dragging trim sliders (no state update, just video seek)
      window.editorLiveTrim = (sceneId, point, value) => {
        const val = parseFloat(value);
        const player = document.getElementById("preview-player");
        if (!player || !player.duration) return;

        // Seek video to show trim position
        player.currentTime = val * player.duration;
        player.pause(); // Pause during scrubbing

        // Update display text without full re-render
        const displayEl =
          point === "in"
            ? document.querySelector(".text-violet-400")
            : document.querySelector(".text-fuchsia-400");

        if (displayEl) {
          const scene = state.scenes.find((s) => s.id === sceneId);
          if (scene) {
            displayEl.textContent = (val * scene.duration).toFixed(2) + "s";
          }
        }
      };

      window.editorResetTrim = (sceneId) => {
        const scenes = state.scenes.map((s) => {
          if (s.id === sceneId) {
            return { ...s, trim: { in: 0, out: 1 } };
          }
          return s;
        });
        setState({ scenes });
      };

      window.editorSetTrimToCurrentTime = (sceneId, point) => {
        const player = document.getElementById("preview-player");
        if (!player || !player.duration) return;

        const percent = player.currentTime / player.duration;
        editorUpdateTrim(sceneId, point, percent);
      };

      window.editorRemoveClip = (sceneId) => {
        if (!confirm("Remove this clip from the timeline?")) return;
        const scenes = state.scenes.map((s) => {
          if (s.id === sceneId) {
            return { ...s, generatedVideo: null };
          }
          return s;
        });
        const remaining = scenes
          .filter((s) => s.generatedVideo)
          .sort((a, b) => a.order - b.order);
        remaining.forEach((s, i) => (s.order = i));

        setStateWithHistory({
          scenes,
          selectedScene: Math.min(state.selectedScene, remaining.length - 1),
        });
      };

      // Upload videos directly to editor
      window.handleEditorVideoUpload = async (event) => {
        const files = Array.from(event.target.files);
        if (!files.length) return;

        console.log(`Uploading ${files.length} video(s) to editor...`);

        const existingScenes = state.scenes.filter((s) => s.generatedVideo);
        const maxOrder =
          existingScenes.length > 0
            ? Math.max(...existingScenes.map((s) => s.order)) + 1
            : 0;

        const newScenes = [];

        for (let i = 0; i < files.length; i++) {
          const file = files[i];

          // Get video duration
          const duration = await new Promise((resolve) => {
            const video = document.createElement("video");
            video.preload = "metadata";
            video.onloadedmetadata = () => {
              resolve(video.duration);
              URL.revokeObjectURL(video.src);
            };
            video.onerror = () => resolve(8); // Default 8s if can't get duration
            video.src = URL.createObjectURL(file);
          });

          // Create object URL for the video
          const videoUrl = URL.createObjectURL(file);

          // Create a scene object for this uploaded video
          const scene = {
            id: `uploaded-${Date.now()}-${i}`,
            order: maxOrder + i,
            frame: null,
            generatedImage: null,
            generatedVideo: videoUrl,
            duration: duration,
            trim: { in: 0, out: 1 },
            isUploaded: true,
            fileName: file.name,
          };

          newScenes.push(scene);
          console.log(
            `Added video: ${file.name}, duration: ${duration.toFixed(1)}s`
          );
        }

        // Add to state
        setState({
          scenes: [...state.scenes, ...newScenes],
          selectedScene: existingScenes.length, // Select first new clip
        });

        // Clear the input so same file can be uploaded again
        event.target.value = "";

        console.log(`Added ${newScenes.length} video(s) to editor`);
      };

      // Drag and Drop handlers
      window.editorDragStart = (e, index) => {
        draggedSceneIndex = index;
        e.currentTarget.classList.add("opacity-50", "scale-95");
        e.dataTransfer.effectAllowed = "move";
        e.dataTransfer.setData("text/plain", index.toString());
      };

      window.editorDragOver = (e) => {
        e.preventDefault();
        e.dataTransfer.dropEffect = "move";
        e.currentTarget.classList.add("border-violet-500");
      };

      window.editorDrop = (e, targetIndex) => {
        e.preventDefault();
        e.currentTarget.classList.remove("border-violet-500");

        if (draggedSceneIndex === -1 || draggedSceneIndex === targetIndex)
          return;

        const scenesWithVideo = state.scenes
          .filter((s) => s.generatedVideo)
          .sort((a, b) => a.order - b.order);
        const sceneIds = scenesWithVideo.map((s) => s.id);

        const [movedId] = sceneIds.splice(draggedSceneIndex, 1);
        sceneIds.splice(targetIndex, 0, movedId);

        const scenes = state.scenes.map((s) => {
          const newOrder = sceneIds.indexOf(s.id);
          if (newOrder !== -1) {
            return { ...s, order: newOrder };
          }
          return s;
        });

        setStateWithHistory({ scenes, selectedScene: targetIndex });
        draggedSceneIndex = -1;
      };

      window.editorDragEnd = (e) => {
        e.currentTarget.classList.remove("opacity-50", "scale-95");
        document.querySelectorAll(".clip-item").forEach((el) => {
          el.classList.remove("border-violet-500");
        });
        draggedSceneIndex = -1;
      };

      // ============================================
      // EDITOR TOOLS - DaVinci Resolve Style
      // ============================================

      /**
       * Handle click on timeline clip - dispatch to appropriate tool
       */
      window.handleTimelineClipClick = (e, sceneId, sceneIndex) => {
        e.preventDefault();
        e.stopPropagation();

        if (state.activeTool === "blade") {
          // Blade tool - cut the clip
          performBladeCut(e, sceneId, sceneIndex);
        } else {
          // Selection tool - select the clip
          editorSelectScene(sceneIndex);
        }
      };

      /**
       * Set active editing tool
       */
      window.setActiveTool = (tool) => {
        setState({ activeTool: tool });
        console.log(`[Editor] Tool changed to: ${tool}`);

        // Update cursor style based on tool
        const timeline = document.getElementById("timeline-track");
        if (timeline) {
          if (tool === "blade") {
            timeline.style.cursor = "crosshair";
            document.body.classList.add("blade-mode");
          } else {
            timeline.style.cursor = "pointer";
            document.body.classList.remove("blade-mode");
          }
        }

        // Update all clip cursors
        document.querySelectorAll(".timeline-block").forEach((el) => {
          el.style.cursor = tool === "blade" ? "crosshair" : "pointer";
        });
      };

      /**
       * Perform blade cut at click position
       */
      function performBladeCut(e, sceneId, sceneIndex) {
        const scene = state.scenes.find((s) => s.id === sceneId);
        if (!scene || !scene.generatedVideo) {
          console.log("[Blade] No scene found");
          return;
        }

        // Get click position within the clip element
        const clipEl = e.currentTarget;
        const rect = clipEl.getBoundingClientRect();
        const clickX = e.clientX - rect.left;
        const clipWidth = rect.width;

        // Calculate cut position as percentage of VISIBLE clip (trimmed portion)
        const cutPercent = Math.max(0.05, Math.min(0.95, clickX / clipWidth));

        // Convert to source time position
        const trimRange = scene.trim.out - scene.trim.in;
        const cutPointNormalized = scene.trim.in + cutPercent * trimRange;

        // Frame-accurate: snap to nearest frame boundary
        const cutPointSnapped =
          NLE.snapToFrame(cutPointNormalized * scene.duration) / scene.duration;

        // Minimum clip duration check (at least 2 frames on each side)
        const minDuration = (2 * NLE.frameTime) / scene.duration;
        if (
          cutPointSnapped < scene.trim.in + minDuration ||
          cutPointSnapped > scene.trim.out - minDuration
        ) {
          console.log("[Blade] Cut point too close to edge");
          return;
        }

        console.log(
          `[Blade] Cutting clip ${sceneId} at ${(cutPointSnapped * 100).toFixed(
            1
          )}%`
        );

        // Create LEFT clip (before cut point)
        const leftClip = {
          ...scene,
          id: `${scene.id}_L_${Date.now()}`,
          trim: {
            in: scene.trim.in,
            out: cutPointSnapped,
          },
        };

        // Create RIGHT clip (after cut point)
        const rightClip = {
          ...scene,
          id: `${scene.id}_R_${Date.now()}`,
          order: scene.order + 0.5,
          trim: {
            in: cutPointSnapped,
            out: scene.trim.out,
          },
        };

        // Build new scenes array
        let newScenes = state.scenes.filter((s) => s.id !== sceneId);
        newScenes.push(leftClip, rightClip);

        // Re-sort by order and renumber
        const withVideo = newScenes
          .filter((s) => s.generatedVideo)
          .sort((a, b) => a.order - b.order);
        withVideo.forEach((s, i) => (s.order = i));

        // Preserve scenes without video
        const withoutVideo = newScenes.filter((s) => !s.generatedVideo);
        newScenes = [...withVideo, ...withoutVideo];

        setStateWithHistory({ scenes: newScenes, selectedScene: sceneIndex });
        console.log(`[Blade] Split complete!`);
      }

      // Legacy handler (keeping for compatibility)
      window.handleBladeCut = (e, sceneId, sceneIndex) => {
        performBladeCut(e, sceneId, sceneIndex);
      };

      /**
       * SPLIT AT PLAYHEAD
       * Keyboard shortcut: Ctrl+B / Cmd+B (DaVinci Resolve standard)
       * Splits the currently selected clip at the playhead position
       */
      window.splitAtPlayhead = () => {
        const player = document.getElementById("preview-player");
        if (!player || !player.duration) {
          console.log("[NLE] No video loaded for split");
          return;
        }

        const scenesWithVideo = state.scenes
          .filter((s) => s.generatedVideo)
          .sort((a, b) => a.order - b.order);
        const currentScene = scenesWithVideo[state.selectedScene];
        if (!currentScene) {
          console.log("[NLE] No clip selected for split");
          return;
        }

        // Get current playhead position as normalized value (0-1)
        const playheadNormalized = player.currentTime / player.duration;

        // Check if playhead is within the clip's trim bounds
        if (
          playheadNormalized <= currentScene.trim.in ||
          playheadNormalized >= currentScene.trim.out
        ) {
          console.log("[NLE] Playhead is outside clip trim bounds");
          return;
        }

        // Frame-accurate: snap to nearest frame
        const cutPointSnapped =
          NLE.snapToFrame(playheadNormalized * currentScene.duration) /
          currentScene.duration;

        // Minimum 2 frames on each side
        const minDuration = (2 * NLE.frameTime) / currentScene.duration;
        if (
          cutPointSnapped < currentScene.trim.in + minDuration ||
          cutPointSnapped > currentScene.trim.out - minDuration
        ) {
          console.log("[NLE] Playhead too close to clip edges (min 2 frames)");
          return;
        }

        console.log(
          `[NLE] Splitting at playhead: ${(cutPointSnapped * 100).toFixed(2)}%`
        );

        // Create split clips
        const leftClip = {
          ...currentScene,
          id: `${currentScene.id}_L_${Date.now()}`,
          trim: { in: currentScene.trim.in, out: cutPointSnapped },
        };

        const rightClip = {
          ...currentScene,
          id: `${currentScene.id}_R_${Date.now()}`,
          order: currentScene.order + 0.5,
          trim: { in: cutPointSnapped, out: currentScene.trim.out },
        };

        // Update scenes
        let newScenes = state.scenes.filter((s) => s.id !== currentScene.id);
        newScenes.push(leftClip, rightClip);

        const withVideo = newScenes
          .filter((s) => s.generatedVideo)
          .sort((a, b) => a.order - b.order);
        withVideo.forEach((s, i) => (s.order = i));

        const withoutVideo = newScenes.filter((s) => !s.generatedVideo);
        newScenes = [...withVideo, ...withoutVideo];

        setStateWithHistory({ scenes: newScenes });
        console.log("[NLE] Split at playhead complete");
      };

      // ============================================
      // HELPER FUNCTIONS
      // ============================================

      /**
       * Format time in seconds to timecode HH:MM:SS:FF
       */
      function formatTimecode(seconds, fps = 30) {
        const h = Math.floor(seconds / 3600);
        const m = Math.floor((seconds % 3600) / 60);
        const s = Math.floor(seconds % 60);
        const f = Math.floor((seconds % 1) * fps);
        return `${String(h).padStart(2, "0")}:${String(m).padStart(
          2,
          "0"
        )}:${String(s).padStart(2, "0")}:${String(f).padStart(2, "0")}`;
      }

      /**
       * Update timecode display
       */
      function updateTimecodeDisplay() {
        const player = document.getElementById("preview-player");
        const timecodeEl = document.getElementById("current-timecode");
        if (player && timecodeEl) {
          timecodeEl.textContent = formatTimecode(player.currentTime);
        }
      }

      // ============================================
      // CLIP OPERATIONS
      // ============================================

      /**
       * Duplicate the currently selected clip
       */
      window.duplicateSelectedClip = () => {
        const scenesWithVideo = state.scenes
          .filter((s) => s.generatedVideo)
          .sort((a, b) => a.order - b.order);
        const currentScene = scenesWithVideo[state.selectedScene];
        if (!currentScene) return;
        duplicateClip(currentScene.id);
      };

      /**
       * Duplicate a specific clip by ID
       */
      window.duplicateClip = (clipId) => {
        const scene = state.scenes.find((s) => s.id === clipId);
        if (!scene) return;

        const newClip = {
          ...scene,
          id: `${scene.id}_dup_${Date.now()}`,
          order: scene.order + 0.5,
        };

        let newScenes = [...state.scenes, newClip];
        const withVideo = newScenes
          .filter((s) => s.generatedVideo)
          .sort((a, b) => a.order - b.order);
        withVideo.forEach((s, i) => (s.order = i));

        const withoutVideo = newScenes.filter((s) => !s.generatedVideo);
        newScenes = [...withVideo, ...withoutVideo];

        // Select the new clip
        const newIndex = withVideo.findIndex((s) => s.id === newClip.id);
        setStateWithHistory({
          scenes: newScenes,
          selectedScene: newIndex >= 0 ? newIndex : state.selectedScene,
        });
        console.log("[Editor] Clip duplicated");
      };

      /**
       * Delete the currently selected clip
       */
      window.deleteSelectedClip = () => {
        const scenesWithVideo = state.scenes
          .filter((s) => s.generatedVideo)
          .sort((a, b) => a.order - b.order);
        if (scenesWithVideo.length === 0) return;

        const currentScene = scenesWithVideo[state.selectedScene];
        if (!currentScene) return;

        editorRemoveClip(currentScene.id);
      };

      // ============================================
      // PLAYBACK NAVIGATION
      // ============================================

      /**
       * Jump to start of timeline
       */
      window.jumpToStart = () => {
        const scenesWithVideo = state.scenes
          .filter((s) => s.generatedVideo)
          .sort((a, b) => a.order - b.order);
        if (scenesWithVideo.length === 0) return;

        setState({ selectedScene: 0 });
        const player = document.getElementById("preview-player");
        if (player) {
          player.src = scenesWithVideo[0].generatedVideo;
          player.currentTime =
            scenesWithVideo[0].trim.in * scenesWithVideo[0].duration;
        }
      };

      /**
       * Jump to end of timeline
       */
      window.jumpToEnd = () => {
        const scenesWithVideo = state.scenes
          .filter((s) => s.generatedVideo)
          .sort((a, b) => a.order - b.order);
        if (scenesWithVideo.length === 0) return;

        const lastIndex = scenesWithVideo.length - 1;
        setState({ selectedScene: lastIndex });
        const player = document.getElementById("preview-player");
        if (player) {
          const lastScene = scenesWithVideo[lastIndex];
          player.src = lastScene.generatedVideo;
          player.currentTime = lastScene.trim.out * lastScene.duration;
        }
      };

      /**
       * Step backward by frames
       */
      window.stepBackward = (frames = 1) => {
        const player = document.getElementById("preview-player");
        if (player) {
          player.pause();
          player.currentTime = Math.max(
            0,
            player.currentTime - frames / NLE.fps
          );
          updateTimecodeDisplay();
        }
      };

      /**
       * Step forward by frames
       */
      window.stepForward = (frames = 1) => {
        const player = document.getElementById("preview-player");
        if (player) {
          player.pause();
          player.currentTime = Math.min(
            player.duration,
            player.currentTime + frames / NLE.fps
          );
          updateTimecodeDisplay();
        }
      };

      /**
       * Go to previous clip
       */
      window.previousClip = () => {
        if (state.selectedScene > 0) {
          editorSelectScene(state.selectedScene - 1);
        }
      };

      /**
       * Go to next clip
       */
      window.nextClip = () => {
        const scenesWithVideo = state.scenes.filter((s) => s.generatedVideo);
        if (state.selectedScene < scenesWithVideo.length - 1) {
          editorSelectScene(state.selectedScene + 1);
        }
      };

      /**
       * Fit timeline zoom to show all clips
       */
      window.fitTimelineToView = () => {
        const scenesWithVideo = state.scenes
          .filter((s) => s.generatedVideo)
          .sort((a, b) => a.order - b.order);
        const totalDuration = scenesWithVideo.reduce(
          (sum, s) => sum + s.duration * (s.trim.out - s.trim.in),
          0
        );

        const timelineContainer = document.getElementById("timeline-container");
        if (timelineContainer && totalDuration > 0) {
          const containerWidth = timelineContainer.clientWidth - 120; // Account for padding
          const optimalZoom = Math.max(
            20,
            Math.min(200, containerWidth / totalDuration)
          );
          setTimelineZoom(optimalZoom);
          console.log(
            `[Timeline] Fit to view: ${optimalZoom.toFixed(0)}px/sec`
          );
        }
      };

      // Play all clips in sequence
      // ============================================
      // PLAY ALL - Sequential playback across clips
      // ============================================
      window.editorPlayAll = async () => {
        const scenesWithVideo = state.scenes
          .filter((s) => s.generatedVideo)
          .sort((a, b) => a.order - b.order);
        if (scenesWithVideo.length === 0) return;

        const playAllBtn = document.querySelector(
          '[onclick="editorPlayAll()"]'
        );

        if (isPlayingAll) {
          // Stop playback
          isPlayingAll = false;
          const player = document.getElementById("preview-player");
          if (player) player.pause();
          if (playAllBtn)
            playAllBtn.innerHTML =
              '<svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24"><path d="M8 5v14l11-7z"/></svg> Play All';
          return;
        }

        isPlayingAll = true;
        if (playAllBtn)
          playAllBtn.innerHTML =
            '<svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24"><path d="M6 4h4v16H6zM14 4h4v16h-4z"/></svg> Stop';

        // Start from current selected scene or first
        let currentIndex = state.selectedScene || 0;

        const playClip = async (index) => {
          if (!isPlayingAll || index >= scenesWithVideo.length) {
            isPlayingAll = false;
            if (playAllBtn)
              playAllBtn.innerHTML =
                '<svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24"><path d="M8 5v14l11-7z"/></svg> Play All';
            return;
          }

          const scene = scenesWithVideo[index];
          currentPlayingIndex = index;

          // Update selection without full re-render during playback
          state.selectedScene = index;

          // Highlight the current clip visually
          document
            .querySelectorAll(".timeline-clip-container")
            .forEach((el, i) => {
              const block = el.querySelector(".timeline-block");
              if (block) {
                if (i === index) {
                  block.classList.add("ring-2", "ring-violet-500");
                } else {
                  block.classList.remove("ring-2", "ring-violet-500");
                }
              }
            });

          const player = document.getElementById("preview-player");
          if (!player) return;

          // Load the clip
          player.src = scene.generatedVideo;

          await new Promise((resolve, reject) => {
            player.onloadedmetadata = resolve;
            player.onerror = reject;
            player.load();
          });

          // Set start position
          const startTime = scene.trim.in * player.duration;
          const endTime = scene.trim.out * player.duration;
          player.currentTime = startTime;

          // Play with boundary checking
          await new Promise((resolve) => {
            const checkBoundary = () => {
              if (!isPlayingAll) {
                player.pause();
                resolve();
                return;
              }

              updatePlayheadForPlayAll(
                index,
                player.currentTime,
                player.duration,
                scenesWithVideo
              );

              if (player.currentTime >= endTime - 0.05) {
                player.pause();
                resolve();
              }
            };

            player.ontimeupdate = checkBoundary;
            player.onended = resolve;
            player.play().catch(resolve);
          });

          // Move to next clip
          if (isPlayingAll) {
            await playClip(index + 1);
          }
        };

        await playClip(currentIndex);
      };

      let currentPlayingIndex = 0;

      function updatePlayheadForPlayAll(
        clipIndex,
        currentTime,
        clipDuration,
        scenesWithVideo
      ) {
        const playhead = document.getElementById("timeline-playhead");
        if (!playhead) return;

        const pxPerSecond =
          typeof timelineZoom !== "undefined" ? timelineZoom : 60;

        // Calculate position: sum of previous clips + current position
        let pxPosition = 8; // Initial padding

        for (let i = 0; i < clipIndex; i++) {
          const s = scenesWithVideo[i];
          const clipTrimmedDuration = s.duration * (s.trim.out - s.trim.in);
          pxPosition += clipTrimmedDuration * pxPerSecond + 2; // +2 for gap
        }

        // Add current clip progress
        const currentScene = scenesWithVideo[clipIndex];
        if (currentScene && clipDuration) {
          const trimIn = currentScene.trim.in;
          const trimOut = currentScene.trim.out;
          const clipProgress =
            (currentTime / clipDuration - trimIn) / (trimOut - trimIn);
          const clipTrimmedDuration =
            currentScene.duration * (trimOut - trimIn);
          pxPosition +=
            Math.max(0, Math.min(1, clipProgress)) *
            clipTrimmedDuration *
            pxPerSecond;
        }

        playhead.style.left = `${pxPosition}px`;
      }

      // ============================================
      // TIMELINE SCRUBBING - Click/drag to seek
      // ============================================
      let isScrubbingTimeline = false;

      window.initTimelineScrubbing = () => {
        const track = document.getElementById("timeline-track");
        const ruler = document.getElementById("time-ruler-container");
        const player = document.getElementById("preview-player");

        if (track) {
          track.addEventListener("mousedown", handleTimelineMouseDown);
        }
        if (ruler) {
          ruler.addEventListener("mousedown", handleTimelineMouseDown);
        }

        // Set up video player event listeners for timecode display
        if (player) {
          player.addEventListener("timeupdate", () => {
            updateTimecodeDisplay();
            updatePlayhead();
          });
          player.addEventListener("loadedmetadata", () => {
            updateTimecodeDisplay();
          });
        }
      };

      function handleTimelineMouseDown(e) {
        // Only handle clicks on the track background, not on clips
        if (
          e.target.closest(".timeline-clip-container") &&
          !e.target.classList.contains("trim-handle")
        ) {
          return;
        }

        isScrubbingTimeline = true;
        document.body.style.cursor = "col-resize";

        seekToTimelinePosition(e);

        document.addEventListener("mousemove", handleTimelineScrub);
        document.addEventListener("mouseup", handleTimelineScrubEnd);
      }

      function handleTimelineScrub(e) {
        if (!isScrubbingTimeline) return;
        seekToTimelinePosition(e);
      }

      function handleTimelineScrubEnd(e) {
        isScrubbingTimeline = false;
        document.body.style.cursor = "";
        document.removeEventListener("mousemove", handleTimelineScrub);
        document.removeEventListener("mouseup", handleTimelineScrubEnd);
      }

      function seekToTimelinePosition(e) {
        const track = document.getElementById("timeline-track");
        if (!track) return;

        const rect = track.getBoundingClientRect();
        const scrollLeft = track.scrollLeft;
        const clickX = e.clientX - rect.left + scrollLeft - 8; // -8 for padding

        const pxPerSecond =
          typeof timelineZoom !== "undefined" ? timelineZoom : 60;
        const clickedTime = clickX / pxPerSecond;

        const scenesWithVideo = state.scenes
          .filter((s) => s.generatedVideo)
          .sort((a, b) => a.order - b.order);

        // Find which clip this time falls into
        let cumulativeTime = 0;
        let targetClipIndex = 0;
        let timeWithinClip = 0;

        for (let i = 0; i < scenesWithVideo.length; i++) {
          const scene = scenesWithVideo[i];
          const clipDuration =
            scene.duration * (scene.trim.out - scene.trim.in);

          if (clickedTime <= cumulativeTime + clipDuration) {
            targetClipIndex = i;
            timeWithinClip = clickedTime - cumulativeTime;
            break;
          }
          cumulativeTime += clipDuration;
          targetClipIndex = i;
          timeWithinClip = clipDuration;
        }

        const targetScene = scenesWithVideo[targetClipIndex];
        if (!targetScene) return;

        // Convert timeWithinClip to actual video time
        const clipTrimmedDuration =
          targetScene.duration * (targetScene.trim.out - targetScene.trim.in);
        const progressInClip = Math.min(
          timeWithinClip / clipTrimmedDuration,
          1
        );
        const actualVideoTime =
          targetScene.trim.in +
          progressInClip * (targetScene.trim.out - targetScene.trim.in);

        // Update playhead position visually
        const playhead = document.getElementById("timeline-playhead");
        if (playhead) {
          playhead.style.left = `${Math.max(8, clickX + 8)}px`;
        }

        // Load and seek the clip if different from current
        if (targetClipIndex !== state.selectedScene) {
          state.selectedScene = targetClipIndex;

          // Update clip selection visually
          document
            .querySelectorAll(".timeline-clip-container")
            .forEach((el, i) => {
              const block = el.querySelector(".timeline-block");
              if (block) {
                if (i === targetClipIndex) {
                  block.classList.add(
                    "ring-2",
                    "ring-violet-500",
                    "ring-offset-1",
                    "ring-offset-zinc-900"
                  );
                } else {
                  block.classList.remove(
                    "ring-2",
                    "ring-violet-500",
                    "ring-offset-1",
                    "ring-offset-zinc-900"
                  );
                }
              }
            });
        }

        const player = document.getElementById("preview-player");
        if (!player) return;

        // If same clip, just seek
        if (
          player.src === targetScene.generatedVideo ||
          player.src.endsWith(targetScene.generatedVideo.split("/").pop())
        ) {
          player.currentTime = actualVideoTime * player.duration;
        } else {
          // Load new clip and seek
          player.src = targetScene.generatedVideo;
          player.onloadedmetadata = () => {
            player.currentTime = actualVideoTime * player.duration;
            // Store trim bounds for playback
            player.dataset.trimIn = targetScene.trim.in;
            player.dataset.trimOut = targetScene.trim.out;
          };
          player.load();
        }
      }

      // Initialize scrubbing after render
      setTimeout(() => {
        if (state.step === 2) {
          initTimelineScrubbing();
        }
      }, 100);

      // ============================================
      // PLAYHEAD DRAGGING
      // ============================================
      let isDraggingPlayhead = false;

      window.startPlayheadDrag = (e) => {
        e.preventDefault();
        e.stopPropagation();

        isDraggingPlayhead = true;
        document.body.style.cursor = "ew-resize";

        document.addEventListener("mousemove", handlePlayheadDrag);
        document.addEventListener("mouseup", endPlayheadDrag);
      };

      function handlePlayheadDrag(e) {
        if (!isDraggingPlayhead) return;
        seekToTimelinePosition(e);
      }

      function endPlayheadDrag(e) {
        isDraggingPlayhead = false;
        document.body.style.cursor = "";
        document.removeEventListener("mousemove", handlePlayheadDrag);
        document.removeEventListener("mouseup", endPlayheadDrag);
      }

      // ============================================
      // DOWNLOAD ALL ASSETS AS ZIP
      // ============================================
      window.downloadAllAssets = async function () {
        if (state.scenes.length === 0) {
          alert("No scenes to download. Add scenes first.");
          return;
        }

        // Create or get progress overlay
        let overlay = document.getElementById("export-overlay");
        let statusEl, progressEl;

        // If overlay doesn't exist (we're in Generate view), create a temporary one
        if (!overlay) {
          overlay = document.createElement("div");
          overlay.id = "download-overlay-temp";
          overlay.className =
            "fixed inset-0 bg-black/80 flex items-center justify-center z-50";
          overlay.innerHTML = `
          <div class="bg-zinc-900 rounded-2xl p-8 max-w-md w-full mx-4 text-center border border-zinc-700">
            <div class="w-16 h-16 mx-auto mb-4 rounded-full bg-emerald-500/20 flex items-center justify-center">
              <svg class="w-8 h-8 text-emerald-400 animate-pulse" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 16v1a3 3 0 003 3h10a3 3 0 003-3v-1m-4-4l-4 4m0 0l-4-4m4 4V4"/>
              </svg>
            </div>
            <h3 class="text-lg font-semibold mb-2">Preparing Download</h3>
            <p id="download-status" class="text-zinc-400 mb-4">Initializing...</p>
            <p id="download-progress" class="text-2xl font-bold text-emerald-400">0%</p>
          </div>
        `;
          document.body.appendChild(overlay);
          statusEl = document.getElementById("download-status");
          progressEl = document.getElementById("download-progress");
        } else {
          overlay.classList.remove("hidden");
          statusEl = document.getElementById("export-status");
          progressEl = document.getElementById("export-progress");
        }

        const updateStatus = (text) => {
          if (statusEl) statusEl.textContent = text;
        };
        const updateProgress = (text) => {
          if (progressEl) progressEl.textContent = text;
        };

        updateStatus("Preparing ZIP file...");
        updateProgress("0%");

        try {
          const zip = new JSZip();
          const timestamp = new Date().toISOString().slice(0, 10);
          const projectName = `sceneforge-export-${timestamp}`;

          // Create folders
          const imagesFolder = zip.folder("01_images");
          const promptsFolder = zip.folder("02_prompts");
          const videosFolder = zip.folder("03_videos");
          const metadataFolder = zip.folder("04_metadata");

          // Sort scenes by order
          const sortedScenes = [...state.scenes].sort(
            (a, b) => a.order - b.order
          );

          // Create manifest data
          const manifest = {
            exportDate: new Date().toISOString(),
            totalScenes: sortedScenes.length,
            scenesWithImages: sortedScenes.filter((s) => s.generatedImage)
              .length,
            scenesWithVideos: sortedScenes.filter((s) => s.generatedVideo)
              .length,
            videoModel: state.videoModel,
            scenes: [],
          };

          // Process each scene
          for (let i = 0; i < sortedScenes.length; i++) {
            const scene = sortedScenes[i];
            const sceneNum = String(i + 1).padStart(3, "0");
            const progress = Math.round(((i + 1) / sortedScenes.length) * 80);

            updateStatus(`Processing scene ${i + 1}/${sortedScenes.length}...`);
            updateProgress(`${progress}%`);

            // Extract prompts from object structure for manifest
            const imagePromptText =
              scene.imagePrompt?.image_prompt ||
              scene.imagePrompt?.positive ||
              (typeof scene.imagePrompt === "string" ? scene.imagePrompt : "");
            const negativePromptText =
              scene.imagePrompt?.negative_prompt ||
              scene.imagePrompt?.negative ||
              "";
            const motionText =
              scene.motionPrompt?.motion ||
              (typeof scene.motionPrompt === "string"
                ? scene.motionPrompt
                : "");
            const speechText = scene.motionPrompt?.speech || "";

            // Scene metadata for manifest
            const sceneData = {
              sceneNumber: i + 1,
              duration: scene.duration,
              hasImage: !!scene.generatedImage,
              hasVideo: !!scene.generatedVideo,
              prompts: {
                image: imagePromptText,
                negative: negativePromptText,
                motion: motionText,
                speech: speechText,
                voiceover: scene.voiceoverText || "",
              },
            };
            manifest.scenes.push(sceneData);

            // Save image
            if (scene.generatedImage) {
              try {
                let imageData = scene.generatedImage;
                let extension = "png";

                // Handle base64 data
                if (imageData.startsWith("data:")) {
                  const match = imageData.match(
                    /^data:image\/(\w+);base64,(.+)$/
                  );
                  if (match) {
                    extension = match[1] === "jpeg" ? "jpg" : match[1];
                    imageData = match[2];
                  }
                } else if (imageData.startsWith("http")) {
                  // Fetch remote image
                  try {
                    const response = await fetch(imageData);
                    const blob = await response.blob();
                    const arrayBuffer = await blob.arrayBuffer();
                    imagesFolder.file(
                      `scene_${sceneNum}.${extension}`,
                      arrayBuffer
                    );
                    continue;
                  } catch (e) {
                    console.error("Failed to fetch remote image:", e);
                  }
                }

                // Decode base64
                const binaryString = atob(imageData);
                const bytes = new Uint8Array(binaryString.length);
                for (let j = 0; j < binaryString.length; j++) {
                  bytes[j] = binaryString.charCodeAt(j);
                }
                imagesFolder.file(`scene_${sceneNum}.${extension}`, bytes);
              } catch (e) {
                console.error(`Failed to add image for scene ${i + 1}:`, e);
              }
            }

            // Save prompts as text files
            let promptContent = `SCENE ${i + 1}\n`;
            promptContent += `${"=".repeat(50)}\n\n`;
            promptContent += `Duration: ${scene.duration}s\n\n`;

            // Use already extracted prompt variables
            if (imagePromptText) {
              promptContent += `IMAGE PROMPT:\n${"-".repeat(
                30
              )}\n${imagePromptText}\n\n`;
            }
            if (negativePromptText) {
              promptContent += `NEGATIVE PROMPT:\n${"-".repeat(
                30
              )}\n${negativePromptText}\n\n`;
            }
            if (motionText) {
              promptContent += `MOTION PROMPT:\n${"-".repeat(
                30
              )}\n${motionText}\n\n`;
            }
            if (speechText) {
              promptContent += `SPEECH/DIALOGUE:\n${"-".repeat(
                30
              )}\n${speechText}\n\n`;
            }
            if (scene.voiceoverText) {
              promptContent += `VOICEOVER TEXT:\n${"-".repeat(30)}\n${
                scene.voiceoverText
              }\n\n`;
            }

            promptsFolder.file(`scene_${sceneNum}_prompts.txt`, promptContent);

            // Save video if exists
            if (scene.generatedVideo) {
              try {
                let videoUrl = scene.generatedVideo;

                // Handle blob URLs
                if (videoUrl.startsWith("blob:")) {
                  // Can't download blob URLs from ZIP context
                  console.log(
                    `Scene ${i + 1} video is blob URL, skipping in ZIP`
                  );
                  sceneData.videoNote =
                    "Video was blob URL - use Export MP4 for videos";
                } else if (videoUrl.startsWith("http")) {
                  // Fetch remote video
                  updateStatus(
                    `Downloading video ${i + 1}/${sortedScenes.length}...`
                  );
                  try {
                    const response = await fetch(videoUrl);
                    if (response.ok) {
                      const blob = await response.blob();
                      const arrayBuffer = await blob.arrayBuffer();
                      videosFolder.file(`scene_${sceneNum}.mp4`, arrayBuffer);
                    }
                  } catch (e) {
                    console.error("Failed to fetch remote video:", e);
                    sceneData.videoNote = "Failed to download video";
                  }
                }
              } catch (e) {
                console.error(`Failed to add video for scene ${i + 1}:`, e);
              }
            }
          }

          // Add manifest JSON
          metadataFolder.file(
            "manifest.json",
            JSON.stringify(manifest, null, 2)
          );

          // Create combined prompts file
          let allPromptsText = `SCENEFORGE EXPORT - ALL PROMPTS\n`;
          allPromptsText += `${"=".repeat(60)}\n`;
          allPromptsText += `Export Date: ${new Date().toLocaleString()}\n`;
          allPromptsText += `Total Scenes: ${sortedScenes.length}\n`;
          allPromptsText += `${"=".repeat(60)}\n\n`;

          sortedScenes.forEach((scene, i) => {
            allPromptsText += `\n${"─".repeat(60)}\n`;
            allPromptsText += `SCENE ${i + 1} (${scene.duration}s)\n`;
            allPromptsText += `${"─".repeat(60)}\n\n`;

            // Extract prompts from object structure
            const imagePromptText =
              scene.imagePrompt?.image_prompt ||
              scene.imagePrompt?.positive ||
              (typeof scene.imagePrompt === "string" ? scene.imagePrompt : "");
            const negativePromptText =
              scene.imagePrompt?.negative_prompt ||
              scene.imagePrompt?.negative ||
              "";
            const motionText =
              scene.motionPrompt?.motion ||
              (typeof scene.motionPrompt === "string"
                ? scene.motionPrompt
                : "");
            const speechText = scene.motionPrompt?.speech || "";

            if (imagePromptText) {
              allPromptsText += `[IMAGE PROMPT]\n${imagePromptText}\n\n`;
            }
            if (negativePromptText) {
              allPromptsText += `[NEGATIVE PROMPT]\n${negativePromptText}\n\n`;
            }
            if (motionText) {
              allPromptsText += `[MOTION]\n${motionText}\n\n`;
            }
            if (speechText) {
              allPromptsText += `[SPEECH/DIALOGUE]\n${speechText}\n\n`;
            }
          });

          metadataFolder.file("all_prompts.txt", allPromptsText);

          // Create README
          let readme = `# SceneForge Export\n\n`;
          readme += `**Export Date:** ${new Date().toLocaleString()}\n\n`;
          readme += `## Contents\n\n`;
          readme += `- \`01_images/\` - Generated scene images (PNG)\n`;
          readme += `- \`02_prompts/\` - Individual scene prompts (TXT)\n`;
          readme += `- \`03_videos/\` - Generated scene videos (MP4)\n`;
          readme += `- \`04_metadata/\` - Manifest and combined prompts\n\n`;
          readme += `## Statistics\n\n`;
          readme += `- Total Scenes: ${sortedScenes.length}\n`;
          readme += `- Images Generated: ${manifest.scenesWithImages}\n`;
          readme += `- Videos Generated: ${manifest.scenesWithVideos}\n`;
          readme += `- Video Model: ${state.videoModel}\n\n`;
          readme += `## File Naming\n\n`;
          readme += `Files are numbered with 3-digit padding (001, 002, etc.) for proper sorting.\n`;

          zip.file("README.md", readme);

          // Generate ZIP
          updateStatus("Compressing ZIP file...");
          updateProgress("90%");

          const zipBlob = await zip.generateAsync(
            {
              type: "blob",
              compression: "DEFLATE",
              compressionOptions: { level: 6 },
            },
            (metadata) => {
              updateProgress(`${Math.round(90 + metadata.percent * 0.1)}%`);
            }
          );

          // Download
          const url = URL.createObjectURL(zipBlob);
          const link = document.createElement("a");
          link.href = url;
          link.download = `${projectName}.zip`;
          document.body.appendChild(link);
          link.click();
          document.body.removeChild(link);
          URL.revokeObjectURL(url);

          const sizeMB = (zipBlob.size / 1024 / 1024).toFixed(1);
          updateStatus(`Download complete! (${sizeMB} MB)`);
          updateProgress("100%");

          console.log(
            `[ZIP Export] Complete: ${sizeMB} MB, ${sortedScenes.length} scenes`
          );

          setTimeout(() => {
            if (overlay.id === "download-overlay-temp") {
              overlay.remove();
            } else {
              overlay?.classList.add("hidden");
            }
          }, 2000);
        } catch (error) {
          console.error("ZIP export error:", error);
          updateStatus(`Error: ${error.message}`);
          setTimeout(() => {
            if (overlay.id === "download-overlay-temp") {
              overlay.remove();
            } else {
              overlay?.classList.add("hidden");
            }
          }, 3000);
        }
      };

      // Export merged video using Canvas + MediaRecorder with audio support
      /**
       * ============================================
       * EXPORT / OFFLINE RENDER PIPELINE
       * ============================================
       *
       * Export differs from live preview:
       * - Deterministic render (not real-time dependent)
       * - Higher quality settings
       * - Full resolution decode
       * - Proper frame timing
       * - Audio mixing from all clips
       *
       * Pipeline:
       * 1. Create AudioContext and audio destination for mixing
       * 2. For each clip:
       *    a. Play video with audio connected to mixer
       *    b. Draw frames to canvas
       *    c. Record combined video+audio stream
       * 3. Mux to MP4 container using mp4-muxer
       *
       * Frame timing: t = frameNumber / fps
       */
      window.exportMergedVideo = async () => {
        const scenesWithVideo = state.scenes
          .filter((s) => s.generatedVideo)
          .sort((a, b) => a.order - b.order);
        if (scenesWithVideo.length === 0) {
          alert("No videos to export");
          return;
        }

        const overlay = document.getElementById("export-overlay");
        const statusEl = document.getElementById("export-status");
        const progressEl = document.getElementById("export-progress");

        overlay?.classList.remove("hidden");
        statusEl.textContent = "Initializing MP4 export...";
        progressEl.textContent = "0%";

        try {
          // Calculate total timeline duration
          const totalDuration = scenesWithVideo.reduce((sum, s) => {
            return sum + s.duration * (s.trim.out - s.trim.in);
          }, 0);

          console.log(
            `[NLE Export] Starting MP4 render: ${
              scenesWithVideo.length
            } clips, ${totalDuration.toFixed(2)}s total`
          );

          // Create canvas for video compositing (9:16 vertical format)
          const canvas = document.createElement("canvas");
          canvas.width = 720;
          canvas.height = 1280;
          const ctx = canvas.getContext("2d", {
            alpha: false,
            willReadFrequently: true,
          });

          // Fill with black initially
          ctx.fillStyle = "#000";
          ctx.fillRect(0, 0, canvas.width, canvas.height);

          // Target FPS for export
          const targetFPS = 30;

          // Check if WebCodecs is available for native MP4 encoding
          console.log("[NLE Export] Checking capabilities...");
          console.log("[NLE Export] VideoEncoder:", typeof VideoEncoder);
          console.log("[NLE Export] Mp4Muxer:", typeof Mp4Muxer);
          console.log("[NLE Export] window.Mp4Muxer:", typeof window.Mp4Muxer);

          // Try to find mp4-muxer (might be exposed differently)
          const mp4MuxerLib =
            typeof Mp4Muxer !== "undefined"
              ? Mp4Muxer
              : typeof window.Mp4Muxer !== "undefined"
              ? window.Mp4Muxer
              : null;

          const useWebCodecs =
            typeof VideoEncoder !== "undefined" && mp4MuxerLib !== null;

          if (useWebCodecs) {
            console.log(
              "[NLE Export] Using WebCodecs + mp4-muxer for native MP4"
            );
            await exportWithWebCodecs(
              scenesWithVideo,
              canvas,
              ctx,
              targetFPS,
              totalDuration,
              statusEl,
              progressEl,
              overlay,
              mp4MuxerLib
            );
          } else {
            const reason =
              typeof VideoEncoder === "undefined"
                ? "VideoEncoder not supported"
                : "mp4-muxer not loaded";
            console.log(
              `[NLE Export] ${reason}, falling back to MediaRecorder (WebM)`
            );
            statusEl.textContent = `Note: ${reason}. Using WebM format.`;
            await new Promise((r) => setTimeout(r, 1500)); // Let user see the message
            await exportWithMediaRecorder(
              scenesWithVideo,
              canvas,
              ctx,
              targetFPS,
              totalDuration,
              statusEl,
              progressEl,
              overlay
            );
          }
        } catch (error) {
          console.error("[NLE Export] Error:", error);
          statusEl.textContent = `Export failed: ${error.message}`;
          progressEl.textContent = "";

          setTimeout(() => {
            overlay?.classList.add("hidden");
          }, 3000);
        }
      };

      // Export using WebCodecs + mp4-muxer (native MP4)
      async function exportWithWebCodecs(
        scenesWithVideo,
        canvas,
        ctx,
        targetFPS,
        totalDuration,
        statusEl,
        progressEl,
        overlay,
        mp4MuxerLib
      ) {
        // Use passed library or try global
        const MuxerLib = mp4MuxerLib || Mp4Muxer || window.Mp4Muxer;
        if (!MuxerLib) {
          throw new Error("mp4-muxer library not found");
        }

        const sampleRate = 48000;
        const numberOfChannels = 2;

        // Set up mp4-muxer with both video and audio
        const muxer = new MuxerLib.Muxer({
          target: new MuxerLib.ArrayBufferTarget(),
          video: {
            codec: "avc",
            width: canvas.width,
            height: canvas.height,
          },
          audio: {
            codec: "aac",
            numberOfChannels: numberOfChannels,
            sampleRate: sampleRate,
          },
          fastStart: "in-memory",
          firstTimestampBehavior: "offset",
        });

        // Set up VideoEncoder
        const videoEncoder = new VideoEncoder({
          output: (chunk, meta) => {
            muxer.addVideoChunk(chunk, meta);
          },
          error: (e) => console.error("VideoEncoder error:", e),
        });

        videoEncoder.configure({
          codec: "avc1.4d0028",
          width: canvas.width,
          height: canvas.height,
          bitrate: 8_000_000,
          framerate: targetFPS,
          latencyMode: "quality",
        });

        // Set up AudioEncoder
        const audioEncoder = new AudioEncoder({
          output: (chunk, meta) => {
            muxer.addAudioChunk(chunk, meta);
          },
          error: (e) => console.error("AudioEncoder error:", e),
        });

        audioEncoder.configure({
          codec: "mp4a.40.2",
          numberOfChannels: numberOfChannels,
          sampleRate: sampleRate,
          bitrate: 192000,
        });

        // Create AudioContext for decoding source audio
        const audioContext = new AudioContext({ sampleRate: sampleRate });

        // Process each clip
        let videoTimestamp = 0; // microseconds
        let audioTimestamp = 0; // microseconds

        for (let clipIdx = 0; clipIdx < scenesWithVideo.length; clipIdx++) {
          const scene = scenesWithVideo[clipIdx];
          statusEl.textContent = `Encoding clip ${clipIdx + 1}/${
            scenesWithVideo.length
          }...`;

          // Load video
          const videoEl = document.createElement("video");
          videoEl.muted = false;
          videoEl.playsInline = true;
          videoEl.preload = "auto";
          videoEl.crossOrigin = "anonymous";

          await new Promise((resolve, reject) => {
            videoEl.onloadedmetadata = resolve;
            videoEl.onerror = reject;
            videoEl.src = scene.generatedVideo;
          });

          const clipStartTime = scene.trim.in * videoEl.duration;
          const clipEndTime = scene.trim.out * videoEl.duration;
          const clipDuration = clipEndTime - clipStartTime;
          const totalFrames = Math.ceil(clipDuration * targetFPS);

          // Extract and encode audio from this clip
          statusEl.textContent = `Extracting audio from clip ${clipIdx + 1}...`;

          try {
            // Fetch the video file to get audio
            const response = await fetch(scene.generatedVideo);
            const arrayBuffer = await response.arrayBuffer();

            // Decode audio
            const audioBuffer = await audioContext.decodeAudioData(
              arrayBuffer.slice(0)
            );

            // Calculate sample positions for trimming
            const startSample = Math.floor(clipStartTime * sampleRate);
            const endSample = Math.floor(clipEndTime * sampleRate);
            const numSamples = endSample - startSample;

            if (numSamples > 0) {
              // Create audio data for encoding
              const audioData = new AudioData({
                format: "f32-planar",
                sampleRate: sampleRate,
                numberOfFrames: numSamples,
                numberOfChannels: numberOfChannels,
                timestamp: audioTimestamp,
                data: extractAudioSamples(
                  audioBuffer,
                  startSample,
                  numSamples,
                  numberOfChannels
                ),
              });

              audioEncoder.encode(audioData);
              audioData.close();

              console.log(
                `[Audio] Encoded ${numSamples} samples for clip ${clipIdx + 1}`
              );
            }
          } catch (audioError) {
            console.warn(
              `[Audio] Could not extract audio from clip ${clipIdx + 1}:`,
              audioError
            );
          }

          // Encode video frames
          statusEl.textContent = `Encoding video frames ${clipIdx + 1}/${
            scenesWithVideo.length
          }...`;

          for (let f = 0; f < totalFrames; f++) {
            const localTime = clipStartTime + f / targetFPS;
            if (localTime >= clipEndTime) break;

            // Seek to frame
            videoEl.currentTime = localTime;
            await new Promise((r) => {
              videoEl.onseeked = r;
              setTimeout(r, 100);
            });

            // Draw to canvas
            ctx.fillStyle = "#000";
            ctx.fillRect(0, 0, canvas.width, canvas.height);

            const videoAspect = videoEl.videoWidth / videoEl.videoHeight;
            const canvasAspect = canvas.width / canvas.height;
            let drawWidth, drawHeight, drawX, drawY;

            if (videoAspect > canvasAspect) {
              drawHeight = canvas.height;
              drawWidth = drawHeight * videoAspect;
              drawX = (canvas.width - drawWidth) / 2;
              drawY = 0;
            } else {
              drawWidth = canvas.width;
              drawHeight = drawWidth / videoAspect;
              drawX = 0;
              drawY = (canvas.height - drawHeight) / 2;
            }

            ctx.drawImage(videoEl, drawX, drawY, drawWidth, drawHeight);

            // Create VideoFrame
            const frame = new VideoFrame(canvas, {
              timestamp: videoTimestamp,
              duration: Math.floor(1_000_000 / targetFPS),
            });

            const isKeyframe = f % (targetFPS * 2) === 0;
            videoEncoder.encode(frame, { keyFrame: isKeyframe });
            frame.close();

            videoTimestamp += Math.floor(1_000_000 / targetFPS);

            // Update progress
            const clipProgress = f / totalFrames;
            const totalProgress =
              ((clipIdx + clipProgress) / scenesWithVideo.length) * 90;
            progressEl.textContent = `${Math.round(totalProgress)}%`;

            // Yield to UI
            if (f % 5 === 0) {
              await new Promise((r) => setTimeout(r, 0));
            }
          }

          // Update audio timestamp for next clip
          audioTimestamp += Math.floor(clipDuration * 1_000_000);

          // Cleanup video element
          videoEl.src = "";
        }

        // Finalize
        statusEl.textContent = "Finalizing MP4...";
        progressEl.textContent = "95%";

        await videoEncoder.flush();
        await audioEncoder.flush();
        videoEncoder.close();
        audioEncoder.close();
        await audioContext.close();
        muxer.finalize();

        // Get the MP4 buffer
        const mp4Buffer = muxer.target.buffer;
        const mp4Blob = new Blob([mp4Buffer], { type: "video/mp4" });

        // Download
        const url = URL.createObjectURL(mp4Blob);
        const link = document.createElement("a");
        link.href = url;
        link.download = `sceneforge-export-${Date.now()}.mp4`;
        document.body.appendChild(link);
        link.click();
        document.body.removeChild(link);
        URL.revokeObjectURL(url);

        console.log(
          `[NLE Export] MP4 Complete: ${(mp4Blob.size / 1024 / 1024).toFixed(
            2
          )} MB`
        );

        statusEl.textContent = `Export complete! (MP4, ${(
          mp4Blob.size /
          1024 /
          1024
        ).toFixed(1)}MB)`;
        progressEl.textContent = "100%";

        setTimeout(() => {
          overlay?.classList.add("hidden");
        }, 2000);
      }

      // Helper function to extract audio samples from AudioBuffer
      function extractAudioSamples(
        audioBuffer,
        startSample,
        numSamples,
        targetChannels
      ) {
        const sourceChannels = audioBuffer.numberOfChannels;
        const data = new Float32Array(numSamples * targetChannels);

        for (let ch = 0; ch < targetChannels; ch++) {
          const sourceChannel = ch < sourceChannels ? ch : 0; // Use first channel if mono
          const channelData = audioBuffer.getChannelData(sourceChannel);
          const offset = ch * numSamples;

          for (let i = 0; i < numSamples; i++) {
            const sampleIdx = startSample + i;
            if (sampleIdx < channelData.length) {
              data[offset + i] = channelData[sampleIdx];
            }
          }
        }

        return data;
      }

      // Fallback: Export using MediaRecorder (WebM)
      async function exportWithMediaRecorder(
        scenesWithVideo,
        canvas,
        ctx,
        targetFPS,
        totalDuration,
        statusEl,
        progressEl,
        overlay
      ) {
        const frameInterval = 1000 / targetFPS;

        // AudioContext for mixing audio from all clips
        let audioContext = new (window.AudioContext ||
          window.webkitAudioContext)();
        let audioDestination = audioContext.createMediaStreamDestination();

        // Get video stream from canvas
        const videoStream = canvas.captureStream(targetFPS);

        // Combine video and audio streams
        const combinedStream = new MediaStream([
          ...videoStream.getVideoTracks(),
          ...audioDestination.stream.getAudioTracks(),
        ]);

        // Set up MediaRecorder with WebM
        let mediaRecorder;
        const codecOptions = [
          {
            mimeType: "video/webm;codecs=vp9,opus",
            videoBitsPerSecond: 15000000,
          },
          {
            mimeType: "video/webm;codecs=vp8,opus",
            videoBitsPerSecond: 12000000,
          },
          { mimeType: "video/webm", videoBitsPerSecond: 10000000 },
        ];

        for (const options of codecOptions) {
          try {
            if (MediaRecorder.isTypeSupported(options.mimeType)) {
              mediaRecorder = new MediaRecorder(combinedStream, {
                mimeType: options.mimeType,
                videoBitsPerSecond: options.videoBitsPerSecond,
                audioBitsPerSecond: 192000,
              });
              console.log(`[NLE Export] Recording codec: ${options.mimeType}`);
              break;
            }
          } catch (e) {
            console.log(
              `[NLE Export] Codec not supported: ${options.mimeType}`
            );
          }
        }

        if (!mediaRecorder) {
          throw new Error("No supported video codec found");
        }

        const chunks = [];
        mediaRecorder.ondataavailable = (e) => {
          if (e.data.size > 0) chunks.push(e.data);
        };

        mediaRecorder.start(100);

        // Process each clip
        for (let clipIdx = 0; clipIdx < scenesWithVideo.length; clipIdx++) {
          const scene = scenesWithVideo[clipIdx];
          const clipProgress = (clipIdx / scenesWithVideo.length) * 100;

          statusEl.textContent = `Rendering clip ${clipIdx + 1}/${
            scenesWithVideo.length
          }...`;
          progressEl.textContent = `${Math.round(clipProgress)}%`;

          await renderClipToCanvasWithAudio(
            scene,
            ctx,
            canvas,
            targetFPS,
            frameInterval,
            audioContext,
            audioDestination
          );
        }

        // Finalize
        statusEl.textContent = "Encoding...";
        progressEl.textContent = "90%";

        await new Promise((resolve, reject) => {
          mediaRecorder.onstop = resolve;
          mediaRecorder.onerror = reject;
          mediaRecorder.stop();
        });

        if (audioContext && audioContext.state !== "closed") {
          await audioContext.close();
        }

        // Create blob
        const webmBlob = new Blob(chunks, { type: "video/webm" });

        // Download as WebM (fallback)
        const url = URL.createObjectURL(webmBlob);
        const link = document.createElement("a");
        link.href = url;
        link.download = `sceneforge-export-${Date.now()}.webm`;
        document.body.appendChild(link);
        link.click();
        document.body.removeChild(link);
        URL.revokeObjectURL(url);

        console.log(
          `[NLE Export] WebM Complete: ${(webmBlob.size / 1024 / 1024).toFixed(
            2
          )} MB`
        );

        statusEl.textContent = `Export complete! (WebM, ${(
          webmBlob.size /
          1024 /
          1024
        ).toFixed(1)}MB) - Note: Use a video converter for MP4`;
        progressEl.textContent = "100%";

        setTimeout(() => {
          overlay?.classList.add("hidden");
        }, 2000);
      }

      /**
       * Render a single clip to canvas with audio routing
       * Plays the video, draws frames to canvas, and routes audio to the mixer
       */
      async function renderClipToCanvasWithAudio(
        scene,
        ctx,
        canvas,
        targetFPS,
        frameInterval,
        audioContext,
        audioDestination
      ) {
        return new Promise((resolve, reject) => {
          const videoEl = document.createElement("video");
          videoEl.muted = false; // Keep audio enabled
          videoEl.playsInline = true;
          videoEl.preload = "auto";
          videoEl.crossOrigin = "anonymous"; // Required for audio routing

          let audioSource = null;
          let gainNode = null;

          videoEl.onloadedmetadata = async () => {
            const startTime = scene.trim.in * videoEl.duration;
            const endTime = scene.trim.out * videoEl.duration;

            // Set up audio routing
            try {
              audioSource = audioContext.createMediaElementSource(videoEl);
              gainNode = audioContext.createGain();
              gainNode.gain.value = 1.0; // Full volume

              // Route: video -> gain -> destination (for recording) + speakers
              audioSource.connect(gainNode);
              gainNode.connect(audioDestination);
              // Also connect to speakers so we can hear during export (optional)
              // gainNode.connect(audioContext.destination);

              console.log("[NLE Export] Audio routing connected for clip");
            } catch (audioErr) {
              console.warn(
                "[NLE Export] Could not route audio:",
                audioErr.message
              );
              // Continue without audio for this clip
            }

            videoEl.currentTime = startTime;

            // Wait for video to be ready at start position
            await new Promise((r) => {
              const onSeeked = () => {
                videoEl.removeEventListener("seeked", onSeeked);
                setTimeout(r, 100); // Small delay for stability
              };
              videoEl.addEventListener("seeked", onSeeked);
              setTimeout(r, 500); // Fallback
            });

            let animationId = null;
            let lastFrameTime = 0;
            const minFrameInterval = 1000 / targetFPS;

            // Use real-time playback with requestAnimationFrame
            const captureLoop = (timestamp) => {
              if (videoEl.currentTime >= endTime || videoEl.ended) {
                videoEl.pause();
                if (animationId) cancelAnimationFrame(animationId);

                // Disconnect audio
                if (gainNode) {
                  try {
                    gainNode.disconnect();
                  } catch (e) {}
                }
                if (audioSource) {
                  try {
                    audioSource.disconnect();
                  } catch (e) {}
                }

                resolve();
                return;
              }

              // Throttle to target FPS
              if (timestamp - lastFrameTime >= minFrameInterval) {
                // Draw current frame to canvas
                ctx.drawImage(videoEl, 0, 0, canvas.width, canvas.height);
                lastFrameTime = timestamp;
              }

              animationId = requestAnimationFrame(captureLoop);
            };

            // Start playback
            videoEl
              .play()
              .then(() => {
                animationId = requestAnimationFrame(captureLoop);
              })
              .catch((err) => {
                console.error("Video play error:", err);
                // Fallback: try muted playback (no audio)
                videoEl.muted = true;
                videoEl
                  .play()
                  .then(() => {
                    animationId = requestAnimationFrame(captureLoop);
                  })
                  .catch(reject);
              });

            // Safety timeout - don't let it run forever
            const expectedDuration = (endTime - startTime) * 1000;
            setTimeout(() => {
              if (animationId) {
                cancelAnimationFrame(animationId);
                videoEl.pause();
                if (gainNode)
                  try {
                    gainNode.disconnect();
                  } catch (e) {}
                if (audioSource)
                  try {
                    audioSource.disconnect();
                  } catch (e) {}
                resolve();
              }
            }, expectedDuration + 2000); // Extra 2s buffer
          };

          videoEl.onerror = (e) => {
            console.error("Video load error:", e);
            reject(new Error("Failed to load video clip"));
          };

          videoEl.src = scene.generatedVideo;
          videoEl.load();
        });
      }

      async function exportIndividualClips() {
        const scenesWithVideo = state.scenes
          .filter((s) => s.generatedVideo)
          .sort((a, b) => a.order - b.order);

        for (let i = 0; i < scenesWithVideo.length; i++) {
          const scene = scenesWithVideo[i];
          try {
            const response = await fetch(scene.generatedVideo);
            const blob = await response.blob();
            const url = URL.createObjectURL(blob);
            const link = document.createElement("a");
            link.href = url;
            link.download = `sceneforge-${String(i + 1).padStart(
              2,
              "0"
            )}-clip.mp4`;
            document.body.appendChild(link);
            link.click();
            document.body.removeChild(link);
            URL.revokeObjectURL(url);
            await new Promise((r) => setTimeout(r, 500));
          } catch (e) {
            console.error("Download failed for clip", i + 1, e);
          }
        }
      }

      window.exportVideo = exportIndividualClips;

      // ============================================
      // EVENT HANDLERS
      // ============================================
      function attachEventListeners() {
        const dropzone = document.getElementById("dropzone");
        const videoInput = document.getElementById("video-input");

        if (dropzone && videoInput) {
          console.log("Attaching dropzone listeners");

          dropzone.onclick = (e) => {
            console.log("Dropzone clicked");
            e.preventDefault();
            e.stopPropagation();

            if (!state.apiKeys.gemini) {
              alert(
                'Please add your Gemini API key first!\n\nClick "API Settings" at the bottom of the page.'
              );
              return;
            }

            videoInput.click();
          };

          dropzone.ondragover = (e) => {
            e.preventDefault();
            e.stopPropagation();
            dropzone.classList.add("border-violet-500", "bg-violet-500/10");
          };

          dropzone.ondragleave = (e) => {
            e.preventDefault();
            dropzone.classList.remove("border-violet-500", "bg-violet-500/10");
          };

          dropzone.ondrop = (e) => {
            e.preventDefault();
            e.stopPropagation();
            console.log("File dropped");
            dropzone.classList.remove("border-violet-500", "bg-violet-500/10");

            if (!state.apiKeys.gemini) {
              alert(
                'Please add your Gemini API key first!\n\nClick "API Settings" at the bottom of the page.'
              );
              return;
            }

            const file = e.dataTransfer.files[0];
            console.log("Dropped file:", file?.name, file?.type);
            if (file && file.type.startsWith("video/")) {
              console.log("Processing video...");
              processVideo(file);
            } else {
              console.log("Not a video file");
              alert("Please drop a video file (MP4, MOV, WebM)");
            }
          };

          videoInput.onchange = (e) => {
            const file = e.target.files[0];
            console.log("File selected:", file?.name);
            if (file) processVideo(file);
          };
        } else {
          console.log("Dropzone or video input not found");
        }

        // Reference image toggle for character consistency
        const useReferenceImage = document.getElementById(
          "use-reference-image"
        );
        if (useReferenceImage) {
          useReferenceImage.onchange = (e) =>
            setState({ useReferenceImage: e.target.checked });
        }

        // Consistent character toggle
        const consistentCharacter = document.getElementById(
          "consistent-character"
        );
        if (consistentCharacter) {
          consistentCharacter.onchange = (e) => {
            setState({ consistentCharacter: e.target.checked });
            if (!e.target.checked) {
              setState({ referenceSceneId: null });
            }
          };
        }

        // Initialize custom scrollbar for scene timeline
        initCustomScrollbar();

        // Pipeline screen - proceed to editor button
        if (state.step === 1) {
          const proceedBtn = document.getElementById("proceed-to-editor-btn");
          if (proceedBtn) {
            proceedBtn.onclick = (e) => {
              e.preventDefault();
              e.stopPropagation();
              console.log("[Nav] Proceed to Editor clicked");
              // Initialize history with current state when entering editor
              historyStack.length = 0;
              historyIndex = -1;
              pushHistory();
              state.step = 2;
              render();
            };
          }
        }

        // Editor keyboard shortcuts (only when in editor view)
        if (state.step === 2) {
          document.onkeydown = (e) => {
            // Don't trigger if typing in an input
            if (e.target.tagName === "INPUT" || e.target.tagName === "TEXTAREA")
              return;

            const player = document.getElementById("preview-player");

            switch (e.code) {
              case "Space":
                e.preventDefault();
                editorPlayPause();
                break;
              case "KeyA": // Selection tool (DaVinci Resolve)
                e.preventDefault();
                setActiveTool("select");
                break;
              case "KeyB": // Blade tool or split at playhead
                e.preventDefault();
                if (e.ctrlKey || e.metaKey) {
                  // Ctrl+B / Cmd+B: Split at playhead (DaVinci Resolve)
                  splitAtPlayhead();
                } else {
                  // B: Toggle blade tool
                  setActiveTool(
                    state.activeTool === "blade" ? "select" : "blade"
                  );
                }
                break;
              case "KeyD": // Duplicate clip
                if (e.ctrlKey || e.metaKey) {
                  e.preventDefault();
                  duplicateSelectedClip();
                }
                break;
              case "KeyJ": // Rewind / play backward (shuttle left)
                e.preventDefault();
                if (player) {
                  player.currentTime = Math.max(0, player.currentTime - 5);
                  updatePlayhead();
                  updateTimecodeDisplay();
                }
                break;
              case "KeyK": // Pause (shuttle stop)
                e.preventDefault();
                if (player) player.pause();
                break;
              case "KeyL": // Fast forward (shuttle right)
                e.preventDefault();
                if (player) {
                  player.currentTime = Math.min(
                    player.duration,
                    player.currentTime + 5
                  );
                  updatePlayhead();
                  updateTimecodeDisplay();
                }
                break;
              case "KeyI": // Mark In point
                e.preventDefault();
                markInPoint();
                break;
              case "KeyO": // Mark Out point
                e.preventDefault();
                markOutPoint();
                break;
              case "ArrowLeft":
                e.preventDefault();
                if (e.shiftKey) {
                  // Shift+Left: back 1 second
                  if (player)
                    player.currentTime = Math.max(0, player.currentTime - 1);
                } else {
                  // Left: back 1 frame
                  stepBackward(1);
                }
                updatePlayhead();
                break;
              case "ArrowRight":
                e.preventDefault();
                if (e.shiftKey) {
                  // Shift+Right: forward 1 second
                  if (player)
                    player.currentTime = Math.min(
                      player.duration,
                      player.currentTime + 1
                    );
                } else {
                  // Right: forward 1 frame
                  stepForward(1);
                }
                updatePlayhead();
                break;
              case "ArrowUp":
                e.preventDefault();
                previousClip();
                break;
              case "ArrowDown":
                e.preventDefault();
                nextClip();
                break;
              case "Delete":
              case "Backspace":
                e.preventDefault();
                deleteSelectedClip();
                break;
              case "Home":
                e.preventDefault();
                jumpToStart();
                break;
              case "End":
                e.preventDefault();
                jumpToEnd();
                break;
              case "Escape":
                // Exit blade mode
                if (state.activeTool === "blade") {
                  setActiveTool("select");
                }
                break;
              case "Equal":
              case "NumpadAdd":
                // Zoom in timeline
                if (e.ctrlKey || e.metaKey) {
                  e.preventDefault();
                  zoomTimeline(1);
                }
                break;
              case "Minus":
              case "NumpadSubtract":
                // Zoom out timeline
                if (e.ctrlKey || e.metaKey) {
                  e.preventDefault();
                  zoomTimeline(-1);
                }
                break;
              case "Digit0":
              case "Numpad0":
                // Fit to view
                if (e.ctrlKey || e.metaKey) {
                  e.preventDefault();
                  fitTimelineToView();
                }
                break;
              case "KeyZ":
                // Undo/Redo
                if (e.ctrlKey || e.metaKey) {
                  e.preventDefault();
                  if (e.shiftKey) {
                    // Ctrl/Cmd + Shift + Z: Redo
                    redo();
                  } else {
                    // Ctrl/Cmd + Z: Undo
                    undo();
                  }
                }
                break;
              case "KeyY":
                // Alternative Redo (Ctrl+Y like in some apps)
                if (e.ctrlKey || e.metaKey) {
                  e.preventDefault();
                  redo();
                }
                break;
            }
          };
        } else {
          document.onkeydown = null;
        }
      }

      /**
       * Mark In point for current clip
       */
      window.markInPoint = () => {
        const player = document.getElementById("preview-player");
        if (!player || !player.duration) return;

        const scenesWithVideo = state.scenes
          .filter((s) => s.generatedVideo)
          .sort((a, b) => a.order - b.order);
        const currentScene = scenesWithVideo[state.selectedScene];
        if (!currentScene) return;

        const newInPoint = player.currentTime / player.duration;
        if (newInPoint < currentScene.trim.out - 0.05) {
          const scenes = state.scenes.map((s) =>
            s.id === currentScene.id
              ? { ...s, trim: { ...s.trim, in: newInPoint } }
              : s
          );
          setState({ scenes });
          console.log(`[Editor] Mark In: ${(newInPoint * 100).toFixed(1)}%`);
        }
      };

      /**
       * Mark Out point for current clip
       */
      window.markOutPoint = () => {
        const player = document.getElementById("preview-player");
        if (!player || !player.duration) return;

        const scenesWithVideo = state.scenes
          .filter((s) => s.generatedVideo)
          .sort((a, b) => a.order - b.order);
        const currentScene = scenesWithVideo[state.selectedScene];
        if (!currentScene) return;

        const newOutPoint = player.currentTime / player.duration;
        if (newOutPoint > currentScene.trim.in + 0.05) {
          const scenes = state.scenes.map((s) =>
            s.id === currentScene.id
              ? { ...s, trim: { ...s.trim, out: newOutPoint } }
              : s
          );
          setState({ scenes });
          console.log(`[Editor] Mark Out: ${(newOutPoint * 100).toFixed(1)}%`);
        }
      };

      // Custom scrollbar sync with scene container
      function initCustomScrollbar() {
        const container = document.getElementById("scene-container");
        const track = document.getElementById("scrollbar-track");
        const thumb = document.getElementById("scrollbar-thumb");

        console.log("initCustomScrollbar called, elements found:", {
          container: !!container,
          track: !!track,
          thumb: !!thumb,
        });

        if (!container || !track || !thumb) {
          console.log("Scrollbar elements not found, skipping init");
          return;
        }

        let isDragging = false;
        let startX = 0;
        let startScrollLeft = 0;

        // Update thumb size and position based on container scroll
        const updateThumb = () => {
          const scrollWidth = container.scrollWidth;
          const clientWidth = container.clientWidth;
          const scrollLeft = container.scrollLeft;

          console.log("updateThumb:", { scrollWidth, clientWidth, scrollLeft });

          if (scrollWidth <= clientWidth) {
            // No scroll needed, show full width thumb
            thumb.style.width = "100%";
            thumb.style.left = "0%";
            return;
          }

          // Calculate thumb width as percentage (min 15%)
          const thumbWidth = Math.max(15, (clientWidth / scrollWidth) * 100);
          thumb.style.width = thumbWidth + "%";

          // Calculate thumb position
          const maxScroll = scrollWidth - clientWidth;
          const scrollPercent = scrollLeft / maxScroll;
          const maxThumbLeft = 100 - thumbWidth;
          thumb.style.left = scrollPercent * maxThumbLeft + "%";

          console.log("Thumb updated:", { thumbWidth, left: thumb.style.left });
        };

        // Sync scroll from container to thumb
        container.addEventListener("scroll", updateThumb);

        // Handle thumb drag
        thumb.addEventListener("mousedown", (e) => {
          isDragging = true;
          startX = e.clientX;
          startScrollLeft = container.scrollLeft;
          thumb.style.cursor = "grabbing";
          document.body.style.userSelect = "none";
          e.preventDefault();
        });

        document.addEventListener("mousemove", (e) => {
          if (!isDragging) return;

          const trackRect = track.getBoundingClientRect();
          const scrollWidth = container.scrollWidth;
          const clientWidth = container.clientWidth;
          const maxScroll = scrollWidth - clientWidth;

          // Calculate how much the mouse moved as a percentage of track
          const deltaX = e.clientX - startX;
          const trackWidth = trackRect.width;
          const thumbWidth = thumb.offsetWidth;
          const maxThumbMove = trackWidth - thumbWidth;

          // Convert mouse delta to scroll delta
          const scrollDelta = (deltaX / maxThumbMove) * maxScroll;
          container.scrollLeft = startScrollLeft + scrollDelta;
        });

        document.addEventListener("mouseup", () => {
          if (isDragging) {
            isDragging = false;
            thumb.style.cursor = "grab";
            document.body.style.userSelect = "";
          }
        });

        // Handle click on track to jump
        track.addEventListener("click", (e) => {
          if (e.target === thumb) return;

          const trackRect = track.getBoundingClientRect();
          const clickX = e.clientX - trackRect.left;
          const trackWidth = trackRect.width;
          const clickPercent = clickX / trackWidth;

          const scrollWidth = container.scrollWidth;
          const clientWidth = container.clientWidth;
          const maxScroll = scrollWidth - clientWidth;

          container.scrollTo({
            left: clickPercent * maxScroll,
            behavior: "smooth",
          });
        });

        // Initial update
        updateThumb();

        // Update on window resize
        window.addEventListener("resize", updateThumb);
      }

      // Set a scene as the character reference
      window.setCharacterReference = (sceneId) => {
        const scene = state.scenes.find((s) => s.id === sceneId);
        if (!scene) return;

        if (!scene.generatedImage) {
          alert(
            "Generate an image for this scene first to use it as character reference."
          );
          return;
        }

        // Toggle: if already selected, deselect; otherwise select
        if (state.referenceSceneId === sceneId) {
          setState({ referenceSceneId: null });
        } else {
          setState({ referenceSceneId: sceneId });
        }
      };

      // ============================================
      // EDIT PROMPT MODAL FUNCTIONS
      // ============================================

      let currentEditingSceneId = null;

      // Open the edit prompt modal for a scene
      window.openEditPromptModal = (sceneId) => {
        const scene = state.scenes.find((s) => s.id === sceneId);
        if (!scene) return;

        currentEditingSceneId = sceneId;
        const sceneIndex = state.scenes.findIndex((s) => s.id === sceneId);

        // Update modal label
        document.getElementById(
          "edit-prompt-scene-label"
        ).textContent = `Scene ${sceneIndex + 1} of ${state.scenes.length}`;

        // Populate fields with current prompt values
        const imagePromptText =
          scene.imagePrompt?.image_prompt ||
          scene.imagePrompt?.positive ||
          (typeof scene.imagePrompt === "string" ? scene.imagePrompt : "");
        const negativePromptText =
          scene.imagePrompt?.negative_prompt ||
          scene.imagePrompt?.negative ||
          "";
        const motionText =
          scene.motionPrompt?.motion ||
          (typeof scene.motionPrompt === "string" ? scene.motionPrompt : "");
        // Get speech from motionPrompt first, then fall back to segmentationData
        const speechText =
          scene.motionPrompt?.speech ||
          scene.segmentationData?.dialogueTranscript ||
          scene.segmentationData?.dialogueIntent ||
          "";
        // Clean up [no dialogue] placeholder
        const cleanSpeechText =
          speechText === "[no dialogue]" ? "" : speechText;

        document.getElementById("edit-image-prompt").value = imagePromptText;
        document.getElementById("edit-negative-prompt").value =
          negativePromptText;
        document.getElementById("edit-motion-prompt").value = motionText;
        document.getElementById("edit-speech-prompt").value = cleanSpeechText;

        // Show modal
        document.getElementById("edit-prompt-modal").classList.remove("hidden");

        // Focus on image prompt field
        setTimeout(() => {
          document.getElementById("edit-image-prompt").focus();
        }, 100);
      };

      // Close the edit prompt modal
      window.closeEditPromptModal = () => {
        document.getElementById("edit-prompt-modal").classList.add("hidden");
        currentEditingSceneId = null;
      };

      // Save the edited prompts
      window.saveEditedPrompts = () => {
        if (!currentEditingSceneId) return;

        const scene = state.scenes.find((s) => s.id === currentEditingSceneId);
        if (!scene) return;

        const newImagePrompt = document
          .getElementById("edit-image-prompt")
          .value.trim();
        const newNegativePrompt = document
          .getElementById("edit-negative-prompt")
          .value.trim();
        const newMotionPrompt = document
          .getElementById("edit-motion-prompt")
          .value.trim();
        const newSpeechPrompt = document
          .getElementById("edit-speech-prompt")
          .value.trim();

        // Build updated prompt objects, preserving structure
        const updatedImagePrompt = {
          ...(typeof scene.imagePrompt === "object" ? scene.imagePrompt : {}),
          image_prompt: newImagePrompt,
          positive: newImagePrompt,
          negative_prompt: newNegativePrompt,
          negative: newNegativePrompt,
        };

        const updatedMotionPrompt = {
          ...(typeof scene.motionPrompt === "object" ? scene.motionPrompt : {}),
          motion: newMotionPrompt,
          speech: newSpeechPrompt,
        };

        // Also update segmentationData to reflect speech changes on card
        const updatedSegmentationData = {
          ...(scene.segmentationData || {}),
          dialogueTranscript: newSpeechPrompt || "[no dialogue]",
          dialogueIntent: newSpeechPrompt || "[no dialogue]",
        };

        // Update the scene
        updateScene(currentEditingSceneId, {
          imagePrompt: updatedImagePrompt,
          motionPrompt: updatedMotionPrompt,
          segmentationData: updatedSegmentationData,
        });

        console.log(
          `[EditPrompt] Updated prompts for scene ${currentEditingSceneId}`
        );

        // Close modal
        closeEditPromptModal();

        // Re-render to show updated prompts
        render();
      };

      // Close modal on Escape key
      document.addEventListener("keydown", (e) => {
        if (
          e.key === "Escape" &&
          !document
            .getElementById("edit-prompt-modal")
            .classList.contains("hidden")
        ) {
          closeEditPromptModal();
        }
      });

      // Regenerate the image prompt for a scene
      window.regeneratePrompt = async (sceneId) => {
        const scene = state.scenes.find((s) => s.id === sceneId);
        if (!scene) return;

        if (!state.apiKeys.gemini) {
          alert("Please set your Gemini API key first");
          return;
        }

        try {
          updateScene(sceneId, { status: "generating_image" });
          setState({ processingStatus: "Regenerating prompt..." });

          const imagePromptData = await generateImagePrompt(
            scene,
            state.creativeDirection,
            state.repurposeLevel
          );

          updateScene(sceneId, {
            imagePrompt: imagePromptData,
            status: "pending",
          });

          setState({ processingStatus: "" });
          console.log("Prompt regenerated for", sceneId);
        } catch (error) {
          console.error("Error regenerating prompt:", error);
          updateScene(sceneId, { status: "pending" });
          setState({ processingStatus: "" });
          alert("Failed to regenerate prompt: " + error.message);
        }
      };

      // Sync all scene prompts to match reference scene's environment, lighting, clothing
      window.syncPromptsToReference = async () => {
        if (!state.consistentCharacter || !state.referenceSceneId) {
          alert(
            "Please enable Consistent Character and select a reference scene first."
          );
          return;
        }

        const refScene = state.scenes.find(
          (s) => s.id === state.referenceSceneId
        );
        if (!refScene || !refScene.generatedImage) {
          alert("Reference scene must have a generated image.");
          return;
        }

        const refPrompt = refScene.imagePrompt?.image_prompt || "";
        if (!refPrompt) {
          alert("Reference scene has no image prompt.");
          return;
        }

        const apiKey = state.apiKeys.gemini;
        if (!apiKey) {
          alert("Please set your Gemini API key first");
          return;
        }

        setState({
          isGenerating: true,
          processingStatus: "Analyzing reference scene style...",
        });

        try {
          // Extract style elements from reference prompt using Gemini
          const styleAnalysisPrompt = `Analyze this image generation prompt and extract ONLY the style elements. Return a JSON object with these fields:
- environment: The setting/location description
- lighting: The lighting setup and quality  
- clothing: What the person is wearing
- color_palette: The overall color scheme
- mood: The visual mood/atmosphere

Prompt to analyze:
"${refPrompt}"

Return ONLY valid JSON, no markdown:
{"environment":"...","lighting":"...","clothing":"...","color_palette":"...","mood":"..."}`;

          const styleResponse = await fetch(
            `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`,
            {
              method: "POST",
              headers: { "Content-Type": "application/json" },
              body: JSON.stringify({
                contents: [{ parts: [{ text: styleAnalysisPrompt }] }],
                generationConfig: { temperature: 0.3, maxOutputTokens: 500 },
              }),
            }
          );

          const styleData = await styleResponse.json();
          const styleText =
            styleData.candidates?.[0]?.content?.parts?.[0]?.text || "";
          const styleElements = safeParseJSON(styleText, "style analysis");

          console.log("Extracted style elements:", styleElements);

          // Now update each non-reference scene's prompt
          const scenesToUpdate = state.scenes.filter(
            (s) => s.id !== state.referenceSceneId
          );

          for (let i = 0; i < scenesToUpdate.length; i++) {
            const scene = scenesToUpdate[i];
            setState({
              processingStatus: `Syncing scene ${i + 1}/${
                scenesToUpdate.length
              }...`,
            });

            const currentPrompt = scene.imagePrompt?.image_prompt || "";

            // Use Gemini to rewrite the prompt with reference style
            const rewritePrompt = `Rewrite this image prompt to match a consistent visual style.

ORIGINAL PROMPT:
"${currentPrompt}"

STYLE TO MATCH (from reference scene):
- Environment: ${styleElements.environment || "keep original"}
- Lighting: ${styleElements.lighting || "keep original"}
- Clothing: ${styleElements.clothing || "keep original"}
- Color palette: ${styleElements.color_palette || "keep original"}
- Mood: ${styleElements.mood || "keep original"}

RULES:
1. Keep the SAME action/pose from the original prompt
2. Apply the reference style's environment, lighting, clothing, colors
3. Maintain photorealistic quality
4. Use single quotes inside the prompt, no double quotes

Return ONLY valid JSON:
{"image_prompt":"rewritten prompt here","negative_prompt":"text, watermark, blurry, low quality, distorted"}`;

            const rewriteResponse = await fetch(
              `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`,
              {
                method: "POST",
                headers: { "Content-Type": "application/json" },
                body: JSON.stringify({
                  contents: [{ parts: [{ text: rewritePrompt }] }],
                  generationConfig: { temperature: 0.5, maxOutputTokens: 1000 },
                }),
              }
            );

            const rewriteData = await rewriteResponse.json();
            const rewriteText =
              rewriteData.candidates?.[0]?.content?.parts?.[0]?.text || "";
            const newPrompt = safeParseJSON(rewriteText, "prompt rewrite");

            updateScene(scene.id, {
              imagePrompt: newPrompt,
              generatedImage: null, // Clear old image since prompt changed
              status: "pending",
            });

            console.log("Synced prompt for", scene.id);
          }

          setState({ isGenerating: false, processingStatus: "" });
          alert(
            `Synced ${scenesToUpdate.length} scene prompts to match reference style. Click "Generate All Images" to regenerate.`
          );
        } catch (error) {
          console.error("Error syncing prompts:", error);
          setState({ isGenerating: false, processingStatus: "" });
          alert("Failed to sync prompts: " + error.message);
        }
      };

      // Global functions for onclick handlers
      window.openSettings = () => setState({ showSettings: true });
      window.closeSettings = () => setState({ showSettings: false });
      window.saveSettings = () => {
        const gemini = document.getElementById("gemini-key").value;
        const kieai = document.getElementById("kieai-key").value;
        const videoModel = document.getElementById("video-model").value;

        localStorage.setItem("gemini_api_key", gemini);
        localStorage.setItem("kieai_api_key", kieai);
        localStorage.setItem("video_model", videoModel);

        setState({
          apiKeys: { gemini, kieai },
          videoModel: videoModel,
          showSettings: false,
        });
      };

      window.generateImage = (sceneId) => generateSceneImage(sceneId);
      window.generateVideo = (sceneId) => generateSceneVideo(sceneId);

      window.generateAllImages = async () => {
        setState({ isGenerating: true });

        // If consistent character is enabled, generate the reference scene first
        if (state.consistentCharacter && state.referenceSceneId) {
          const refScene = state.scenes.find(
            (s) => s.id === state.referenceSceneId
          );
          if (refScene && !refScene.generatedImage) {
            console.log(
              "Generating reference scene first:",
              state.referenceSceneId
            );
            await generateSceneImage(state.referenceSceneId);
          }
        }

        // Generate all other scenes
        for (const scene of state.scenes) {
          if (!scene.generatedImage) {
            await generateSceneImage(scene.id);
          }
        }
        setState({ isGenerating: false });
      };

      window.generateAllVideos = async () => {
        setState({ isGenerating: true });
        for (const scene of state.scenes) {
          if (scene.generatedImage && !scene.generatedVideo) {
            await generateSceneVideo(scene.id);
          }
        }
        setState({ isGenerating: false });
      };

      window.proceedToEditor = () => {
        console.log("[Nav] Proceeding to editor");
        // Initialize history with current state when entering editor
        historyStack.length = 0;
        historyIndex = -1;
        pushHistory();
        setState({ step: 2 });
      };

      window.backToPipeline = () => {
        console.log("[Nav] Going back to pipeline");
        setState({ step: 1 });
      };

      window.goToEditor = () => {
        console.log("[Nav] Going to editor");
        setState({ step: 2 });
      };

      /**
       * Open fullscreen image preview
       */
      window.openFullscreenImage = (imageUrl) => {
        if (!imageUrl) return;
        console.log("[Fullscreen] Opening image preview");
        setState({ fullscreenImage: imageUrl });
        // Add ESC key listener
        document.addEventListener("keydown", handleFullscreenEsc);
      };

      /**
       * Close fullscreen image preview
       */
      window.closeFullscreenImage = () => {
        console.log("[Fullscreen] Closing image preview");
        setState({ fullscreenImage: null });
        // Remove ESC key listener
        document.removeEventListener("keydown", handleFullscreenEsc);
      };

      /**
       * Handle ESC key for fullscreen modal
       */
      function handleFullscreenEsc(e) {
        if (e.key === "Escape" && state.fullscreenImage) {
          closeFullscreenImage();
        }
      }

      window.deleteScene = (sceneId) => {
        if (!confirm("Delete this scene? This action cannot be undone."))
          return;

        const updatedScenes = state.scenes.filter((s) => s.id !== sceneId);
        setState({
          scenes: updatedScenes,
          selectedScene: Math.min(
            state.selectedScene,
            Math.max(0, updatedScenes.length - 1)
          ),
        });
      };

      window.copyPrompt = (sceneId, type) => {
        const scene = state.scenes.find((s) => s.id === sceneId);
        if (!scene) return;

        const text =
          type === "image"
            ? scene.imagePrompt.image_prompt || scene.imagePrompt.positive
            : `MOTION: ${scene.motionPrompt.motion}\nSPEECH: ${scene.motionPrompt.speech}`;

        navigator.clipboard.writeText(text);
      };

      window.copyFullPrompt = (sceneId, type) => {
        const scene = state.scenes.find((s) => s.id === sceneId);
        if (!scene) return;

        let text;
        if (type === "image") {
          const imagePrompt =
            scene.imagePrompt?.image_prompt ||
            scene.imagePrompt?.positive ||
            "";
          const negativePrompt =
            scene.imagePrompt?.negative_prompt ||
            scene.imagePrompt?.negative ||
            "";
          text = `IMAGE PROMPT:\n${imagePrompt}`;
          if (negativePrompt) {
            text += `\n\nNEGATIVE PROMPT:\n${negativePrompt}`;
          }
        } else {
          const motion = scene.motionPrompt?.motion || "";
          const speech = scene.motionPrompt?.speech || "[none]";
          text = `MOTION PROMPT:\n${motion}\n\nSPEECH/DIALOGUE:\n${speech}`;
        }

        navigator.clipboard.writeText(text).then(() => {
          alert("Prompt copied to clipboard!");
        });
      };

      window.uploadCustomImage = (sceneId, event) => {
        const file = event.target.files[0];
        if (!file) return;

        console.log(
          "[Upload] Custom image for scene:",
          sceneId,
          "File:",
          file.name,
          file.type,
          file.size
        );

        const reader = new FileReader();
        reader.onload = (e) => {
          const base64 = e.target.result;
          console.log("[Upload] Image loaded, base64 length:", base64.length);

          updateScene(sceneId, {
            generatedImage: base64,
            status: "image_done",
            wasClean: false, // Reset clean flag
            originalGeneratedImage: null, // Clear original reference
            error: null,
          });
          console.log("[Upload] Custom image uploaded for scene:", sceneId);
        };
        reader.onerror = (e) => {
          console.error("[Upload] Error reading file:", e);
          alert("Error reading image file");
        };
        reader.readAsDataURL(file);
      };

      // Upload the user's product image
      window.uploadProductImage = (event) => {
        const file = event.target.files[0];
        if (!file) return;

        const reader = new FileReader();
        reader.onload = (e) => {
          setState({ productImage: e.target.result });
          console.log("Product image uploaded");
        };
        reader.readAsDataURL(file);
      };

      window.removeProductImage = () => {
        setState({ productImage: null });
      };

      // Swap product in a scene using Gemini image editing
      window.swapProduct = async (sceneId) => {
        const scene = state.scenes.find((s) => s.id === sceneId);
        if (!scene || !scene.generatedImage) {
          alert("Generate an image first before swapping product");
          return;
        }
        if (!state.productImage) {
          alert(
            'Upload your product image first (use the "My Product" upload above)'
          );
          return;
        }

        try {
          setState({ isGenerating: true });
          updateScene(sceneId, { status: "swapping_product", error: null });

          const sceneDescription =
            scene.segmentationData?.visualSummary || "product scene";
          const swappedImage = await swapProductInImage(
            scene.generatedImage,
            state.productImage,
            sceneDescription
          );

          updateScene(sceneId, {
            generatedImage: swappedImage,
            status: "image_done",
          });

          setState({ isGenerating: false });
        } catch (error) {
          console.error("Product swap error:", error);
          updateScene(sceneId, {
            status: "error",
            error: error.message,
          });
          setState({ isGenerating: false });
        }
      };

      // Swap product in all scenes that have generated images
      window.swapAllProducts = async () => {
        if (!state.productImage) {
          alert("Upload your product image first");
          return;
        }

        setState({ isGenerating: true });
        for (const scene of state.scenes) {
          if (scene.generatedImage && scene.status !== "swapping_product") {
            await window.swapProduct(scene.id);
          }
        }
        setState({ isGenerating: false });
      };

      // ============================================
      // GOOGLE AUTH
      // ============================================
      const GoogleAuth = {
        // IMPORTANTE: Substitua pelo seu Client ID do Google Cloud Console
        // Crie em: https://console.cloud.google.com/apis/credentials
        CLIENT_ID:
          "691701024698-0e20aul7n7h1fe2nb55gsga07s4qe8r0.apps.googleusercontent.com",

        init() {
          // Verifica se já existe sessão salva
          const savedUser = localStorage.getItem("sceneforge_user");
          if (savedUser) {
            try {
              const user = JSON.parse(savedUser);
              setState({ isAuthenticated: true, user: user });
              console.log("[Auth] User restored from session:", user.name);
            } catch (e) {
              localStorage.removeItem("sceneforge_user");
            }
          }

          // Inicializa o Google Identity Services quando carregado
          if (typeof google !== "undefined" && google.accounts) {
            this.initGoogleClient();
          } else {
            // Aguarda o script do Google carregar
            window.addEventListener("load", () => {
              setTimeout(() => this.initGoogleClient(), 500);
            });
          }

          // Fecha dropdown ao clicar fora
          document.addEventListener("click", (e) => {
            const dropdown = document.getElementById("user-dropdown");
            const avatar = document.getElementById("user-avatar");
            if (
              dropdown &&
              !dropdown.contains(e.target) &&
              e.target !== avatar
            ) {
              dropdown.classList.remove("active");
            }
          });
        },

        initGoogleClient() {
          if (typeof google === "undefined" || !google.accounts) {
            console.warn("[Auth] Google Identity Services not loaded");
            return;
          }

          google.accounts.id.initialize({
            client_id: this.CLIENT_ID,
            callback: (response) => this.handleCredentialResponse(response),
            auto_select: false,
            cancel_on_tap_outside: true,
          });

          console.log("[Auth] Google client initialized");
        },

        signIn() {
          if (typeof google === "undefined" || !google.accounts) {
            alert("Google Auth não carregado. Recarregue a página.");
            return;
          }

          if (
            this.CLIENT_ID === "SEU_GOOGLE_CLIENT_ID.apps.googleusercontent.com"
          ) {
            alert(
              "Configure o Google Client ID primeiro!\n\n1. Acesse https://console.cloud.google.com/apis/credentials\n2. Crie um OAuth 2.0 Client ID\n3. Adicione as origens JavaScript autorizadas\n4. Substitua CLIENT_ID no código"
            );
            console.info("Para configurar o Google Login:");
            console.info(
              "1. Acesse https://console.cloud.google.com/apis/credentials"
            );
            console.info("2. Crie um OAuth 2.0 Client ID");
            console.info("3. Adicione as origens JavaScript autorizadas");
            console.info("4. Substitua CLIENT_ID no código");
            return;
          }

          google.accounts.id.prompt((notification) => {
            if (notification.isNotDisplayed()) {
              // Fallback: usar popup
              this.signInWithPopup();
            }
          });
        },

        signInWithPopup() {
          const client = google.accounts.oauth2.initTokenClient({
            client_id: this.CLIENT_ID,
            scope: "email profile",
            callback: async (response) => {
              if (response.access_token) {
                // Busca informações do usuário
                const userInfo = await fetch(
                  "https://www.googleapis.com/oauth2/v3/userinfo",
                  {
                    headers: {
                      Authorization: `Bearer ${response.access_token}`,
                    },
                  }
                ).then((r) => r.json());

                const user = {
                  name: userInfo.name,
                  email: userInfo.email,
                  picture: userInfo.picture,
                  id: userInfo.sub,
                };

                localStorage.setItem("sceneforge_user", JSON.stringify(user));
                setState({ isAuthenticated: true, user: user });
                console.log("[Auth] User signed in:", user.name);
              }
            },
          });
          client.requestAccessToken();
        },

        handleCredentialResponse(response) {
          // Decodifica o JWT token
          const payload = this.parseJwt(response.credential);

          const user = {
            name: payload.name,
            email: payload.email,
            picture: payload.picture,
            id: payload.sub,
          };

          localStorage.setItem("sceneforge_user", JSON.stringify(user));
          setState({ isAuthenticated: true, user: user });
          console.log("[Auth] User signed in via credential:", user.name);
        },

        parseJwt(token) {
          const base64Url = token.split(".")[1];
          const base64 = base64Url.replace(/-/g, "+").replace(/_/g, "/");
          const jsonPayload = decodeURIComponent(
            atob(base64)
              .split("")
              .map((c) => {
                return "%" + ("00" + c.charCodeAt(0).toString(16)).slice(-2);
              })
              .join("")
          );
          return JSON.parse(jsonPayload);
        },

        signOut() {
          localStorage.removeItem("sceneforge_user");
          setState({ isAuthenticated: false, user: null });

          if (typeof google !== "undefined" && google.accounts) {
            google.accounts.id.disableAutoSelect();
          }

          this.toggleDropdown(false);
          console.log("[Auth] User signed out");
        },

        toggleDropdown(forceState) {
          const dropdown = document.getElementById("user-dropdown");
          if (!dropdown) return;

          if (typeof forceState === "boolean") {
            dropdown.classList.toggle("active", forceState);
          } else {
            dropdown.classList.toggle("active");
          }
        },

        isLoggedIn() {
          return state.isAuthenticated;
        },

        getUser() {
          return state.user;
        },
      };

      // Expose GoogleAuth globally
      window.GoogleAuth = GoogleAuth;

      // ============================================
      // INIT
      // ============================================
      console.log("SceneForge initializing...");

      // Initialize Google Auth first
      GoogleAuth.init();

      console.log(
        "Gemini API Key:",
        state.apiKeys.gemini
          ? "SET (" + state.apiKeys.gemini.substring(0, 8) + "...)"
          : "NOT SET"
      );
      console.log(
        "Using: Gemini 2.0 Flash (analysis), Nano Banana Pro (images), Veo 3.1 (video)"
      );

      subscribe(render);
      render();
    </script>
  </body>
</html>
